\documentclass[t,usepdftitle=false]{beamer}

\input{../../../tex-beamer-custom/preamble.tex}

\usepackage{multirow}

\title[NLA for CS and IE -- Lecture 13]{Numerical Linear Algebra\\for Computational Science and Information Engineering}
\subtitle{\vspace{.3cm}Lecture 13\\Krylov Subspace Methods for Linear Systems}
\hypersetup{pdftitle={NLA-for-CS-and-IE\_Lecture13}}

\date[Summer 2025]{Summer 2025}

\author[nicolas.venkovic@tum.de]{Nicolas Venkovic\\{\small nicolas.venkovic@tum.de}}
\institute[]{Group of Computational Mathematics\\School of Computation, Information and Technology\\Technical University of Munich}

\titlegraphic{\vspace{0cm}\includegraphics[height=1.1cm]{../../../logos/TUM-logo.png}}

\begin{document}
	
\begin{frame}[noframenumbering, plain]
	\maketitle
\end{frame}
	
\myoutlineframe

\section{Projection methods for linear systems}

% Slide 01 
\begin{frame}{General framework of projection methods for linear systems}
\begin{itemize}
\item Let $\mathcal{K}_m$ be a proper $m$-dimensional subspace of $\mathbb{R}^n$, i.e., $\mathcal{K}_m\subset\mathbb{R}^n$, typically with $m\ll n$.
\item[] We then seek for a $\tilde{x}\in\mathcal{K}_m$ which approximates the solution $x$ of $Ax=b$.
\item[] A typical way to form the approximation $\boxed{\tilde{x}\in\mathcal{K}_m}$ is to impose $m$ \textbf{independent orthogonality conditions on the residual} $r:=b-A\tilde{x}$ with respect to a $m$-dimensional \textbf{constraint subspace} $\mathcal{L}_m\subset\mathbb{R}^n$:
\begin{align}\label{eq:Petrov-Galerkin}
\boxed{r=b-A\tilde{x}\perp\mathcal{L}_m}\,.
\end{align}
If $\mathcal{K}_m=\mathcal{L}_m$, then Eq.~\eqref{eq:Petrov-Galerkin} is referred to as the \textbf{Galerkin condition}, and $\tilde{x}$ is formed by \textbf{orthogonal projection}.
\item[] More generally, we have $\mathcal{L}_m\neq\mathcal{K}_m$, in which case Eq.~\eqref{eq:Petrov-Galerkin} is referred to as the \textbf{Petrov-Galerkin condition}.
Then, the process of forming $\tilde{x}$ is an \textbf{oblique projection}.
\item[] A projection technique onto the approximation/search space $\mathcal{K}_m$ along the constraint subspace $\mathcal{L}_m$ is summarized as:
\begin{align*}
\boxed{
\text{Find }\tilde{x}\in\mathcal{K}_m\text{ such that }b-A\tilde{x}\perp\mathcal{L}_m
}\,.
\end{align*}
\end{itemize}
\end{frame}

% Slide 02
\begin{frame}{General framework of projection methods for linear systems, cont'd}
\begin{itemize}
\item The \textbf{projection techniques} presented in this lecture are \textbf{iterative}.
\item[] That is, as a pair $(\mathcal{K}_m,\mathcal{L}_m)\subset\mathbb{R}^n\times\mathbb{R}^n$ of $m$-dimensional search and constraint subspaces is used to form an approximate solution $\tilde{x}$ of $Ax=b$, \textbf{the next iteration consists of expanding those subspaces}, leading to a pair $(\mathcal{K}_{m+1},\mathcal{L}_{m+1})$ which is \textbf{then used to form a subsequent approximate solution}.
\item[] A projection technique is deployed with an \textbf{initial iterate} $x_0\in\mathbb{R}^n$.
\item[] Subsequent iterates are then formed leveraging $x_0$ by searching in the \textbf{affine subspace} $x_0+\mathcal{K}_m$.
The projection technique is then summarized as
\begin{align*}
\boxed{
\text{Find }\tilde{x}\in x_0+\mathcal{K}_m\text{ such that }b-A\tilde{x}\perp\mathcal{L}_m
}\,.
\end{align*}
If we write $\tilde{x}:=x_0+\hat{x}$ with $\hat{x}\in\mathcal{K}_m$, then the projection technique is reformulated as\vspace{-.3cm}
\begin{align*}
\boxed{
\text{Find }\hat{x}\in\mathcal{K}_m\text{ such that }r_0-A\hat{x}\perp\mathcal{L}_m
}
\end{align*}
where $r_0:=b-Ax_0$.
\end{itemize}
\end{frame}

% Slide 03
\begin{frame}{Matrix form of projection techniques for linear systems}
\begin{itemize}
\item Let the columns of $V_m:=[v_1,\dots,v_m]$ and $W_m:=[w_1,\dots,w_m]$ form \textbf{bases of the search and constraints spaces}, respectively, i.e.,
\begin{align*}
\text{range}(V_m)=\mathcal{K}_m\text{ and }\text{range}(W_m)=\mathcal{L}_m.
\end{align*}
\item[]Once equipped with such bases, one can recast the projection defined as finding $\tilde{x}\in x_0+\mathcal{K}_m$ such that $b-A\tilde{x}\perp\mathcal{L}_m$ into
\begin{align*}
\text{Find }\tilde{y}\in\mathbb{R}^m\text{ such that }\tilde{x}:=x_0+V_m\tilde{y}\text{ and }b-A\tilde{x}\perp\text{range}(W_m).
\end{align*}
Taking the dot product as inner product, this leads up to the following matrix form:
\begin{align*}
\text{Find }\tilde{y}\in\mathbb{R}^m\text{ such that }\tilde{x}:=x_0+V_m\tilde{y}\text{ and }W_m^T(r_0-AV_m\tilde{y})=0.
\end{align*}
\textbf{If $W_m^TAV_m$ is not singular}, we then have
\begin{align*}
\tilde{y}=(W_m^TAV_m)^{-1}W_m^Tr_0
\end{align*}
so that\vspace{-.3cm}
\begin{align*}
\boxed{
\tilde{x}=x_0+V_m(W_m^TAV_m)^{-1}W_m^Tr_0
}\,.
\end{align*}
\end{itemize}
\end{frame}

% Slide 04
\begin{frame}{Matrix form of projection techniques for linear systems, cont'd\textsubscript{1}}
\begin{itemize}
\item A proper projection technique to approximate the solution of a linear system in $x_0+\text{range}(V_m)$ along $\text{range}(W_m)$ requires that $W_m^TAV_m$ is not singular.
\item[] It can be shown that $W_m^TAV_m$ is \textbf{not singular} if and only if \textbf{no vector of the subspace $A\mathcal{K}$ is orthogonal to the constraints subspace} $\mathcal{L}_m$, i.e., $A\mathcal{K}_m\cap \mathcal{L}_m^\perp=\{0\}$.
\item[] Saad~(2003) states the following theorem:
\begin{theorem}[Non-singularity of $W_m^TAV_m$]
If $A$, $\mathcal{K}_m$ and $\mathcal{L}_m$ satisfy either of the two following conditions:\vspace{-.15cm}
\begin{itemize}\normalsize
\item[-] $A$ is symmetric positive definite and $\mathcal{L}_m=\mathcal{K}_m$, or\vspace{-.15cm}
\item[-] $A$ is non-singular and $\mathcal{L}_m=A\mathcal{K}_m$.\vspace{-.15cm}
\end{itemize}
Then the $W_m^TAV_m$ matrix is non-singular for any full-rank $V_m$ and $W_m$.
\end{theorem}
\end{itemize}\smallskip 
\tiny{Saad, Y. (2003). Iterative methods for sparse linear systems. Society for Industrial and Applied Mathematics.}
\end{frame}

% Slide 05
\begin{frame}{Matrix form of projection techniques for linear systems, cont'd\textsubscript{2}}
\begin{itemize}
\item In practical implementations of projection techniques to build approximate solutions to linear systems, we need to consider:
\begin{itemize}\normalsize
\item[-] How to choose the search and constraints subspaces $\mathcal{K}_m$ and $\mathcal{L}_m$ at a given iteration $m$.
\item[-] If an approximation is not good enough, how to expand those subspaces to  $\mathcal{K}_{m+1}$ and $\mathcal{L}_{m+1}$.
\end{itemize}
\item[] Of particular interest for the definition of projection techniques are the so-called \textbf{Krylov subspaces}:
\begin{align*}
\mathcal{K}_m(A,r_0):=\text{span}\{r_0,Ar_0,\dots,A^{m-1}r_0\}\subseteq\mathbb{F}^n
\end{align*}
which form a nested sequence:
\begin{align*}
\mathcal{K}_1(A,r_0)\subseteq \mathcal{K}_2(A,r_0)\subseteq\dots\subseteq\mathcal{K}_m(A,r_0)\subseteq\dots.
\end{align*}
\item[] A \textbf{Krylov subspace method} is a projection technique based on the subspace $\mathcal{K}_m(A,r_0)$.
Different choices of a constraints subspace lead to different kinds of Krylov subspace methods.
\end{itemize}
\end{frame}

% Slide 06
\begin{frame}{Matrix form of projection techniques for linear systems, cont'd\textsubscript{3}}
\begin{itemize}
\item The choice of the constraint subspace $\mathcal{L}_m$ is often made so that the approximation in $\mathcal{K}_m$ possesses some \textbf{optimality properties}, such as \textbf{minimizing the residual norm} or the \textbf{norm of the forward error}.
\item[] Some widely used Krylov subspace methods are proposed based on the choices
\begin{align*}
\mathcal{L}_m=\mathcal{K}_m(A,r_0),\;
\mathcal{L}_m=A\mathcal{K}_m(A,r_0)\text{ and }
\mathcal{L}_m=\mathcal{K}_m(A^T,r_0).
\end{align*}
\end{itemize}
\end{frame}

\section{Methods for general linear systems}

\subsection{Full orthogonalization method (FOM)}
% Slide 07
\begin{frame}{Full orthogonalization method (FOM)}
\begin{itemize}
\item The full orthogonalization method (FOM), proposed by Saad (1981), is an \textbf{orthogonal projection in a Krylov subspace} $\mathcal{K}_m(A,r_0)$, with constraints subspace $\mathcal{L}_m=\mathcal{K}_m$, i.e., it reads
\begin{align*}
\boxed{
\text{Find }x_m\in x_0+\mathcal{K}_m(A,r_0)\text{ such that }b-Ax_m\perp\mathcal{K}_m(A,r_0)}
\,.
\end{align*}
Assuming that the columns of $V_m:=[v_1,\dots,v_m]$ form a basis of the Krylov subspace $\mathcal{K}_m(A,r_0)$, the iterate formed by FOM is then given by
\begin{align*}
x_m:=x_0+V_m(V_m^TAV_m)^{-1}V_m^Tr_0.
\end{align*}
We saw in lecture 11 that, if the columns of $V_m$ form an othonormal basis of $\mathcal{K}_m(A,r_0)$ as obtained by Arnoldi, we then have
\begin{align*}
V_m^TAV_m=H_m
\end{align*}
where $H_m$ is an upper-Hessenberg matrix.
\item[] Moreover, we have $v_1:=r_0/\beta$, where $\beta:=\|r_0\|_2$, so that 
\begin{align*}
V_m^Tr_0=[v_1,\dots,v_m]^Tv_1\beta=\beta e_1^{(m)}
\;\text{ where }\;
e_1^{(m)}:=I_m[:,1].
\end{align*}
\end{itemize}
\tiny{Saad, Y. (1981). Krylov subspace methods for solving large unsymmetric linear systems. Mathematics of computation, 37(155), 105-126.}
\end{frame}

% Slide 08
\begin{frame}{Full orthogonalization method (FOM), cont'd\textsubscript{1}}
\begin{itemize}
\item[] Consequently, we have 
\begin{align*}
\boxed{
x_m:=x_0+V_m\tilde{y}
\;\text{ where }
H_m\tilde{y}=\beta e_1^{(m)}
}\,.
\end{align*}
In most cases, the dimension $m$ of the Krylov subspace $\mathcal{K}_m(A,r_0)$ is much smaller $n$, so that one can solve for $\tilde{y}$ such that $H_m\tilde{y}=\beta e_1^{(m)}$ using a direct method or, since $H_m$ is Hessenberg, possibly also using a QR factorization.
\item Let $x_m\in x_0+\mathcal{K}_m(A,r_0)$ be an iterate formed by FOM.
Then, we have
\begin{align*}
r_m
:=&\,b-Ax_m\\
=&\,b-A(x_0+V_m\tilde{y})\;\text{ where }H_m\tilde{y}=\beta e_1^{(m)}\\
=&\,r_0-AV_m\tilde{y}
\end{align*}
where we recall the Arnoldi relation $AV_m=V_m H_m+h_{m+1,m}v_{m+1}e_m^{(m)}{}^T$, so that\vspace{-.4cm}
\begin{align*}
r_m
=&\,r_0-V_mH_my-h_{m+1,m}(e_m^{(m)}{}^T\tilde{y})v_{m+1}\\
=&\,r_0-\beta v_1-h_{m+1,m}(e_m^{(m)}{}^T\tilde{y})v_{m+1}.
\end{align*}
\end{itemize}
\end{frame}

% Slide 09
\begin{frame}{Full orthogonalization method (FOM), cont'd\textsubscript{2}}
\begin{itemize}
\item[]But, remember that we have $r_0=\beta v_1$, so that we obtain
\begin{align*}
\boxed{
r_m=-h_{m+1,m}(e_m^{(m)}{}^T\tilde{y})v_{m+1}
}\,.
\end{align*}
One can then promptly evaluate the residual norm $\|r_m\|_2$, without having to form the iterate $x_m$, nor to evaluate an additional matrix-vector product.
Indeed, we have
\begin{align*}
\boxed{\|r_m\|_2=|h_{m+1,m}||e_m^{(m)}{}^T\tilde{y}|}\,.
\end{align*}
\item In practice, FOM is seldom used for the purpose of solving linear systems.
\end{itemize}
\end{frame}

% Slide 10
\begin{frame}{Full orthogonalization method (FOM), cont'd\textsubscript{3}}
\begin{itemize}
\item Implementations of the FOM method are defined by specifying a procedure to construct an orthonormal basis of the Krylov subspace $\mathcal{K}_m(A,r_0)$.
\item[] This can be done using any variant of the Arnoldi algorithm, e.g.,\vspace{-.3cm}
\begin{algorithm}[H]
\small
\caption{MGS-based FOM$:(x_0,\varepsilon)\mapsto x_j$}
\begin{algorithmic}[1]
\STATE{$r_0:=b-Ax_0$}
\STATE{$\beta:=\|r_0\|_2$}
\STATE{$v_1:=r_0/\beta$}
\FOR{$j=1,2\dots$}
\STATE{$w:=Av_j$}
\FOR{$i=1,\dots,j$}
\STATE{$h_{ij}:=w^Tv_i$}
\STATE{$w:=w-h_{ij}v_i$}
\ENDFOR
\STATE{$h_{j+1,j}:=\|w\|_2$}
\STATE{Solve for $\tilde{y}$ such that $H_j\tilde{y}=\beta e_1^{(j)}$}
\IF{$h_{j+1,j}|e_j^{(j)}{}^T\tilde{y}|<\varepsilon\|b\|_2$}
\STATE{Stop}
\COMMENT{Stop if $\|r_j\|_2<\varepsilon\|b\|_2$}
\ENDIF
\STATE{$v_{j+1}:=w/h_{j+1,j}$}
\ENDFOR
\STATE{$x_j:=x_0+V_j\tilde{y}$}
\end{algorithmic}
\end{algorithm}
\end{itemize}
\end{frame}

\subsection{General minimal residual (GMRES) method}

% Slide 11
\begin{frame}{Generalized minimal residual (GMRES) method}
\begin{itemize}
\item The generalized minimal residual (GMRES) method, proposed by Saad and Schultz (1986), is an \textbf{oblique projection in a Krylov subspace} $\mathcal{K}_m$, with constraints subspace $\mathcal{L}_m=A\mathcal{K}_m$, i.e., it reads
\begin{align}\label{eq:gmres}
\boxed{
\text{Find }x_m\in x_0+\mathcal{K}_m(A,r_0)\text{ such that }b-Ax_m\perp A\mathcal{K}_m(A,r_0)}
\,.
\end{align}
Assuming that the columns of $V_m:=[v_1,\dots,v_m]$ form a basis of the Krylov subspace $\mathcal{K}_m(A,r_0)$, the GMRES iterate is given by\vspace{-.1cm}
\begin{align*}
x_m:=x_0+((AV_m)^TAV_m)^{-1}(AV_m)^Tr_0.
\end{align*}
$\vspace{-.55cm}$\\
However, it is more common and practical to derive the GMRES iterate based on its \textbf{optimality property}:\vspace{-.15cm}
\begin{theorem}[Optimality of GMRES iterates]
The iterate $x_m$ is the solution of Pb.~\ref{eq:gmres} if and only it minimizes the residual norm $\|b-Ax\|_2$ over the affine subspace $x_0+\mathcal{K}_m(A,r_0)$, i.e., if and only if\vspace{-.2cm}
\begin{align*}
\|b-Ax_m\|_2=\min_{x\in x_0+\mathcal{K}_m(A,r_0)}\|b-Ax\|_2.
\end{align*}
\end{theorem}
\end{itemize}
%\smallskip
\tiny{Saad, Y., \& Schultz, M. H. (1986). GMRES: A generalized minimal residual algorithm for solving nonsymmetric linear systems. SIAM Journal on scientific and statistical computing, 7(3), 856-869.}
\end{frame}

% Slide 12
\begin{frame}{Generalized minimal residual (GMRES) method, cont'd\textsubscript{1}}
\begin{itemize}
\item Consequently, the GMRES iterate $x_m\in x_0+\mathcal{K}_m(A,r_0)$ is given by $\boxed{x_m:=x_0+V_m\tilde{y}}\,$, where
\begin{align*}
\tilde{y}
:=&\,\arg\min_{y\in\mathbb{R}^{m}}\|b-A(x_0+V_my)\|_2\\
=&\,\arg\min_{y\in\mathbb{R}^m}\|r_0-AV_my\|_2
\end{align*}
in which, we recall that $r_0=\beta v_1$, where $\beta:=\|r_0\|_2$, and, as the Arnoldi relation reads $AV_m=V_{m+1}\underline{H_m}$ in which $\underline{H_m}:=V_{m+1}^TAV_m$, we obtain:
\begin{align*}
\tilde{y}
=&\,\arg\min_{y\in\mathbb{R}^m}\|\beta v_1-V_{m+1}\underline{H_{m}}y\|_2\\
=&\,\arg\min_{y\in\mathbb{R}^m}\|V_{m+1}(\beta e_1^{(m+1)}-\underline{H_{m}}y)\|_2\\
=&\,\arg\min_{y\in\mathbb{R}^m}\|\beta e_1^{(m+1)}-\underline{H_{m}}y\|_2
\text{ where }e_1^{(m+1)}:=I_{m+1}[:,1].
\end{align*}
\end{itemize}
\end{frame}

% Slide 13
\begin{frame}{Generalized minimal residual (GMRES) method, cont'd\textsubscript{2}}
\begin{itemize}
\item The least-squares problem $\min_{y\in\mathbb{R}^m}\|\beta e_1^{(m+1)}-\underline{H_m}y\|_2$ is solved using the QR decomposition of the Hessenberg matrix, which can be done efficiently provided that the dimension $m$ of the approximation and constraints subspaces is not too large.
\item[] Let $Q_{m+1}\in\mathbb{R}^{(m+1)\times(m+1)}$ be the orthogonal matrix s.t. $\underline{H_m}=Q_{m+1}^T\underline{R_{m}}$, where $\underline{R_{m}}\in\mathbb{R}^{(m+1)\times m}$ is an upper-triangular matrix.
\item[] Then, the least-squares problem is recast into
\begin{align*}
\min_{y\in\mathbb{R}^m}\|\beta e_1^{(m+1)}-\underline{H_{m}}y\|_2
=&\min_{y\in\mathbb{R}^m}\|\beta e_1^{(m+1)}-Q_{m+1}^T\underline{R_m}y\|_2\\
=&\min_{y\in\mathbb{R}^m}\|\beta Q_{m+1}e_1^{(m+1)}-\underline{R_m}y\|_2\\
=&\min_{y\in\mathbb{R}^m}\left\|\beta q_1-\begin{bmatrix}R_m\\0_{1\times m}\end{bmatrix}y\right\|_2
\end{align*}
where $q_1:=Q_{m+1}e_1^{(m+1)}=Q_{m+1}[1:m+1,1]$ and $R_m=\underline{R_{m}}[1\!:\!m,1\!:\!m]$.
\end{itemize}
\end{frame}

% Slide 14
\begin{frame}{Generalized minimal residual (GMRES) method, cont'd\textsubscript{3}}
\begin{itemize}
\item[] So that the least-squares problem is solved by solving the following triangular system:\vspace{-.25cm}
\begin{align*}
\boxed{R_m\tilde{y}=\beta q_1[1:m]}\,.
\end{align*}
\item Then, the residual $r_m:=b-Ax_m$ is s.t. $r_m=V_{m+1}(\beta e_1^{(m+1)}-\underline{H_m}\tilde{y})$ and
\begin{align*}
\|r_m\|_2
=&\,\|\beta e_1^{(m+1)}-\underline{H_m}\tilde{y}\|_2\\
=&\,\left\|\beta q_1-\begin{bmatrix}R_m\\0_{1\times m}\end{bmatrix}\tilde{y}\right\|_2\\
=&\,\left\|\beta q_1-\begin{bmatrix}\beta q_1[1:m]\\0\end{bmatrix}\right\|_2\\
=&\,\left\|\begin{bmatrix}0_{m\times 1}\\\beta q_1[m+1]\end{bmatrix}\right\|_2
\end{align*}
\vspace{-.4cm}
\begin{align*}
\hspace{-2.8cm}\text{so that } \boxed{\|r_m\|_2=\beta|q_1[m+1]|}\,.
\end{align*}
Thus, one needs not to assemble the iterate $x_m$, nor to perform an additional matrix-vector product in order to monitor convergence.
\end{itemize}
\end{frame}

% Slide 15
\begin{frame}{Generalized minimal residual (GMRES) method, cont'd\textsubscript{4}}
\begin{itemize}
\item Just like with FOM, the workhorse of GMRES is the orthogonalization of Krylov basis vectors.
In particular, this is most frequently implemented on the basis of the MGS procedure:\vspace{-.35cm}
\begin{algorithm}[H]
\small
\caption{MGS-based GMRES$:(x_0,\varepsilon)\mapsto x_j$}
\begin{algorithmic}[1]
\STATE{$r_0:=b-Ax_0$}
\STATE{$\beta:=\|r_0\|_2$}
\STATE{$v_1:=r_0/\beta$}
\FOR{$j=1,2\dots$}
\STATE{$w:=Av_j$}
\FOR{$i=1,\dots,j$}
\STATE{$h_{ij}:=w^Tv_i$}
\STATE{$w:=w-h_{ij}v_i$}
\ENDFOR
\STATE{$h_{j+1,j}:=\|w\|_2$}
\STATE{Solve for $\tilde{y}=\arg\min_{y\in\mathbb{R}^j}\|\beta e_1^{(j+1)}-\underline{H_j}y\|$}
\IF{$\|\beta e_1^{(j+1)}-\underline{H_j}\tilde{y}\|<\varepsilon\|b\|_2$}
\STATE{Stop}
\COMMENT{Stop if $\|r_j\|_2<\varepsilon\|b\|_2$}
\ENDIF
\STATE{$v_{j+1}:=w/h_{j+1,j}$}
\ENDFOR
\STATE{$x_j:=x_0+V_j\tilde{y}$\vspace{-.1cm}}
\end{algorithmic}
\end{algorithm}
\end{itemize}
\smallskip
\tiny{Bai, Z. Z., \& Pan, J. Y. (2021). Matrix analysis and computations. Society for Industrial and Applied Mathematics.}
\end{frame}

% Slide 16
\begin{frame}{Generalized minimal residual (GMRES) method, cont'd\textsubscript{5}}
\begin{itemize}
\item Remember that the least-squares problem $\min_{y\in\mathbb{R}^m}\|\beta e_1^{(m+1)}-\underline{H_m}y\|_2$ is recast into the linear system $R_m\tilde{y}=\beta q_1[1\!:\!m]$ where $R_m\!:=\underline{R_m}[1\!:\!m,1\! :\!m],\hspace{-1cm}$\\ 
in which the QR decomposition $Q_{m+1}^T\underline{R_m}=\underline{H_m}$ is needed.
\item[] Suppose that we have obtained the QR decomposition of the matrix $\underline{H_{j-1}}$, and we are interested in getting the decomposition of $\underline{H_j}$ with the least amount of work possible.
Clearly, we have
\begin{align*}
\underline{H_j}=
\begin{bmatrix}
\underline{H_{j-1}}&h_{1:j,j}\\
0_{1\times j-1}&h_{j+1,j}
\end{bmatrix}.
\end{align*}
We saw in Lecture 07 that Givens rotations can be used to turn an upper Hessenberg matrix into triangular form.
In particular, for $\underline{H_{j-1}}$, we have
\begin{align*}
\underline{R_{j-1}}=
\begin{bmatrix}
R_{j-1}\\
0_{1\times (j-1)}
\end{bmatrix}=
G_{j-1}^{(j)}G^{(j)}_{j-2}\dots G^{(j)}_1\underline{H_{j-1}}=Q_j\underline{H_{j-1}}
\end{align*}
\end{itemize}
\end{frame}

% Slide 17
\begin{frame}{Generalized minimal residual (GMRES) method, cont'd\textsubscript{6}}
\begin{itemize}
\item[]where the Givens rotation matrices $G^{(j)}_1,\dots,G^{(j)}_{j-1}\in\mathbb{R}^{j\times j}$ are given by
{\small
\begin{align*}
G_i^{(j)}=
\begin{bmatrix}
1&&&&&&&\\
&\ddots&&&&&&\\
&&1&&&&&\\
&&&{\color{blue}{c_i}}&{\color{blue}{s_i}}&&&\\
&&&{\color{red}{-s_i}}&{\color{red}{c_i}}&&&\\
&&&&&1&&\\
&&&&&&\ddots&\\
&&&&&&&1\\
\end{bmatrix}
\begin{matrix}
{\color{blue}{i\text{-th row}}}\hfill\\
{\color{red}{(i+1)\text{-th row}}}\hfill
\end{matrix}
\end{align*}}\\
in which the scalars $s_i$ and $c_i$ are set so as to zero the $(i+1,i)$-entry of the Hessenberg matrix $G_i^{(j)}$ is applied to.
\item[] Clearly, we have 
\begin{align*}
G^{(j+1)}_i=
\begin{bmatrix}
G^{(j)}_i&0_{j\times 1}\\
0_{1\times j}&1
\end{bmatrix}
\end{align*}
for $i=1,\dots,j-1$.
\end{itemize}
\end{frame}

% Slide 18
\begin{frame}{Generalized minimal residual (GMRES) method, cont'd\textsubscript{7}}
\begin{itemize}
\item[]so that\vspace{-.15cm}
\begin{align*}
\underline{R_j}
=&\,G_j^{(j+1)}\dots G_1^{(j+1)}\underline{H_j}\\
=&\,G_j^{(j+1)}\begin{bmatrix}
G_{j-1}^{(j)}\dots G_1^{(j)}\underline{H_{j-1}}&G_{j-1}^{(j)}\dots G_1^{(j)}h_{1:j,j}\\
0_{1\times(j-1)}&h_{j+1,j}
\end{bmatrix}\\
=&\,G_j^{(j+1)}\begin{bmatrix}
\underline{R_{j-1}}&G_{j-1}^{(j)}\dots G_1^{(j)}h_{1:j,j}\\
0_{1\times(j-1)}&h_{j+1,j}
\end{bmatrix}\\
=&\,\begin{bmatrix}
\underline{R_{j-1}}&G_j^{(j+1)}[1:j,1:j+1]\begin{bmatrix}G_{j-1}^{(j)}\dots G_1^{(j)}h_{1:j,j}\\h_{j+1,j}\end{bmatrix}\\
0_{1\times(j-1)}&0
\end{bmatrix}.
\end{align*}
Therefore, while performing the $j$-th iteration of GMRES, one is equipped with $\underline{R_{j-1}}$ and $\underline{H_j}$.
In order to assemble $\underline{R_j}$, there only remains to apply the Givens rotations $G_1^{(j+1)},\dots,G_j^{(j+1)}$ to the last column of $\underline{H_j}$, i.e.,\vspace{-.15cm}
\begin{align*}
\boxed{\underline{R_j}[1:j+1,j]=G_j^{(j+1)}\dots G_1^{(j+1)}h_{1:j+1,j}}\,.
\end{align*}
\end{itemize}
\end{frame}

% Slide 19
\begin{frame}{Generalized minimal residual (GMRES) method, cont'd\textsubscript{8}}
\begin{itemize}
\item We saw that the least-squares problem $\min_{x\in x_0+\mathcal{K}_j(A,r_0)}\|b-Ax\|_2$ can be recast in the linear system $R_j\tilde{y}=\underline{g_j}[1:j]$ where $\underline{g_j}:=\beta Q_{j+1}e_1^{(j+1)}$ so that
\begin{align*}
\underline{g_j}:=\beta G_j^{(j+1)}\dots G_1^{(j+1)}e_1^{(j+1)}=
\begin{bmatrix}
\gamma_1\\\vdots\\\gamma_{j-1}\\c_j\gamma_{j}\\-s_j\gamma_{j}
\end{bmatrix}
\;\text{ where }\;
\begin{bmatrix}
\gamma_1\\\vdots\\\gamma_{j}
\end{bmatrix}
=\underline{g_{j-1}}
\end{align*}
with $\underline{g_0}=\beta$, and in which the scalars $s_i$ and $c_i$ are given by
\begin{align*}
s_j=\frac{h_{j+1,j}}{\sqrt{\left(h_{jj}^{(j-1)}\right)^{2}+h_{j+1,j}^2}}
\text{ and }
c_j=\frac{h_{jj}^{(j-1)}}{\sqrt{\left(h_{jj}^{(j-1)}\right)^{2}+h_{j+1,j}^2}}.
\end{align*}
where $\underline{H_{j}^{(j)}}:=\underline{R_j}$.
\end{itemize}
\end{frame}

% Slide 20
\begin{frame}{Generalized minimal residual (GMRES) method, cont'd\textsubscript{9}}
\begin{itemize}
\item In practice, the $\underline{R_1},\dots,\underline{R_m}$ and $\underline{
g_1},\dots,\underline{g_m}$ are often computed in-place, stored in pre-allocated $\underline{H_m}$ and $\underline{g_m}$.
This yields the following algorithm\vspace{-.3cm}
\begin{algorithm}[H]
\footnotesize
\caption{Practical GMRES$:(x_0,m,\varepsilon)\mapsto x_j$}
\begin{algorithmic}[1]
\STATE{\color{gray}{// Allocate $\underline{H}\in\mathbb{R}^{(m+1)\times m}$, $\underline{g}\in\mathbb{R}^{m+1}$ and $V\in\mathbb{R}^{n\times (m+1)}\vspace{-.07cm}$}}
\STATE{$r_0:=b-Ax_0;$ $\beta:=\|r_0\|_2;$ $\underline{g}:=[\beta,0,\dots,0]^T;$ $v_1:=r_0/\beta$}
\FOR{$j=1,2\dots$}
\STATE{Compute $h_{1:j+1,j}$ and $v_{j+1}$}
\FOR{$i=1,\dots,j-1$}
\STATE{\color{gray}{// Apply $G_i^{(j+1)}$ to $h_{1:j+1,j}$.}}
\STATE{$\begin{bmatrix}h_{ij}\\h_{i+1,j}\end{bmatrix}
:=
\begin{bmatrix}c_i&s_i\\-s_i&c_i\end{bmatrix}
\begin{bmatrix}h_{ij}\\h_{i+1,j}\end{bmatrix}$
where
$\begin{cases}
s_i:=h_{i+1,i}/(h_{ii}^2+h_{i+1,i}^2)^{1/2}\\
c_i:=h_{ii}/(h_{ii}^2+h_{i+1,i}^2)^{1/2}
\end{cases}$}
\ENDFOR
\STATE{\color{gray}{// Apply $G_{j}^{(j+1)}$ to $\underline{g}[1:j+1]$ and $h_{1:j+1,j}$}}
\STATE{$\begin{bmatrix}\underline{g}[j]\\\underline{g}[j+1]\end{bmatrix}
:=
\begin{bmatrix}c_j&s_j\\-s_j&c_j\end{bmatrix}
\begin{bmatrix}\underline{g}[j]\\0\end{bmatrix}$
where
$\begin{cases}
s_j:=h_{j+1,j}/(h_{jj}^2+h_{j+1,j}^2)^{1/2}\\
c_j:=h_{jj}/(h_{jj}^2+h_{j+1,j}^2)^{1/2}
\end{cases}$}
\STATE{$h_{jj}:=c_jh_{jj}+s_jh_{j+1,j};$ $h_{j+1,j}:=0$}
\IF{$|\underline{g}[j+1]|<\varepsilon\|b\|_2$}
\STATE{Stop}
\COMMENT{Stop if $\|r_j\|_2<\varepsilon\|b\|_2$}
\ENDIF
\ENDFOR
\STATE{$x_j:=x_0+V_j\tilde{y}$ where $\tilde{y}$ is solution of triangular system $H[1:j,1:j]\tilde{y}=\underline{g}[1:j]$}
\end{algorithmic}
\end{algorithm}
\end{itemize}
\end{frame}

\section{Methods for symmetric linear systems}

\subsection{Conjugate gradient (CG) method}
% Slide 21
\begin{frame}{Conjugate gradient (CG) method}
\begin{itemize}
\item Here, we assume that the matrix $A$ is \textbf{SPD}.
Similarly to FOM, the CG method (Hestenes and Stiefel, 1952) is an \textbf{orthogonal projection} in the Krylov subspace $\mathcal{K}_m(A,r_0)$.
That is, CG iterates are formed as follows:
\begin{align*}
\boxed{
\text{Find }x_m\in x_0+\mathcal{K}_m(A,r_0)
\text{ such that }
b-Ax_m\perp\mathcal{K}_m(A,r_0)
}\,.
\end{align*}
Once again, assuming that the columns of $V_m:=[v_1,\dots,v_m]$ form a basis of $\mathcal{K}_m(A,r_0)$, the CG iterate is given by
\begin{align*}
x_m:=x_0+V_m(V_m^TAV_m)^{-1}V_m^Tr_0.
\end{align*}
We saw in Lecture 11 that, if the columns of $V_m$ form an orthonormal basis of $\mathcal{K}_m(A,r_0)$ as obtained by the Lanczos method, we then have
\begin{align*}
V_m^TAV_m=T_m
\end{align*}
where $T_m$ is a tridiagonal matrix.
\item[] Moreover, we have $v_1:=r_0/\beta$, where $\beta:=\|r_0\|_2$, so that\vspace{-.1cm}
\begin{align*}
V_m^Tr_0=[v_1,\dots,v_m]^Tv_1\beta=\beta e_1^{(m)}
\text{ where }
e_1^{(m)}:=I_m[:,1].
\end{align*}
\end{itemize}%\smallskip
\tiny{Hestenes M. R. \& Stiefel E. L. (1952). Methods of conjugate gradients for solving linear systems. Journal of Research of the National Bureau of Standards, 49, 409–436.}
\end{frame}

% Slide 22
\begin{frame}{Conjugate gradient (CG) method, cont'd\textsubscript{1}}
\begin{itemize}
\item[]Consequently, we have
\begin{align*}
\boxed{x_m:=x_0+V_m\tilde{y}
\text{ where }
T_m\tilde{y}=\beta e_1^{(m)}}\,.
\end{align*}
As formulated above, each CG iterate $x_m$ requires to solve a linear system for $\tilde{y}$ with the tridiagonal matrix $T_m$.
\item[] As $A$ is SPD, so is $T_m$.
Thus, one can make use of the LU decomposition of $T_m$ in order to solve $T_m\tilde{y}=\beta e_1^{(m)}$.
\item[] Let $x_{m+1}$ denote the CG iterate in $x_0+\mathcal{K}_{m+1}(A,r_0)$, i.e.,
\begin{align*}
x_{m+1}:=x_0+V_{m+1}\tilde{y}
\;\text{ where }\;
T_{m+1}\tilde{y}=\beta e_1^{(m+1)}.
\end{align*}
In what follows, we present the steps enumerated by Bai and Pan~(2021) in order to construct the CG iterate $x_{m+1}$ given $x_m$.
\end{itemize}
\smallskip
\tiny{Bai, Z. Z., \& Pan, J. Y. (2021). Matrix analysis and computations. Society for Industrial and Applied Mathematics.}
\end{frame}

% Slide 23
\begin{frame}{Conjugate gradient (CG) method, cont'd\textsubscript{2}}
\begin{itemize}
\item[]Let the tridiagonal matrices $T_{m}$ and $T_{m+1}$ admit LU decompositions of the form $L_mU_m$ and $L_{m+1}U_{m+1}$, respectively, in which we have
\begin{align*}
\hspace{-.5cm}
L_\ell=
\begin{bmatrix}
1\\
\gamma_1&1\\
&\ddots&\ddots\\
&&\gamma_{\ell-1}&1
\end{bmatrix}
\text{and }\;
U_\ell=
\begin{bmatrix}
\eta_1&\beta_1\\
&\eta_2&\ddots\\
&&\ddots&\beta_{\ell-1}\\
&&&\eta_\ell
\end{bmatrix}
\text{ for }
\ell=m,m+1.
\end{align*}
That is, $L_m$ and $U_m$ are the $m$-th leading principal sub-matrices of $L_{m+1}$ and $U_{m+1}$.
\item[] More precisely, we have
\begin{align*}
\begin{cases*}
\eta_1:=\alpha_1\\
\gamma_i:=\beta_i/\eta_i&\text{ for }i=1,\dots,m\\
\eta_{i+1}:=\alpha_{i+1}-\gamma_i\beta_i&\text{ for }i=1,\dots,m
\end{cases*}
\end{align*}
where $\alpha_j:=T_{jj}=v_j^TAv_j$ and $\beta_j:=T_{j+1,j}:=v_{j+1}^TAv_j=v_j^TAv_{j+1}$ denote the diagonal and off-diagonal components of $T_m$, respectively.
\end{itemize}
\end{frame}

% Slide 24
\begin{frame}{Conjugate gradient (CG) method, cont'd\textsubscript{3}}
\begin{itemize}
\item[] Given those LU factorizations, the CG iterate $x_m\in x_0+\mathcal{K}_m(A,r_0)$ may be recast into
\begin{align*}
x_m:=x_0+P_mz^{(m)}
\end{align*}
where $P_m:=V_mU_m^{-1}\in\mathbb{R}^{n\times m}$ and $z^{(m)}:=\beta L_m^{-1}e_1^{(m)}\in\mathbb{R}^m$.
Then, we have
\begin{align*}
P_{m+1}:=V_{m+1}U_{m+1}^{-1}=
[V_m\,v_{m+1}]
\begin{bmatrix}
U_m^{-1}&*_{m\times 1}\\
0_{1\times m}&1/\eta_{m+1}
\end{bmatrix}
=&\,[V_mU_m^{-1}\,p_{m+1}]\\
=&\,[P_m\,p_{m+1}].
\end{align*}
And, from $V_{m+1}=P_{m+1}U_{m+1}$, we get
\begin{align*}
v_{m+1}=\beta_mp_m+\eta_{m+1}p_{m+1}
\implies
\boxed{p_{m+1}=(v_{m+1}-\beta_mp_m)/\eta_{m+1}}
\end{align*}
for $m=1,2,\dots$, while $\boxed{p_1=v_1/\eta_1}$.
\end{itemize}
\end{frame}

% Slide 25
\begin{frame}{Conjugate gradient (CG) method, cont'd\textsubscript{4}}
\begin{itemize}
\item[] Then, as we denote $z^{(m+1)}:=[z^{(m)}{}^T\,z_{m+1}]^T=[z_1,\dots,z_m,z_{m+1}]^T$, we see that
\begin{align*}
L_{m+1}z^{(m+1)}
=&\,\beta e_1^{(m+1)}\\
\begin{bmatrix}L_mz^{(m)}\\\gamma_mz_m+z_{m+1}\end{bmatrix}
=&\,\begin{bmatrix}\beta e_1^{(m)}\\0\end{bmatrix}
\end{align*}
so that $\boxed{z_{m+1}=-\gamma_mz_m}$ for $m=1,2,\dots$ while $\boxed{z_1=\beta}$.
Therefore, we get\vspace{-.3cm}
\begin{align*}
\hspace{-.5cm}
x_{m+1}:=&\,x_0+P_{m+1}z^{(m+1)}\\
=&\,x_0+[P_m\,p_{m+1}]\begin{bmatrix}z^{(m)}\\z_{m+1}\end{bmatrix}\\
=&\,x_0+P_mz^{(m)}+z_{m+1}p_{m+1}
\end{align*}
so that
\begin{align*}
\boxed{x_{m+1}:=x_m+z_{m+1}p_{m+1}
\text{ for }m=0,1,2,\dots}\,.
\end{align*}
\end{itemize}
\end{frame}

% Slide 26
\begin{frame}{Conjugate gradient (CG) method, cont'd\textsubscript{5}}
\begin{itemize}
\item Then, alongside an implementation of Lanczos procedure which generates a set of orthonormal basis vectors $v_1,v_2,\dots,v_{m+1}$ spanning the subspace $\mathcal{K}_m(A,r_0)$ with the tridiagonal components $\alpha_1,\dots,\alpha_{m+1}$ and $\beta_1,\dots,\beta_{m}$, one can generate the sequence $x_1,x_2,\dots,x_{m+1}$ of CG iterates as follows:\vspace{-.1cm}
\begin{empheq}[box=\fbox]{align*}
&r_0:=b-Ax_0\\
&\beta:=\|r_0\|_2;\,z_1:=\beta\\
&v_1:=r_0/\beta;\,\alpha_1:=v_1^TAv_1;\,\eta_1:=\alpha_1;\,p_1:=v_1/\eta_1\\
&\text{for }j=1,\dots,m\\
&\hspace{.5cm}x_{j}:=x_{j-1}+z_{j}p_{j}\\
&\hspace{.5cm}\text{Compute }\alpha_{j+1},\beta_j\text{ and }v_{j+1}\text{ by Lanczos iteration}\\
&\hspace{.5cm}\gamma_j:=\beta_j/\eta_j\\
&\hspace{.5cm}\eta_{j+1}:=\alpha_{j+1}-\gamma_j\beta_j\\
&\hspace{.5cm}z_{j+1}:=-\gamma_jz_j\\
&\hspace{.5cm}p_{j+1}:=(v_{j+1}-\beta_jp_j)/\eta_{j+1}
\end{empheq}
\end{itemize}
\end{frame}

% Slide 27
\begin{frame}{Conjugate gradient (CG) method, cont'd\textsubscript{6}}
\begin{itemize}
\item We will find it useful to consider \textbf{generic inner products} $(\cdot,\cdot)$ in place of the usual dot product.
Two important results prove to be useful in deriving the CG algorithm.
First, there is the \textbf{conjugacy} of the $p$ vectors:
\begin{theorem}[$A$-orthogonality of $p$ vectors]
Assuming $A$ is SPD, the vectors $p_1,\dots,p_{m+1}$ built as described on the previous slides are $A$\textbf{-orthogonal} (or \textbf{conjugate}).
That is,\vspace{-.25cm}
\begin{align*}
(p_i,p_j)_A:=(Ap_i,p_j)=0\text{ if }i\neq j.
\end{align*}
\end{theorem}
\item[] Second, there is the \textbf{orthogonality of residual vectors}:\vspace{-.1cm}
\begin{theorem}[Orthogonality of residual vectors]
Let $r_j:=b-Ax_j$ where $x_j$ is the CG iterate in $x_0+\mathcal{K}_m(A,r_0)$.
Then, \vspace{-.25cm}
\begin{align*}
r_j=\rho_jv_{j+1},\;\text{ where }\;
\rho_0:=\beta\;\text{ and }\;
\rho_j:=-\beta_je_j^{(j)}{}^T\tilde{y}\;\text{ s.t. }\;
T_j\tilde{y}=\beta e_1^{(j)}
\end{align*}
$\vspace{-.7cm}$\\
so that, by virtue of orthogonality of the Krylov basis vectors $v_1,\dots,v_{m+1}$, the CG residual vectors $r_0,\dots,r_{m}$ are orthogonal, i.e., $(r_i,r_j)=0$ if $i\neq j$.
\end{theorem}
\end{itemize}
\end{frame}

% Slide 28
\begin{frame}{Conjugate gradient (CG) method, cont'd\textsubscript{7}}
\begin{itemize}
\item Now, let us define the \textbf{search direction} $\tilde{p}_{j+1}:=\rho_j\eta_{j+1}p_{j+1}$ so that, using the fact that $r_j=\rho_jv_{j+1}$, we get\vspace{-.3cm}
\begin{align*}
p_{j+1}:=&\,(v_{j+1}-\beta_jp_j)/\eta_{j+1}\\
\rho_j\eta_{j+1}p_{j+1}:=&\,\rho_jv_{j+1}-\rho_j\beta_jp_j\\
\tilde{p}_{j+1}:=&\,r_{j}-\rho_j\beta_jp_j
\end{align*}
$\vspace{-.9cm}$
\begin{align*}
\hspace{-.5cm}\boxed{\tilde{p}_{j+1}:=r_{j}+\tau_j\tilde{p}_j}
\end{align*}
where $\tau_j:=-\rho_j\beta_j/(\rho_{j-1}\eta_j)$.
These search directions are $A$\textbf{-orthogonal}.
\item[] Then, from $x_{j}:=x_{j-1}+z_{j}p_{j}$, we get
\begin{align*}
\boxed{x_{j}:=x_{j-1}+\xi_j\tilde{p}_j}
\;\text{ where }\;
\xi_j:=z_j/(\rho_{j-1}\eta_j).
\end{align*}
Also, the CG residual vector $r_j$ can be reformulated as follows:
\begin{align*}
r_j:=b-Ax_j=b-A(x_{j-1}+\xi_j\tilde{p}_j)
=b-Ax_{j-1}-\xi_jA\tilde{p}_j
\end{align*}
so that $\boxed{r_j:=r_{j-1}-\xi_jA\tilde{p}_j}$.
\end{itemize}
\end{frame}

% Slide 29
\begin{frame}{Conjugate gradient (CG) method, cont'd\textsubscript{8}}
\begin{itemize}
\item Now, we are only left with finding alternative expressions for $\tau_j$ and $\xi_j$ which do not explicitly depend on the tridiagonal form $T_j$ and its LU decomposition.
\begin{itemize}\normalsize
\item[-] First, using the stated \textbf{orthogonality of CG residuals}, we get
\begin{align*}
(r_j,r_{j-1})=&\,0\\
(r_{j-1}-\xi_jA\tilde{p}_j,r_{j-1})=&\,0\\
(r_{j-1},r_{j-1})-\xi_j(A\tilde{p}_j,r_{j-1})=&\,0
\end{align*}
for which using the \textbf{conjugacy of search directions} as well as $\tilde{p}_{j+1}:=r_j+\tau_j\tilde{p}_j$ leads to
\begin{align*}
(A\tilde{p}_j,r_{j-1})
=&\,(A\tilde{p}_j,\tilde{p}_j-\tau_{j-1}\tilde{p}_{j-1})\\
=&\,(A\tilde{p}_j,\tilde{p}_j)-\tau_{j-1}(A\tilde{p}_j,\tilde{p}_{j-1})\\
=&\,(A\tilde{p}_j,\tilde{p}_j)
\end{align*}
so that $\boxed{\xi_j=(r_{j-1},r_{j-1})/(A\tilde{p}_j,\tilde{p}_j)}$.
\end{itemize}
\end{itemize}
\end{frame}

% Slide 30
\begin{frame}{Conjugate gradient (CG) method, cont'd\textsubscript{9}}
\begin{itemize}
\item[]
\begin{itemize}\normalsize
\item[-] Second, in order to find an alternative expression for $\tau_j$, we start as follows from the statement of \textbf{conjugacy of search directions}:
\begin{align*}
(A\tilde{p}_j,\tilde{p}_{j+1})=&\,0\\
(A\tilde{p}_j,r_j+\tau_j\tilde{p}_{j})=&\,0\\
(A\tilde{p}_j,r_j)+\tau_j(A\tilde{p}_j,\tilde{p}_{j})=&\,0
\end{align*}
so that $\tau_j=-(A\tilde{p}_j,r_j)/(A\tilde{p}_j,\tilde{p}_j)$.
Then, using $r_j:=r_{j-1}-\xi_jA\tilde{p}_j$ as well as the \textbf{orthogonality of CG residuals}, we get
\begin{align*}
\tau_j=
-\frac{(A\tilde{p}_j,r_j)}{(A\tilde{p}_j,\tilde{p}_j)}=
\frac{1}{\xi_j}\frac{(r_j-r_{j-1},r_j)}{(A\tilde{p}_j,\tilde{p}_j)}=
\frac{(A\tilde{p}_j,\tilde{p}_j)}{(r_{j-1},r_{j-1})}\frac{(r_j,r_j)}{(A\tilde{p}_j,\tilde{p}_j)}
\end{align*}
so that $\boxed{\tau_j=(r_j,r_j)/(r_{j-1},r_{j-1})}$.
\end{itemize}
\end{itemize}
\end{frame}

% Slide 31
\begin{frame}{Conjugate gradient (CG) method, cont'd\textsubscript{10}}
\begin{itemize}
\item Piecing together all the expressions for the update of $\xi_j,x_j,r_j,\tau_j$ and $\tilde{p}_{j+1}$, we get the following iteration for the CG method:
\begin{empheq}[box=\fbox]{align*}
&r_0:=b-Ax_0\\
&\tilde{p}_1:=r_0\\
&\text{for }j=1,\dots,m\\
&\hspace{.5cm}\xi_j:=(r_{j-1},r_{j-1})/(A\tilde{p}_j,\tilde{p}_j)\\
&\hspace{.5cm}x_{j}:=x_{j-1}+\xi_j\tilde{p}_j\\
&\hspace{.5cm}r_j:=r_{j-1}-\xi_jA\tilde{p}_j\\
&\hspace{.5cm}\tau_j:=(r_j,r_j)/(r_{j-1},r_{j-1})\\
&\hspace{.5cm}\tilde{p}_{j+1}:=r_j+\tau_j\tilde{p}_j
\end{empheq}
\end{itemize}
\end{frame}

% Slide 32
\begin{frame}{Conjugate gradient (CG) method, cont'd\textsubscript{11}}
\begin{itemize}
\item In order to reflect the most commonly encountered formulations of the CG method, the following changes of variables are operated
\begin{align*}
\xi_j\mapsto \alpha_j,\;
\tau_j\mapsto \beta_j\;\text{ and }
\tilde{p}_j\mapsto p_j
\end{align*}
where $\alpha_j$ and $\beta_j$ are not to be confused with the components of the tridiagonal form of $A$.
\item[] This leads to the following algorithm:\vspace{-.3cm}
\begin{algorithm}[H]
\small
\caption{CG$:(x_0,\varepsilon)\mapsto x_j$}
\begin{algorithmic}[1]
\STATE{$r_0:=b-Ax_0$}
\STATE{$p_1:=r_0$}
\FOR{$j=1,2\dots$}
\STATE{$\alpha_j:=(r_{j-1},r_{j-1})/(Ap_j,p_j)$}
\STATE{$x_j:=x_{j-1}+\alpha_jp_j$}
\STATE{$r_j:=r_{j-1}-\alpha_jAp_j$}
\IF{$\|r_j\|_2<\varepsilon\|b\|_2$}
\STATE{Stop}
\ENDIF
\STATE{$\beta_j:=(r_j,r_j)/(r_{j-1},r_{j-1})$}
\STATE{$p_{j+1}:=r_j+\beta_jp_j$}
\ENDFOR
\end{algorithmic}
\end{algorithm}
\end{itemize}
\end{frame}

% Slide 33
\begin{frame}{Conjugate gradient (CG) method, cont'd\textsubscript{12}}
\begin{itemize}
\item Note that the CG method can be implemented allocating storage only for the iterate $x$, the search direction $p$, the matrix-vector product $Ap$ and the residual $r$. 
Doing so leads to the following practical implementation:\vspace{-.2cm}
\begin{algorithm}[H]
\small
\caption{Practical CG$:(x_0,\varepsilon)\mapsto x_j$}
\begin{algorithmic}[1]
\STATE{\color{gray}{Allocate memory for} $x,p,w,r\in\mathbb{R}^n$}
\STATE{$r:=b-Ax_0$}
\STATE{$p:=r$}
\FOR{$j=1,2\dots$}
\STATE{$w:=Ap$}
\STATE{$\alpha:=(r,r)/(w,p)$}
\STATE{$\beta:=1/(r,r)$}
\STATE{$x:=x+\alpha p$}
\STATE{$r:=r-\alpha w$}
\IF{$\|r\|_2<\varepsilon\|b\|_2$}
\STATE{Stop}
\ENDIF
\STATE{$\beta:=\beta\cdot(r,r)$}
\STATE{$p:=r+\beta p$}
\ENDFOR
\end{algorithmic}
\end{algorithm}
\end{itemize}
\end{frame}

% Slide 34
\begin{frame}{Conjugate gradient (CG) method, cont'd\textsubscript{13}}
\begin{itemize}
\item An essential property of the CG method is that of \textbf{optimality}, namely
\begin{theorem}[Optimality of CG iterates]
Let $A$ be SPD and $x_j\in x_0+\mathcal{K}_j(A,r_0)$ denote the CG iterate approximating the solution of $Ax=b$.
Then, $x_j$ \textbf{minimizes the $A$-norm of the error} over the search space, i.e.,\vspace{-.3cm}
\begin{align*}
\|x-x_j\|_A=\min_{y\in x_0+\mathcal{K}_j(A,r_0)}\|x-y\|_A
\;\text{ where }\; 
\|x\|_A:=(Ax,x)^{1/2}.
\end{align*}
\end{theorem}
%Proof left as a homework problem.
\item[] Another important results on the CG method is about its convergence:\vspace{-.1cm}
\begin{theorem}[Upper bound on the relative change of $A$-norm of the error]
Let $A$ be SPD with smallest and largest eigenvalues given by $\lambda_{min}$ and $\lambda_{max}$, respectively.
Then, it holds that\vspace{-.3cm}
\begin{align*}
\frac{\|x_j-x\|_A}{\|x_0-x\|_A}\leq
\left(\frac{\sqrt{\kappa_2(A)}-1}{\sqrt{\kappa_2(A)}+1}\right)^j
\end{align*}
$\vspace{-.4cm}$\\
where $\kappa_2(A)=\lambda_{max}/\lambda_{min}$ is the spectral condition number of $A$.
\end{theorem}
\end{itemize}
\end{frame}

% Slide 35
\begin{frame}{Conjugate gradient (CG) method, cont'd\textsubscript{14}}
\begin{itemize}
\item An alternative presentation of the CG method to that of \textbf{orthogonal projection in a Krylov subspace} is frequent in the field of \textbf{optimization}.
\begin{itemize}\normalsize
\item[-] That is, considering an SPD matrix $A\in\mathbb{R}^{n\times n}$ and a vector $b\in\mathbb{R}^n$, the \textbf{quadratic function}
\begin{align*}
f:
\mathbb{R}^n&\,\rightarrow\mathbb{R}\\
x&\,\mapsto x^TAx-x^Tb
\end{align*} 
has $\nabla f(x)=Ax-b$ and $\nabla^2f(x)=A$ for 1st and 2nd derivatives.
\item[-] Since the Hessian $\nabla^2f$ of $f$ is SPD, the critical point $x_*$ such that $\nabla f(x_*)=0$ ($\implies Ax_*=b$), is a \textbf{minimizer} of the function $f(x)$.
\item[-] An iterative procedure started with $x_0$ and aimed at finding $x_*$ is devised upon setting a set of \textbf{search directions} $p_0,p_1,p_2,\dots$, in the span of which subsequent approximations $x_1,x_2,\dots$ of $x_*$ are formed:
\begin{align*}
x_j:=\sum_{i=0}^j\alpha_ip_i.
\end{align*}
\end{itemize}
\end{itemize}
\end{frame}

% Slide 36
\begin{frame}{Conjugate gradient (CG) method, cont'd\textsubscript{15}}
\begin{itemize}
\item[]
\begin{itemize}\normalsize
\item[-] The search directions are chosen to be $A$\textbf{-orthogonal}, or \textbf{conjugate}, i.e., such that $(Ap_i,p_j)=0$ for $i\neq j$.
\item[-] The initial \textbf{search direction} is chosen as the \textbf{opposite of the gradient of $f$ at $x_0$}, i.e., $p_0:=-\nabla f(x_0)=b-Ax_0=:r_0$.
\item[-] Subsequent search directions $p_1,p_2,\dots$ being $A$-orthogonal with respect to $p_0\propto \nabla f(x_0)$, they are \textbf{conjugate to the gradient} $\nabla f(x_0)$, hence the name \textbf{conjugate gradient} given to the method.
\end{itemize}
\end{itemize}
\end{frame}

\subsection{Minimal residual (MINRES) method}
% Slide 37
\begin{frame}{Minimal residual (MINRES) method}
\begin{itemize}
\item The \textbf{optimality property} of the CG method is reliant on the \textbf{assumption of positive definiteness} of $A$.
Furthermore, in cases $A$ is \textbf{not positive definite}, the CG method may \textbf{break down} (Paige et al., 1995).
\item[] For cases where $A$ is \textbf{symmetric but indefinite} (still non-singular), then, the minimal residual (MINRES) method (Paige and Saunders, 1975) is introduced as an \textbf{oblique projection in a Krylov subspace} $\mathcal{K}_m(A,r_0)$, with constraints subspace $\mathcal{L}_m:=A\mathcal{K}_m$, i.e., similarly as GMRES, it reads
\begin{align}\label{eq:minres}
\boxed{\text{Find }x_m\in x_0+\mathcal{K}_m(A,r_0)\text{ such that }
b-Ax_m\perp A\mathcal{K}_m(A,r_0)}\,,
\end{align}
the difference with GMRES being that $A$ is symmetric.
\item[] Assuming that the columns of $V_m:=[v_1,\dots,v_m]$ form a basis of the Krylov subspace $\mathcal{K}_m(A,r_0)$, the MINRES iterate is then given as follows from the \textbf{Petrov-Galerkin condition}:
\begin{align*}
x_m:=x_0+((AV_m)^TAV_m)^{-1}(AV_m)^Tr_0.
\end{align*}
\end{itemize}
\tiny{Paige, C. C., Parlett, B. N., \& Van der Vorst, H. A. (1995). Approximate solutions and eigenvalue bounds from Krylov subspaces. Numerical linear algebra with applications, 2(2), 115-133.}\tinyskip\\
\tiny{Paige, C. C. \& Saunders, M. A. (1975). Solution of sparse indefinite systems of linear equations. SIAM
Journal on Numerical Analysis, 12, 617–629.}
\end{frame}

% Slide 38
\begin{frame}{Minimal residual (MINRES) method, cont'd\textsubscript{1}}
\begin{itemize}
\item However, similarly as for GMRES, it is more common and practical to derive the GMRES iterate based on the following \textbf{optimality property}:
\begin{theorem}[Optimality of MINRES iterates]
The iterate $x_m$ is the solution of Pb.~\eqref{eq:minres} if and only if it minimizes the residual norm $\|b-Ax\|_2$ over the affine subspace $x_0+\mathcal{K}_m(A,r_0)$, i.e., iff\vspace{-.2cm}
\begin{align*}
\|b-Ax_m\|_2=
\min_{x\in x_0+\mathcal{K}_m(A,r_0)}\|b-Ax\|_2.
\end{align*}
\end{theorem}
Consequently, the MINRES iterate $x_m\in x_0+\mathcal{K}_m(A,r_0)$ is given by $\boxed{x_m:=x_0+V_m\tilde{y}}\,$, where
\begin{align*}
\tilde{y}:=\arg\min_{y\in\mathbb{R}^m}\|r_0-AV_my\|_2
\end{align*}
in which, we recall that $r_0=\beta v_1$, where $\beta:=\|r_0\|_2$ and, as the Lanczos relation reads $AV_m=V_{m+1}\underline{T_m}$ in which $\underline{T_m}:=V_{m+1}^TAV_m$, we obtain
\begin{align*}
\tilde{y}=\arg\min_{y\in\mathbb{R}^m}\|\beta e_1^{(m+1)}-\underline{T_m}y\|.
\end{align*}
\end{itemize}
\smallskip
\tiny{Bai, Z. Z., \& Pan, J. Y. (2021). Matrix analysis and computations. Society for Industrial and Applied Mathematics.}
\end{frame}

% Slide 39
\begin{frame}{Minimal residual (MINRES) method, cont'd\textsubscript{2}}
\begin{itemize}
\item Just as with GMRES, the least-squares problem $\min_{y\in\mathbb{R}^m}\|\beta e_1^{(m+1)}\!\!-\!\underline{T_m}y\|_2\hspace{-1cm}$\\
can be solved using the QR decomposition of the tridiagonal matrix.
\item[] Let $Q_{m+1}\in\mathbb{R}^{(m+1)\times(m+1)}$ be the orthogonal matrix s.t. $\underline{T_m}=Q_{m+1}^T\underline{R_m}$, where $\underline{R_m}\in\mathbb{R}^{(m+1)\times m}$ is an upper-triangular matrix.
\item[] Since $\underline{T_m}$ is tridiagonal, the upper-triangular matrix $\underline{R_m}$ is banded with a bandwidth of 3, i.e., we have
\begin{align*}
\underline{R_m}=
\begin{bmatrix}
\tau_1^{(1)}&\tau_1^{(2)}&\tau_1^{(3)}\\
0&\tau_2^{(1)}&\tau_2^{(2)}&\tau_2^{(3)}\\
\vdots&\ddots&\ddots&\ddots&\ddots\\
\vdots&&\ddots&\tau_{m-2}^{(1)}&\tau_{m-2}^{(2)}&\tau_{m-2}^{(3)}\\
\vdots&&&\ddots&\tau_{m-1}^{(1)}&\tau_{m-1}^{(2)}\\
\vdots&&&&\ddots&\tau_{m}^{(1)}\\
0&\dots&\dots&\dots&\dots&0
\end{bmatrix}
=
\begin{bmatrix}
R_m\\
0_{1\times m}
\end{bmatrix}
\end{align*}
where $R_m:=\underline{R_{m}}[1\!:\!m,1\!:\!m]$.
\end{itemize}
\end{frame}

% Slide 40
\begin{frame}{Minimal residual (MINRES) method, cont'd\textsubscript{3}}
\begin{itemize}
\item[] The least-squares problem is recast into
\begin{align*}
\min_{y\in\mathbb{R}^m}\|\beta e_1^{(m+1)}-\underline{T_{m}}y\|_2
=\min_{y\in\mathbb{R}^m}\left\|\beta q_1-\begin{bmatrix}R_m\\0_{1\times m}\end{bmatrix}y\right\|_2
\end{align*}
where $q_1:=Q_{m+1}e_1^{(m+1)}=Q_{m+1}[1:m+1,1]$.
\item[] Then, as we let $\underline{g_m}:=\beta q_1\in\mathbb{R}^{m+1}$ with $\underline{g_0}:=\beta$, the least-squares problem is solved by solving the following triangular system:
\begin{align*}
\boxed{R_m\tilde{y}=\underline{g_m}[1:m]}\,.
\end{align*}
\item[] Then, the residual $r_m:=b-Ax_m$ is s.t. $r_m=V_{m+1}(\beta e_1^{(m+1)}-\underline{T_m}\tilde{y})$ and $\boxed{\|r_m\|_2=\beta|q_1[m+1]|=|\underline{g_m}[m+1]|}\,$.
\item[] Thus, one needs not to assemble the iterate $x_m$, nor to perform an additional matrix-vector product in order to monitor convergence.
\end{itemize}
%\smallskip
%\tiny{Bai, Z. Z., \& Pan, J. Y. (2021). Matrix analysis and computations. Society for Industrial and Applied Mathematics.}
\end{frame}

% Slide 41
\begin{frame}{Minimal residual (MINRES) method, cont'd\textsubscript{4}}
\begin{itemize}
\item[] Suppose that we have obtained the QR decomposition of the matrix $\underline{T_{j-1}}$, and we are interested in getting the decomposition of $\underline{T_j}$ with the least amount of work possible.
Clearly, we have
\begin{align*}
\underline{T_j}=
\begin{bmatrix}
\underline{T_{j-1}}&t_{1:j,j}\\
0_{1\times j-1}&\beta_{j}
\end{bmatrix}
\text{ where }
t_{1:j,j}=\begin{bmatrix}0_{(j-2)\times 1}\\\beta_{j-1}\\\alpha_j\end{bmatrix}.
\end{align*}
We saw in Lecture 07 that Givens rotations can be used to turn an upper Hessenberg matrix into triangular form.
In particular, for $\underline{T_{j-1}}$, we have
\begin{align*}
\underline{R_{j-1}}=
\begin{bmatrix}
R_{j-1}\\
0_{1\times (j-1)}
\end{bmatrix}=
G_{j-1}^{(j)}G^{(j)}_{j-2}\dots G^{(j)}_1\underline{T_{j-1}}=Q_j\underline{T_{j-1}}
\end{align*}
where the Givens rotation matrix $G^{(j)}_i\in\mathbb{R}^{j\times j}$ zeroes the $(i+1,i)$-entry of the tridiagonal matrix it is applied to.
Also, we have 
\begin{align*}
G^{(j+1)}_i=
\begin{bmatrix}
G^{(j)}_i&0_{j\times 1}\\
0_{1\times j}&1
\end{bmatrix}
\text{ for }
i=1,\dots,j-1.
\end{align*}
\end{itemize}
%$\vspace{-.15cm}$\\
%\tiny{Bai, Z. Z., \& Pan, J. Y. (2021). Matrix analysis and computations. Society for Industrial and Applied Mathematics.}
\end{frame}

% Slide 42
\begin{frame}{Minimal residual (MINRES) method, cont'd\textsubscript{5}}
\begin{itemize}
\item[]As we had for GMRES, we have that $\underline{R_{j}}$ can be formed through minimal update of $\underline{R_{j-1}}$, i.e.,
\begin{align*}
\underline{R_j}
=\begin{bmatrix}
\underline{R_{j-1}}&G_j^{(j+1)}[1:j,1:j+1]\begin{bmatrix}G_{j-1}^{(j)}\dots G_1^{(j)}t_{1:j,j}\\\beta_{j}\end{bmatrix}\\
0_{1\times(j-1)}&0
\end{bmatrix}.
\end{align*}
Therefore, while performing the $j$-th iteration of MINRES, one is equipped with $\underline{R_{j-1}}$ and $\underline{T_j}$.
In order to assemble $\underline{R_j}$, there only remains to apply the Givens rotations $G_1^{(j+1)},\dots,G_j^{(j+1)}$ to the last column of $\underline{T_j}$, i.e.,\vspace{-.15cm}
\begin{align*}
\underline{R_j}[1:j+1,j]=G_j^{(j+1)}\dots G_1^{(j+1)}t_{1:j+1,j}.
\end{align*}
But, since $t_{1:j-2,j}=0_{(j-2)\times 1}$, this simplifies to
\begin{align*}
\boxed{\underline{R_j}[1:j+1,j]=G_j^{(j+1)}G_{j-1}^{(j+1)}G_{j-2}^{(j+1)}t_{1:j+1,j}
\;\text{ when }\;
j>2}\,.
\end{align*}
\end{itemize}
%\smallskip
%\tiny{Bai, Z. Z., \& Pan, J. Y. (2021). Matrix analysis and computations. Society for Industrial and Applied Mathematics.}
\end{frame}

% Slide 43
\begin{frame}{Minimal residual (MINRES) method, cont'd\textsubscript{6}}
\begin{itemize}
\item We recall that the MINRES iterate is given by $x_j:=x_0+V_j\tilde{y}$, where 
\begin{align*}
R_j\tilde{y}=\underline{g_j}[1:j],
\end{align*}
so that, for $j=1,\dots,m$, we have $x_j=x_0+P_j\underline{g_j}[1:j]$, in which $P_j=[p_1,\dots,p_j]:=V_jR_j^{-1}$.
But since $R_j$ has a bandwidth of 3, we get
\begin{align*}
p_1=&\,v_1/\tau_1^{(1)},\;
p_2=(v_2-\tau_1^{(2)}p_1)/\tau_2^{(1)}\\
p_j=&\,(v_j-\tau_{j-1}^{(2)}p_{j-1}-\tau_{j-2}^{(3)}p_{j-2})/\tau_j^{(1)}
\;\text{ for }\;j=3,4,\dots,m
\end{align*}
so that the columns of $P_j$ are an accessible by-product of the MINRES iteration.
Finally, since $\underline{g_j}[1:j-1]=\underline{g_{j-1}}[1:j-1]$, we have
\begin{align*}
x_j=x_0+P_j\underline{g_j}[1:j]
=&\,x_0 + [P_{j-1}\,p_j]\begin{bmatrix}\underline{g_{j}}[1:j-1]\\\underline{g_j}[j]\end{bmatrix}\\
=&\,x_0+P_{j-1}\underline{g_{j-1}}[1:j-1]+\underline{g_j}[j]p_j
\end{align*}
so that $\boxed{x_j=x_{j-1}+\underline{g_j}[j]p_j}\,$.
\end{itemize}
\end{frame}

% Slide 44
\begin{frame}{Minimal residual (MINRES) method, cont'd\textsubscript{7}}
\begin{itemize}
\item In practice, the $\underline{R_1},\dots,\underline{R_m}$ and $\underline{
g_1},\dots,\underline{g_m}$ can be computed in-place, stored in pre-allocated $\underline{T_m}$ and $\underline{g_m}$.
This yields the following algorithm\vspace{-.34cm}
\begin{algorithm}[H]
\footnotesize
\caption{MINRES$:(x_0,m,\varepsilon)\mapsto x_j$}
\begin{algorithmic}[1]
\STATE{\color{gray}{// Allocate $\underline{T}\in\mathbb{R}^{(m+1)\times m}$, $\underline{g}\in\mathbb{R}^{m+1}$}}
\STATE{$r_0:=b-Ax_0;$ $\beta:=\|r_0\|_2;$ $v_1:=r_0/\beta;$ $\underline{g}:=[\beta,0,\dots,0]^T$}
\FOR{$j=1,2\dots$}
\STATE{\color{gray}{// Perform Lanczos iteration}}
\STATE{$w_j:=Av_j-\beta_{j-1}v_{j-1}$ where $\beta_0:=0$ and $v_0:=0$}
\STATE{$\alpha_j:=(w_j,v_j);$ $w_j:=w_j-\alpha_jv_j;$ $\beta_j:=\|w_j\|_2$}
\STATE{\color{gray}{// Apply $G_{j-2}^{(j+1)}$ to $t_{1:j+1,j}$.}}
\IF{$j>2$}
\STATE{
$\begin{bmatrix}t_{j-2,j}\\t_{j-1,j}\end{bmatrix}
:=
\begin{bmatrix}c&s\\-s&c\end{bmatrix}
\begin{bmatrix}t_{j-2,j}\\t_{j-1,j}\end{bmatrix}$
where
$\begin{matrix}
\begin{cases}
s:=t_{j-1,j-2}/(t_{j-2,j-2}^2+t_{j-1,j-2}^2)^{1/2}\\
c:=t_{j-2,j-2}/(t_{j-2,j-2}^2+t_{j-1,j-2}^2)^{1/2}
\end{cases}
\end{matrix}$}
\ENDIF
\STATE{\color{gray}{// Apply $G_{j-1}^{(j+1)}$ to $t_{1:j+1,j}$.}}
\IF{$j>1$}
\STATE{
$\begin{bmatrix}t_{j-1,j}\\t_{jj}\end{bmatrix}
:=
\begin{bmatrix}c&s\\-s&c\end{bmatrix}
\begin{bmatrix}t_{j-1,j}\\t_{jj}\end{bmatrix}$
where
$\begin{matrix}
\begin{cases}
s:=t_{j,j-1}/(t_{j-1,j-1}^2+t_{j,j-1}^2)^{1/2}\\
c:=t_{j-1,j-1}/(t_{j-1,j-1}^2+t_{j,j-1}^2)^{1/2}
\end{cases}
\end{matrix}$}
\ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}
\end{itemize}
\end{frame}

% Slide 45
\begin{frame}{Minimal residual (MINRES) method, cont'd\textsubscript{8}}
\begin{itemize}
\vspace{-.1cm}
\item In practice, the $\underline{R_1},\dots,\underline{R_m}$ and $\underline{g_1},\dots,\underline{g_m}$ can be computed in-place, stored in pre-allocated $\underline{T_m}$ and $\underline{g_m}$.
This yields the following algorithm\vspace{-.34cm}
\setcounter{algorithm}{5}
\begin{algorithm}[H]
\footnotesize
\caption{\textbf{cont'd} MINRES$:(x_0,m,\varepsilon)\mapsto x_j$}
\begin{algorithmic}[1]
\setcounter{ALC@line}{11}
\STATE{\hspace{\algorithmicindent}\color{gray}{// Apply $G_{j}^{(j+1)}$ to $\underline{g}[1:j+1]$ and $t_{1:j+1,j}$}}
\STATE{\hspace{\algorithmicindent}
$\begin{bmatrix}\underline{g}[j]\\\underline{g}[j+1]\end{bmatrix}
:=
\begin{bmatrix}c&s\\-s&c\end{bmatrix}
\begin{bmatrix}\underline{g}[j]\\0\end{bmatrix}$
where
$\begin{matrix}
\begin{cases}
s:=t_{j+1,j}/(t_{jj}^2+t_{j+1,j}^2)^{1/2}\\
c:=t_{jj}/(t_{jj}^2+t_{j+1,j}^2)^{1/2}
\end{cases}
\end{matrix}$}
\STATE{\hspace{\algorithmicindent}$t_{jj}:=ct_{jj}+st_{j+1,j};$ $t_{j+1,j}:=0$}
\STATE{\hspace{\algorithmicindent}$p_j:=(v_j-\tau_{j-1}^{(2)}p_{j-1}-\tau_{j-2}^{(3)}p_{j-2})/\tau_{j}^{(1)}$ where $p_0:=0$ and $p_{-1}:=0$}
\STATE{\hspace{\algorithmicindent}$x_j:=x_{j-1}+\underline{g}[j]p_j$}
\STATE{\hspace{\algorithmicindent}\textbf{if} $|\underline{g}[j+1]|<\varepsilon\|b\|_2$ \textbf{then} Stop}
\COMMENT{\hspace{\algorithmicindent}Stop if $\|r_j\|_2<\varepsilon\|b\|_2$}
\STATE{\hspace{\algorithmicindent}$v_{j+1}:=w_j/\beta_j$}
\end{algorithmic}
\end{algorithm}
\end{itemize}
\end{frame}

\subsection{SYMMLQ method}
% Slide 46
\begin{frame}{SYMMLQ method}
\begin{itemize}
\item The SYMMLQ method (Paige and Saunders, 1975) is an \textbf{orthogonal projection} in a Krylov subspace $\mathcal{K}_m(A,r_0)$ where $A$ is \textbf{symmetric}, possibly \textbf{indefinite}.
Thus, equivalently to the CG method, it sums up to
\begin{align*}
\boxed{
\text{Find }x_m\in x_0+\mathcal{K}_m(A,r_0)
\text{ such that }
b-Ax_m\perp\mathcal{K}_m(A,r_0)
}\,.
\end{align*}
Assuming that the columns of $V_m:=[v_1,\dots,v_m]$ form a basis of the Krylov subspace $\mathcal{K}_m(A,r_0)$, the SYMMLQ iterate is given by
\begin{align*}
x_m:=x_0+V_mT_m^{-1}V_m^Tr_0
\end{align*}
where $T_m:=V_m^TAV_m$ is the tridiagonal matrix of a Lanczos procedure.
\item[] The main difference with CG stems from the assumed factorization of $T_m$.
\item[] While CG assumes that $T_m$ admits an \textbf{LU} factorization \textbf{without pivoting} (not guaranteed to exist for an indefinite $A$), the SYMMLQ method relies on a \textbf{LQ} decomposition of $T_m$ (guaranteed to exist for all non-singular $A$).
\item[] That is, we search for the lower-triangular $\tilde{L}_m\in\mathbb{R}^{m\times m}$ and an orthogonal $Q_m\in\mathbb{R}^{m\times m}$ such that $T_m=\tilde{L}_mQ_m$.
\end{itemize}\smallskip
\tiny{Paige C. C. \& Saunders M. A. (1975). Solution of sparse indefinite systems of linear equations. SIAM
Journal on Numerical Analysis, 12, 617–629.}
\end{frame}

% Slide 47
\begin{frame}{SYMMLQ method, cont'd\textsubscript{1}}
\begin{itemize}
\item Given an LQ decomposition of the tridiagonal matrix $T_j$, the SYMMLQ iterate can be recast into
\begin{align*}
x_j=x_0+\tilde{P}_j\tilde{z}^{(j)}
\;\text{ where }\;
\tilde{L}_j\tilde{z}^{(j)}=\beta e_1^{(j)}
\;\text{ and }\;
\tilde{P}_j:=V_jQ_j^T.
\end{align*}
Since $T_j$ is tridiagonal, it is also Hessenberg, and its LQ decomposition can be constructed through the application of \textbf{Givens rotations}:\vspace{-.1cm}
\begin{align*}
\tilde{L}_j=T_jG_1^{(j)}\dots G_{j-1}^{(j)}
\;\text{ so that }\;Q_j=\left(G_1^{(j)}\dots G_{j-1}^{(j)}\right)^T.
\end{align*}
Since $T_j$ is tridiagonal, $\tilde{L}_j$ is banded with a bandwidth of 3.
\item[] Let $\tilde{Q}_{j+1}:=\begin{bmatrix}Q_j&0_{j\times 1}\\0_{1\times j}&1\end{bmatrix}$.
Then, we have
\begin{align*}
G_j^{(j+1)}{}^T\tilde{Q}_{j+1}=
G_j^{(j+1)}{}^T
\begin{bmatrix}
G_{j-1}^{(j)}{}^T\dots G_{1}^{(j)}{}^T&0_{j\times 1}\\
0_{1\times j}&1
\end{bmatrix}
\end{align*}
\end{itemize}
\end{frame}

% Slide 48
\begin{frame}{SYMMLQ method, cont'd\textsubscript{2}}
\begin{itemize}
\item[]\vspace{-.5cm}
\begin{align*}
G_j^{(j+1)}{}^T\tilde{Q}_{j+1}=&\,
G_j^{(j+1)}{}^T
\begin{bmatrix}
G_{j-1}^{(j)}{}^T&0_{j\times 1}\\
0_{1\times j}&1
\end{bmatrix}
\dots
\begin{bmatrix}
G_{1}^{(j)}{}^T&0_{j\times 1}\\
0_{1\times j}&1
\end{bmatrix}\\
=&\,
G_j^{(j+1)}{}^T
G_{j-1}^{(j+1)}{}^T
\dots
G_1^{(j+1)}{}^T
\end{align*}
so that $G_j^{(j+1)}{}^T\tilde{Q}_{j+1}=Q_{j+1}$.
Then, we have
\begin{align*}
T_{j+1}Q_{j+1}^T
=&\,
T_{j+1}\tilde{Q}_{j+1}^TG_j^{(j+1)}\\
=&\,
\begin{bmatrix}T_j&t_{1:j,j+1}\\t_{j+1,1:j}&\alpha_{j+1}\end{bmatrix}
\begin{bmatrix}Q_j^T&0_{j\times 1}\\0_{1\times j}&1\end{bmatrix}
G_j^{(j+1)}\\
=&\,
\begin{bmatrix}T_jQ_j^T&t_{1:j,j+1}\\t_{j+1,1:j}Q_j^T&\alpha_{j+1}\end{bmatrix}
G_j^{(j+1)}
\end{align*}
where\vspace{-.4cm}
\begin{align*}
t_{j+1,1:j}Q_j^T
=&\,[0_{1\times(j-1)}\;\beta_j]G_1^{(j+1)}\dots G_{j-1}^{(j+1)}\\
=&\,[0_{1\times(j-1)}\;\beta_j]G_{j-1}^{(j+1)}\\
=&\,[0_{1\times (j-2)}\;-\!s_{j-1}\beta_j\;c_j\beta_j].
\end{align*}
\end{itemize}
\end{frame}

% Slide 49
\begin{frame}{SYMMLQ method, cont'd\textsubscript{3}}
\begin{itemize}
\item[] We can see that the application of $G_{j}^{(j+1)}$ to the right of $T_{j+1}\tilde{Q}_{j+1}$:
\begin{itemize}\normalsize
\item[-] zeroes the only non-zero component over the diagonal in the last column of $T_{j+1}\tilde{Q}_{j+1}$;
\item[-] modifies the $(j+1,j)$-entry of $T_{j+1}\tilde{Q}_{j+1}$;
\item[-] modifies the $(j,j)$-entry of $(T_{j+1}Q_{j+1}^T)[1:j,1:j]=T_jQ_j^T=\tilde{L}_j$.
\end{itemize}
Consequently, the components of $\tilde{L}_j$ can be denoted as follows:
{\footnotesize
\begin{align*}
\tilde{L}_j=
\begin{bmatrix}
\ell_1^{(1)}&0&\dots&\dots&\dots&0\\
\ell_2^{(2)}&\ell_2^{(1)}&\ddots&&&\vdots\\
\ell_3^{(3)}&\ell_3^{(2)}&\ell_3^{(1)}&\ddots&&\vdots\\
0&\ddots&\ddots&\ddots&\ddots&\vdots\\
\vdots&\ddots&\ell_{j-1}^{(3)}&\ell_{j-1}^{(2)}&\ell_{j-1}^{(1)}&0\\
0&\dots&0&\ell_j^{(3)}&\ell_{j}^{(2)}&\tilde{\ell}_j^{(1)}
\end{bmatrix}
\end{align*}}
\item[]where the $\tilde{\hphantom{x}}$ over $\tilde{L}_j$ marks the difference with $L_j:=\tilde{L}_{j+1}[1:j,1:j]$.
\item[] That is, only the $(j,j)$-entry differ between $\tilde{L}_j$ and $L_j$.
\end{itemize}
\end{frame}

% Slide 50
\begin{frame}{SYMMLQ method, cont'd\textsubscript{4}}
\begin{itemize}
\item Let us introduce $z^{(j)}\in\mathbb{R}^j$ such that
\begin{align*}
L_jz^{(j)}=\beta e_1^{(j)},
\end{align*}
which differs only in its last entry from $\tilde{z}^{(j)}$, which we previously introduced as the solution of $\tilde{L}_j\tilde{z}^{(j)}=\beta e_1^{(j)}$.
\item[] That is, we have
\begin{align*}
z^{(j)}=\begin{bmatrix}z^{(j-1)}\\z_j\end{bmatrix}
\;\text{ and }\;
\tilde{z}^{(j)}=\begin{bmatrix}z^{(j-1)}\\\tilde{z}_j\end{bmatrix}
\end{align*}
where $z^{(j-1)}$ is the solution of $L_{j-1}z^{(j-1)}=\beta e_1^{(j-1)}$.
\item[] Given that $L_j$ and $\tilde{L}_j$ are both lower-triangular and differ from each other only in their $(j,j)$-entry, we have 
\begin{align*}
\tilde{z}_j=\ell_j^{(1)}z_j/\tilde{\ell}^{(1)}_j
\end{align*}
where $\ell^{(1)}_j=L_j[j,j]$ and $\tilde{\ell}^{(1)}_j=\tilde{L}_j[j,j]$.
\end{itemize}
\end{frame}

% Slide 51
\begin{frame}{SYMMLQ method, cont'd\textsubscript{5}}
\begin{itemize}
\item It follows from $L_jz^{(j)}=\beta e_1^{(j)}$ that 
\begin{align*}
\begin{cases}
z_1=\beta/\ell_1^{(1)},\\
z_2=-\ell_2^{(2)}z_1/\ell_2^{(1)},\\
z_j=-\left(\ell_j^{(3)}z_{j-2}+\ell_j^{(2)}z_{j-1}\right)/\ell_j^{(1)}\;\text{ for }\;j=3,4,\dots,m.
\end{cases}
\end{align*}
Given $\tilde{P}_j=V_jQ_j^T$ and $\tilde{P}_{j+1}=V_{j+1}Q_{j+1}^T$, we introduce
\begin{align*}
P_{j-1}:=\tilde{P}_j[1:n,1:j-1]
\;\text{ and }\;
P_{j}:=\tilde{P}_{j+1}[1:n,1:j],
\end{align*}
and we write $\tilde{P}_j=[P_{j-1}\;\tilde{p}_{j}]$ and $\tilde{P}_{j+1}=[P_{j}\;\tilde{p}_{j+1}]$.
Then, we have
\begin{align*}
\tilde{P}_{j+1}=V_{j+1}Q_{j+1}^T=
[V_j\;v_{j+1}]
\begin{bmatrix}
Q_j^T&0_{j\times 1}\\
0_{1\times j}&1
\end{bmatrix}
G_j^{(j+1)}=
[V_jQ_j^T\;v_{j+1}]G_j^{(j+1)}
\end{align*}
so that
\begin{align*}
\tilde{P}_{j+1}=
[\tilde{P}_j\;v_{j+1}]G_j^{(j+1)}=
[P_{j-1}\;\tilde{p}_{j}\;v_{j+1}]G_j^{(j+1)}.
\end{align*}
\end{itemize}
\end{frame}

% Slide 52
\begin{frame}{SYMMLQ method, cont'd\textsubscript{6}}
\begin{itemize}
\item[] Therefore, we have
\begin{align*}
\tilde{P}_{j+1}[1:n,1:j]=
[P_{j-1}\;(c_j\tilde{p}_j-s_jv_{j+1})]
\end{align*}
so that $P_j=[P_{j+1}\;p_j]$, where\vspace{-.075cm}
\begin{align*}
\begin{cases}
\tilde{p}_1=v_1\\
p_j=c_j\tilde{p}_j-s_jv_{j+1}\\
\tilde{p}_{j+1}=s_j\tilde{p}_j+c_jv_{j+1}
\;\text{ for }\;j=1,2,\dots,m.
\end{cases}
\end{align*}
\item Consider the iterate given by $\tilde{x}_j:=x_0+P_jz^{(j)}$, then we have\vspace{-.075cm}
\begin{align*}
\tilde{x}_j=
x_0+[P_{j-1}\;p_j]\begin{bmatrix}z^{(j-1)}\\z_j\end{bmatrix}=
x_0+P_{j-1}z^{(j-1)}+z_jp_j=
\tilde{x}_{j-1}+z_jp_j.
\end{align*}
The new iterate $x_{j+1}:=x_0+\tilde{P}_{j+1}\tilde{z}^{(j+1)}$ can then be recast as follows:\vspace{-.075cm}
\begin{align*}
x_j=x_0+[P_j\;\tilde{p}_{j+1}]\begin{bmatrix}z^{(j)}\\\tilde{z}_{j+1}\end{bmatrix}=
x_0+P_jz^{(j)}+\tilde{z}_{j+1}\tilde{p}_{j+1}=
\tilde{x}_j+\tilde{z}_{j+1}\tilde{p}_{j+1}
\end{align*}
so that $x_{j+1}$ can be formed effciently from $\tilde{x}_j$.
\end{itemize}
\end{frame}

% Slide 53
\begin{frame}{SYMMLQ method, cont'd\textsubscript{7}}
\begin{itemize}
\item We recall that, as an orthogonal projection in the Krylov subspace $\text{range}(V_j)$, the SYMMLQ iterate is equivalently given by\vspace{-.1cm}
\begin{align*}
x_j=x_0+V_j\tilde{y}
\;\text{ where }\;
T_j\tilde{y}=\beta e_1^{(j)}.
\end{align*}
But since $A$, and thus $T_j$ are symmetric, we have\vspace{-.1cm}
\begin{align*}
T_j^T\tilde{y}=&\,\beta e_1^{(j)}\\
(\tilde{L}_jQ_j)^T\tilde{y}=&\,\beta e_1^{(j)}\\
Q_j^T\tilde{L}_j^T\tilde{y}=&\,\beta e_1^{(j)}\\
\tilde{L}_j^T\tilde{y}=&\,\beta Q_je_1^{(j)}.
\end{align*}
$\vspace{-.5cm}$\\
By comparing the last entries on both sides of $\tilde{L}_j^T\tilde{y}=\beta Q_je_1^{(j)}$, we have\vspace{-.1cm}
\begin{align*}
e_j^{(j)}{}^T\tilde{L}_j^T\tilde{y}=&\,\beta e_j^{(j)}{}^TQ_je_1^{(j)}\\
\tilde{\ell}_j^{(1)}(e_j^{(j)}\tilde{y})=&\,\beta e_j^{(j)}{}^T(G_1^{(j)}\dots G_{j-1}^{(j)})^Te_1^{(j)}\\
=&\,\beta (G_1^{(j)}\dots G_{j-1}^{(j)}e_j^{(j)})^Te_1^{(j)}
\end{align*}
\end{itemize}
\end{frame}

% Slide 54
\begin{frame}{SYMMLQ method, cont'd\textsubscript{8}}
\begin{itemize}
\item[]so that
\begin{align}\label{eq:ell}
\tilde{\ell}_j^{(1)}(e_j^{(j)}\tilde{y})=\beta s_1s_2\dots s_{j-1}.
\end{align}
Also, by construction of $G_j^{(j+1)}$, it can be shown that $s_j\tilde{\ell}_j^{(1)}+c_j\beta_j=0$.
\item[] Then, recalling the Lanczos relation, i.e., $AV_j=V_jT_j+\beta_j v_{j+1}e_j^{(j)}{}^T$, the SYMMLQ residual $r_j:=b-Ax_j$ is recast as follows:
\begin{align*}
\hspace{-.55cm}
r_j\!=r_0-AV_j\tilde{y}=\!
r_0-(V_jT_j+\beta_j v_{j+1}e_j^{(j)}{}^T)\tilde{y}=\!
\beta v_1-V_jT_j\tilde{y}-\beta_j(e_j^{(j)}{}^T\tilde{y})v_{j+1}
\end{align*}
where $T_j\tilde{y}=\beta e_1^{(j)}$, so that
\begin{align*}
r_j=
\beta v_1-\beta V_je_1^{(j)}-\beta_j(e_j^{(j)}{}^T\tilde{y})v_{j+1}=
-\beta_j(e_j^{(j)}{}^T\tilde{y})v_{j+1}
\end{align*}
in which we use Eq.~\eqref{eq:ell} to obtain
\begin{align*}
r_j=
-\left(\beta s_1\dots s_{j-1}/\tilde{\ell}^{(1)}_j\right)v_{j+1}=
\left(\beta s_1\dots s_{j}/c_j\right)v_{j+1}.
\end{align*}
\end{itemize}
\end{frame}

% Slide 55
\begin{frame}{SYMMLQ method, cont'd\textsubscript{9}}
\begin{itemize}
\item[]Then, as we have 
\begin{align*}
\|r_{j-1}\|_2=|\beta s_1\dots s_{j-1}/c_{j-1}|
\;\text{ and }\;
\|r_j\|_2=|\beta s_1\dots s_{j}/c_j|
\end{align*}
so that
\begin{align*}
\|r_j\|_2=\left|\frac{c_{j-1}s_j}{c_j}\right|\|r_{j-1}\|_2.
\end{align*} 
Thus, the convergence of SYMMLQ can be monitored without forming the iterate $x_j$, or even solve the tridiagonal system for $\tilde{y}$, neither forming $r_j$ nor computing its vector norm.
\end{itemize}
\end{frame}

% Slide 56
\begin{frame}{SYMMLQ method, cont'd\textsubscript{10}}
\begin{itemize}
\item Now we are equipped to put the SYMMLQ algorithm together:\vspace{-.3cm}
\begin{algorithm}[H]
\footnotesize
\caption{SYMMLQ$:(x_0,m,\varepsilon)\mapsto x_j$}
\begin{algorithmic}[1]
\STATE{\color{gray}{// Allocate $\underline{T}\in\mathbb{R}^{(m+1)\times m}$, $\underline{g}\in\mathbb{R}^{m+1}$}}
\STATE{$r_0:=b-Ax_0;$ $\beta:=\|r_0\|_2;$ $v_1:=r_0/\beta;$ $\underline{g}:=[\beta,0,\dots,0]^T;$ $\tilde{x}_0:=x_0$}
\FOR{$j=1,2\dots$}
\STATE{\color{gray}{// Perform Lanczos iteration}}
\STATE{$w_j:=Av_j-\beta_{j-1}v_{j-1}$ where $\beta_0:=0$ and $v_0:=0$}
\STATE{$\alpha_j:=(w_j,v_j);$ $w_j:=w_j-\alpha_jv_j;$ $\beta_j:=\|w_j	\|_2$}
\STATE{\textbf{if} $j=1$ \textbf{then} $\tilde{\ell}_j^{(1)}:=\alpha_j$}
\STATE{\color{gray}{// Apply $G_{j-2}^{(j)}$ to the last row of $T_j$}}
\STATE{\textbf{if} $j>2$ \textbf{then} 
$\begin{bmatrix}\ell_j^{(3)}&\beta_{j-1}\end{bmatrix}:=
\begin{bmatrix}0&\beta_{j-1}\end{bmatrix}
\begin{bmatrix}c&s\\-s&c\end{bmatrix}$
where
$\begin{cases}
s:=s_{j-2}\\
c:=c_{j-2}
\end{cases}$}
\STATE{\color{gray}{// Apply $G_{j-1}^{(j)}$ to the last 2 columns of $T_j\tilde{Q}_j$}}
\IF{$j>1$}
\STATE{$\ell_{j-1}^{(1)}:=\sqrt{\left(\tilde{\ell}_{j-1}^{(1)}\right)^2+\beta_{j-1}^2}$}
\STATE{$\begin{bmatrix}\ell_j^{(2)}&\tilde{\ell}_j^{(1)}\end{bmatrix}:=
\begin{bmatrix}\beta_{j-1}&\alpha_j\end{bmatrix}
\begin{bmatrix}c&s\\-s&c\end{bmatrix}$
where
$\begin{cases}
s:=s_{j-1}\\
c:=c_{j-1}
\end{cases}$}
\ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}
\end{itemize}\smallskip
\tiny{Bai, Z. Z., \& Pan, J. Y. (2021). Matrix analysis and computations. Society for Industrial and Applied Mathematics.}
\end{frame}

% Slide 57
\begin{frame}{SYMMLQ method, cont'd\textsubscript{11}}
\begin{itemize}
\vspace{-.1cm}
\item Now we are equipped to put the SYMMLQ algorithm together:\vspace{-.35cm}
\setcounter{algorithm}{6}
\begin{algorithm}[H]
\footnotesize
\caption{\textbf{cont'd} SYMMLQ$:(x_0,m,\varepsilon)\mapsto x_j$}
\begin{algorithmic}[1]
\setcounter{ALC@line}{13}
\STATE{\hspace{\algorithmicindent}\color{gray}{// Compute $z_{j-1}$}}
\STATE{\hspace{\algorithmicindent}\textbf{if} $j=2$ \textbf{then} $z_1:=\beta/\ell_1^{(1)}$}
\STATE{\hspace{\algorithmicindent}\textbf{if} $j=3$ \textbf{then} $z_2:=-\ell_2^{(2)}z_1/\ell_2^{(1)}$}
\STATE{\hspace{\algorithmicindent}\textbf{if} $j>3$ \textbf{then} $z_{j-1}:=-\left(\ell_{j-1}^{(3)}z_{j-3}+\ell_{j-1}^{(2)}z_{j-2}\right)/\ell_{j-1}^{(1)}$}
\STATE{\hspace{\algorithmicindent}\textbf{if} $j=1$ \textbf{then} $\tilde{p}_1:=v_1$}
\STATE{\hspace{\algorithmicindent}\textbf{if} $j>1$ \textbf{then}}
\STATE{\hspace{2\algorithmicindent}$p_{j-1}:=c_{j-1}\tilde{p}_{j-1}-s_{j-1}v_j$}
\STATE{\hspace{2\algorithmicindent}$\tilde{p}_j:=s_{j-1}\tilde{p}_{j-1}+c_{j-1}v_j$}
\STATE{\hspace{2\algorithmicindent}$\underline{g}[j]:=\tilde{x}_{j-2}+z_{j-1}p_{j-1}$}
\STATE{\hspace{2\algorithmicindent}$\underline{g}[j]:=(c_{j-2}s_{j-1}/c_{j-1})\underline{g}[j-1]$ where $c_0:=1$}
\STATE{\hspace{2\algorithmicindent}\textbf{if} $|\underline{g}[j]|>\varepsilon \|b\|_2$ \textbf{then}}
\STATE{\hspace{3\algorithmicindent}$x_{j-1}:=\tilde{x}_{j-2}+\left(\ell_{j-1}^{(1)}/\tilde{\ell}_{j-1}^{(1)}\right)\tilde{p}_{j-1}$}
\STATE{\hspace{3\algorithmicindent}Stop}
\end{algorithmic}
\end{algorithm}
\end{itemize}
\end{frame}

\section{More methods for non-symmetric linear systems}

\subsection{Bi-orthogonalization process}
% Slide 58
\begin{frame}{Bi-orthogonalization process}
\begin{itemize}
\item The \textbf{bi-orthogonalization} process is an extension of the Lanczos procedure to \textbf{non-symmetric matrices}.
\item[] It is sometimes called the \textbf{two-sided Lanczos} procedure.
\item This procedure generates a pair of \textbf{bi-orthogonal} bases in the columns of $V_j=[v_1,\dots,v_j]\in\mathbb{R}^{n\times j}$ and $W_j=[w_1,\dots,w_j]\in\mathbb{R}^{n\times j}$ for Krylov subspaces of $A$ and $A^T$, respectively, i.e., that is, we have
\begin{align*}
\text{range}(V_j)=\mathcal{K}_j(A,r_0)
\;\text{ and }\;
\text{range}(W_j)=\mathcal{K}_j(A^T,\tilde{r}_0)
\end{align*}
such that $V_j^TW_j=W_j^TV_j=I_j$ where $\tilde{r}_0$ is an \textbf{auxiliary vector} used to generate the \textbf{left Krylov subspace} $\mathcal{K}_j(A^T,\tilde{r}_0)$ with $(r_0,\tilde{r}_0)\neq 0$.
\item During the bi-orthogonalization process, instead of forming $v_{j+1}$ by orthonormalizing $Av_j$ against $v_j$ and $v_{j-1}$, it is done by orthonormalizing against $w_j$ and $w_{j-1}$.
\item[] Simultaneously, $w_{j+1}$ is obtained by orthonormalizing $A^Tw_j$ against $v_{j}$ and $v_{j-1}$.
\end{itemize}
\end{frame}

% Slide 59
\begin{frame}{Bi-orthogonalization process, cont'd\textsubscript{1}}
\begin{itemize}
\item The resulting procedure is given by the following algorithm:\vspace{-.2cm}
\begin{algorithm}[H]
%\tiny
\caption{Bi-Orthogonalization$:(r_0,\tilde{r}_0,m)\mapsto (V_m,W_m)$}
\begin{algorithmic}[1]
\STATE{\color{gray}{// $r_0$ and $\tilde{r}_0$ must be such that $(r_,\tilde{r}_0)\neq 0$}}
\STATE{$\beta:=\|r_0\|_2;$ $v_1:=r_0/\beta;$ $w_1:=\beta\tilde{r}_0/(\tilde{r}_0,r_0);$ $\beta_0:=0;$ $\gamma_0:=0$}
\FOR{$j=1,2,\dots,m$}
\STATE{$v_{j+1}:=Av_j-\beta_{j-1}v_{j-1}$ where $v_0:=0$}
\STATE{$w_{j+1}:=A^Tw_j-\gamma_{j-1}w_{j-1}$ where $w_0:=0$}
\STATE{$\alpha_j:=(v_j,w_{j+1})$}
%\COMMENT{$\alpha_j=w_j^TAv_j$}
\STATE{$v_{j+1}:=v_{j+1}-\alpha_jv_j$}
\STATE{$w_{j+1}:=w_{j+1}-\alpha_jw_j$}
\STATE{$\gamma_j:=\sqrt{|(v_{j+1},w_{j+1})|}$}
\STATE{$\beta_j:=(v_{j+1},w_{j+1})/\gamma_j$}
\STATE{$v_{j+1}:=v_{j+1}/\gamma_j$}
\STATE{$w_{j+1}:=w_{j+1}/\beta_j$}
\ENDFOR
\end{algorithmic}
\end{algorithm}
\end{itemize}
\end{frame}

% Slide 60
\begin{frame}{Bi-orthogonalization process, cont'd\textsubscript{2}}
\begin{itemize}
\item We obtain the following \textbf{three-term recurrences} from the last algorithm:\vspace{-.1cm}
\begin{align*}
\boxed{
\begin{cases}
\;\gamma_jv_{j+1}=Av_j-\alpha_jv_j-\beta_{j-1}v_{j-1},\\
\beta_jw_{j+1}=A^Tw_j-\alpha_jw_j-\gamma_{j-1}w_{j-1}\;\text{ for }\;j=2,\dots,m
\end{cases}
}\,.
\end{align*}
\item We can show that the bases stored in the columns of $V_m$ and $W_m$ are orthonormal.
\item[-] For that, we first note that $(v_1,w_1)=(r_0/\beta,\beta\tilde{r}_0/(\tilde{r}_0,r_0))=1$.
\item[-] Then, for $j=1$, we have\vspace{-.175cm}
\begin{align*}
(v_{j+1},w_{j+1})
=&\,(Av_1-\alpha_1v_1,A^Tw_1-\alpha_1w_1)/(\beta_1\gamma_1)\\
=&\,\left((Av_1,A^Tw_1)-\alpha_1(v_1,A^Tw_1)\right)/(\beta_1\gamma_1)\\
&\,-\left(\alpha_1(Av_1,w_1)-\alpha_1^2(v_1,w_1)\right)/(\beta_1\gamma_1)\\
=&\,\left((Av_1,A^Tw_1)-\alpha_1^2-\alpha_1(Av_1,w_1)+\alpha_1^2\right)/(\beta_1\gamma_1)\\
=&\,(Av_1,A^Tw_1-\alpha_1w_1)/(\beta_1\gamma_1)
\end{align*}
$\vspace{-.625cm}$\\
where $\beta_1=(Av_1-\alpha_1v_1,A^Tw_1-\alpha_1w_1)/\gamma_1=(Av_1,A^Tw_1-\alpha_1w_1)/\gamma_1$ so that $(v_{j+1},w_{j+1})=1$.
\end{itemize}
\end{frame}

% Slide 61
\begin{frame}{Bi-orthogonalization process, cont'd\textsubscript{3}}
\begin{itemize}
\item[-] For $j=2,\dots,m$, we have
\begin{align*}
\hspace{-.3cm}
(v_{j+1},w_{j+1})
=&\,(\gamma_jv_{j+1},\beta_jw_{j+1})/(\gamma_j\beta_j)\\
=&\,(Av_j-\alpha_jv_j-\beta_{j-1}v_{j-1},A^Tw_j-\alpha_jw_j-\gamma_{j-1}w_{j-1})/(\gamma_j\beta_j)
\end{align*}
where $\beta_j=(Av_j-\alpha_jv_j-\beta_{j-1}v_{j-1},A^Tw_j-\alpha_jw_j-\gamma_{j-1}w_{j-1})/\gamma_j$ so that $(v_{1},w_{1})=\dots=(v_{m+1},w_{m+1})=1$.
\item There remains to show $(v_i,w_j)=0$ if $i\neq j$.
Let us proceed by induction and show that, for an integer $j$ with $2\leq j\leq m+1$, we have
\begin{align}\label{eq:bi-ortho}
(v_i,w_j)=(v_j,w_i)=0
\;\text{ for }\;i=1,\dots,j-1
\end{align}
\item[-] For $j=2$, we have
\begin{align*}
\hspace{-.5cm}
(v_1,w_2)=(v_1,A^Tw_1-\alpha_1w_1)/\beta_1=&\,\left((v_1,A^Tw_1)-\alpha_1(v_1,w_1)\right)/\beta_1\\
=&\,(\alpha_1-\alpha_1)/\beta_1=0
\end{align*}
and\vspace{-.2cm}
\begin{align*}
\hspace{-.55cm}
(v_2,w_1)=(Av_1-\alpha_1v_1,w_1)/\gamma_1=&\left((Av_1,w_1)-\alpha_1(v_1,w_1)\right)/\gamma_1\\
=&\left((v_1,A^Tw_1)-\alpha_1\right)/\gamma_1\!=(\alpha_1-\alpha_1)/\gamma_1=0.
\end{align*}
\end{itemize}
\end{frame}

% Slide 62
\begin{frame}{Bi-orthogonalization process, cont'd\textsubscript{4}}
\begin{itemize}
\item[-] Suppose that Eq:~\eqref{eq:bi-ortho} holds for $j$, then we need to show that
\begin{align*}
(v_i,w_{j+1})=(v_{j+1},w_i)=0
\;\text{ for }\;
i=1,\dots,j.
\end{align*}
\item[] First, we have\vspace{-.12cm}
\begin{align*}
\hspace{-.3cm}
\alpha_j=(v_j,A^Tw_j-\gamma_{j-1}w_{j-1})=(v_j,A^Tw_j)-\gamma_{j-1}(v_j,w_{j-1})=(v_j,A^Tw_j).
\end{align*}
We also have\vspace{-.12cm}
\begin{align*}
(v_j,w_{j+1})
=&\,(v_j,A^Tw_j-\alpha_jw_j-\gamma_{j-1}w_{j-1})/\beta_j\\
=&\,\left((v_j,A^Tw_j)-\alpha_j(v_j,w_j)\right)/\beta_j\\
=&\,(\alpha_j-\alpha_j)/\beta_j=0.
\end{align*}
$\vspace{-.65cm}$\\
as well as\vspace{-.12cm}
\begin{align*}
(v_{j-1},w_{j+1})
=&\,(v_{j-1},A^Tw_j-\alpha_jw_j-\gamma_{j-1}w_{j-1})/\beta_j\\
=&\,\left((v_{j-1},A^Tw_j)-\gamma_{j-1}(v_{j-1},w_{j-1})\right)/\beta_j\\
=&\,\left((Av_{j-1},w_j)-\gamma_{j-1}\right)/\beta_j\\
=&\,\left((\gamma_{j-1}v_j+\alpha_{j-1}v_{j-1}+\beta_{j-2}v_{j-2},w_j)-\gamma_{j-1}\right)/\beta_j\\
=&\,(\gamma_{j-1}-\gamma_{j-1})/\beta_j=0.
\end{align*}
\end{itemize}
\end{frame}

% Slide 63
\begin{frame}{Bi-orthogonalization process, cont'd\textsubscript{5}}
\begin{itemize}
\item[-] and, for $i=1,\dots,j-2$, we get 
\begin{align*}
(v_i,w_{j+1})
=&\,(v_i,A^Tw_j-\alpha_jw_j-\gamma_{j-1}w_{j-1})/\beta_j\\
=&\,(v_i,A^Tw_j)/\beta_j\\
=&\,(Av_i,w_j)/\beta_j\\
=&\,(\gamma_iv_{i+1}+\alpha_iv_i+\beta_{i-1}v_{i-1},w_j)/\beta_j=0.
\end{align*}
\item[-] We have shown that $(v_i,w_{j+1})=0$ for $i=1,\dots,j$. 
\item[] Similarly, we can show that $(v_{j+1},w_i)=0$ for $i=1,\dots,j$, after what the bi-orthonormality of the bases is proven.
\item In the case of the dot product, the stated orthonormality implies 
\begin{align*}
\boxed{
V_m^TW_m=W_m^TV_m=I_m
}\,.
\end{align*}
\end{itemize}
\end{frame}

% Slide 64
\begin{frame}{Bi-orthogonalization process, cont'd\textsubscript{6}}
\begin{itemize}
\item The three-term recurrence formulae can be cast into matrix form as follows:\vspace{-.75cm}\\
\begin{minipage}[t]{.45\textwidth}
\begin{align*}
\boxed{
\begin{aligned}
AV_m
=&\,V_{m+1}\underline{T_m}\\
=&\,V_mT_m+\gamma_mv_{m+1}e_{m}^{(m)}{}^T
\end{aligned}
}
\end{align*}
\end{minipage}
\begin{minipage}[t]{.45\textwidth}
\begin{align*}
\boxed{
\begin{aligned}
A^TW_m
=&\,W_{m+1}\underline{\tilde{T}^T_m}\\
=&\,W_mT_m^T+\beta_mw_{m+1}e_{m}^{(m)}{}^T
\end{aligned}
}
\end{align*}
\end{minipage}
\vspace{.25cm}\\
where the tridiagonal matrices $\underline{T_m}\in\mathbb{R}^{(m+1)\times m}$ and $\underline{\tilde{T}^T_m}\in\mathbb{R}^{(m+1)\times m}$ are given by
\begin{align*}
\underline{T_m}=
\begin{bmatrix}
\alpha_1&\beta_1&&\\
\gamma_1&\ddots&\ddots&\\
&\ddots&\ddots&\beta_{m-1}\\
&&\gamma_{m-1}&\alpha_m\\
&&&\gamma_m
\end{bmatrix}
\;\text{ and }\;
\underline{\tilde{T}^{T}_m}=
\begin{bmatrix}
\alpha_1&\gamma_1&&\\
\beta_1&\ddots&\ddots&\\
&\ddots&\ddots&\gamma_{m-1}\\
&&\beta_{m-1}&\alpha_m\\
&&&\beta_m
\end{bmatrix}
\end{align*}
with $T_m:=\underline{T_m}[1:m,1:m]=\underline{\tilde{T}_m}[1:m,1:m]$.
\end{itemize}
\end{frame}

% Slide 65
\begin{frame}{Bi-orthogonalization process, cont'd\textsubscript{7}}
\begin{itemize}
\item Combining the matrix form of the first three-term recurrence formula with the statement of bi-orthonormality, we obtain:
\begin{align*}
AV_m=&\,V_mT_m+\gamma_mv_{m+1}e_m^{(m)}{}^T\\
W_m^TAV_m=&\,W_m^TV_mT_m+\gamma_mW_m^Tv_{m+1}e_m^{(m)}{}^T\\
W_m^TAV_m=&\,T_m
\end{align*}
where, as for a regular Lanczos procedure, $T_m$ is tridiagonal, although this time not symmetric.
\item In general, neither $\{v_1,\dots,v_m\}$ nor $\{w_1,\dots,w_m\}$ are orthogonal by themselves, i.e., $V_m^TV_m\neq I_m$ and $W_m^TW_m\neq I_m$.
\item The bi-orthogonalization procedure is similar to Arnoldi in that they both apply to non-symmetric matrices.
\item[] The advantage of the bi-orthogonalization method is that relies on short recurrences, unlike Arnoldi, which requires full orthogonalization against all previously formed vectors.
\end{itemize}
\end{frame}

\subsection{Bi-conjugate gradient (BiCG) method}
% Slide 66
\begin{frame}{Bi-conjugate gradient (BiCG) method}
\begin{itemize}
\item The BiCG method (Lanczos, 1952; Fletcher, 1976) is an \textbf{oblique projection} method in a Krylov subspace $\mathcal{K}_m(A,r_0)$, with a left Krylov constraints subspace $\mathcal{L}_m:=\mathcal{K}_m(A^T,\tilde{r}_0)$ and iterates given by
\begin{align*}
\text{Find }x_m\in x_0+\mathcal{K}_m(A,r_0)\text{ such that }
b-Ax_m\perp\mathcal{K}_m(A^T,\tilde{r}_0).
\end{align*}
$\vspace{-.55cm}$\\
\item From a two-sided Lanczos procedure, we get $V_m,W_m\in\mathbb{R}^{n\times m}$ such that
\begin{align*}
\text{range}(V_m)=\mathcal{K}_m(A,r_0)
\;\text{ and }\;
\text{range}(W_m)=\mathcal{K}_m(A^T,\tilde{r}_0)
\end{align*}
$\vspace{-.55cm}$\\
so that $x_m\in x_0+\mathcal{K}_m(A,r_0)$ implies that there exists $\tilde{y}\in\mathbb{R}^m$ such that $x_m=x_0+V_m\tilde{y}$. 
Along with the Petrov-Galerkin condition, this yields
\begin{align*}
W_m^T(b-A(x_0+V_m\tilde{y}))=&\,0\\
W_m^Tr_0-W_m^TAV_m\tilde{y}=&\,0\\
\beta W_m^Tv_1-T_m\tilde{y}=&\,0
\end{align*}
$\vspace{-.55cm}$\\
so that the bi-orthonormality of the bases implies $T_m\tilde{y}=\beta e_1^{(m)}$.
\end{itemize}\tinyskip
\tiny{Lanczos, C. (1952). Solution of systems of linear equations by minimized iterations, Journal of Research of the National Bureau of Standards, 49, 33–53.}\tinyskip\\
\tiny{Fletcher, R. (1976). Conjugate gradient methods for indefinite systems, in “Proceeding of the Dundee Conference on Numerical Analysis 1975”, G. A. Watson (Editor), Lecture Notes in Mathematics, Springer-Verlag, Berlin, 506, pp. 73–89.}
\end{frame}

% Slide 67
\begin{frame}{Bi-conjugate gradient (BiCG) method, cont'd\textsubscript{1}}
\begin{itemize}
\item Analogously to the CG method, we can introduce an LU decomposition with no pivoting of the tridiagonal $T_m$ to derive the BiCG iteration.
\item[] This leads to the following algorithm:\vspace{-.3cm}
\begin{algorithm}[H]
\small
\caption{BiCG$:(x_0,\varepsilon)\mapsto x_j$}
\begin{algorithmic}[1]
\STATE{$r_0:=b-Ax_0$}
\STATE{Pick $\tilde{r}_0$ such that $(r_0,\tilde{r}_0)\neq 0$}
\COMMENT{E.g., $\tilde{r}_0:=r_0$}
\STATE{$p_1:=r_0;$ $\tilde{p}_1:=\tilde{r}_0$}
\FOR{$j=1,2\dots$}
\STATE{$\alpha_j:=(r_{j-1},\tilde{r}_{j-1})/(Ap_j,\tilde{p}_j)$}
\STATE{$x_j:=x_{j-1}+\alpha_jp_j$}
\STATE{$r_j:=r_{j-1}-\alpha_jAp_j$}
\STATE{\textbf{if} $\|r_j\|_2<\varepsilon\|b\|_2$ \textbf{then} Stop}
\STATE{$\tilde{r}_j:=\tilde{r}_{j-1}-\alpha_jA^T\tilde{p}_j$}
\STATE{$\beta_j:=(r_j,\tilde{r}_j)/(r_{j-1},\tilde{r}_{j-1})$}
\STATE{$p_{j+1}:=r_j+\beta_jp_j$}
\STATE{$\tilde{p}_{j+1}:=\tilde{r}_j+\beta_j\tilde{p}_j$}
\ENDFOR
\end{algorithmic}
\end{algorithm}
$\vspace{-.85cm}$\\
\item[] Clearly, if $A$ is SPD and $\tilde{r}_0=r_0$, then the BICG iterates are the same as those from CG.
\end{itemize}
\end{frame}

% Slide 68
\begin{frame}{Bi-conjugate gradient (BiCG) method, cont'd\textsubscript{2}}
\begin{itemize}
\item Six vectors need be allocated for a practical implementation:\vspace{-.25cm}
\begin{algorithm}[H]
\small
\caption{Practical BiCG$:(x_0,\varepsilon)\mapsto x_j$}
\begin{algorithmic}[1]
\STATE{\color{gray}{Allocate memory for} $x,p,\tilde{p},w,r,\tilde{r}\in\mathbb{R}^n$}
\STATE{$r:=b-Ax_0$}
\STATE{Pick $\tilde{r}$ such that $(r,\tilde{r})\neq0$}
\COMMENT{E.g., $\tilde{r}:=r$}
\STATE{$p:=r;$ $\tilde{r}:=\tilde{p}$}
\FOR{$j=1,2\dots$}
\STATE{$w:=Ap$}
\STATE{$\alpha:=(r,\tilde{r})/(w,\tilde{p})$}
\STATE{$\beta:=1/(r,\tilde{r})$}
\STATE{$x:=x+\alpha p$}
\STATE{$r:=r-\alpha w$}
\STATE{\textbf{if} $\|r\|_2<\varepsilon\|b\|_2$ \textbf{then} Stop}
\STATE{$w:=A^T\tilde{p}$}
\STATE{$\tilde{r}:=\tilde{r}-\alpha w$}
\STATE{$\beta:=\beta\cdot(r,\tilde{r})$}
\STATE{$p:=r+\beta p$}
\STATE{$\tilde{p}:=\tilde{r}+\beta\tilde{p}$}
\ENDFOR
\end{algorithmic}
\end{algorithm}
\end{itemize}
\end{frame}

% Slide 69
\begin{frame}{Bi-conjugate gradient (BiCG) method, cont'd\textsubscript{3}}
\begin{itemize}
\item In addition to $Ax=b$, a dual system
\begin{align*}
A^T\tilde{x}=\tilde{b}
\end{align*}
can be solved by BiCG iteration upon setting $\tilde{r}_0:=b\tilde{b}-A^T\tilde{x}_0$ for some initial iterate $\tilde{x}_0$, in which the dual iterate, given by
\begin{align*}
\tilde{x}_j:=\tilde{x}_{j-1}+\alpha_j\tilde{p}_j
\end{align*}
is such that
\begin{align*}
\tilde{x}_j\in\tilde{x}_0+\mathcal{K}_j(A^T,\tilde{r}_0)
\;\text{ with }\;
\tilde{r}_j:=\tilde{b}-A^T\tilde{x}_j\perp \mathcal{K}_j(A,r_0).
\end{align*}
\item Similarly as for CG, we assumed that $T_j$ admits an LU decomposition \textit{without pivoting}.
However, for a general matrix $A$, this may not be true.
\item[] We have also assumed that $T_j$ is \textit{not singular} which also is not guaranteed.
\item Analogously to what we did for the CG method, one can show that the residuals and their duals are orthogonal, while the search directions and their duals are $A$-orthogonal.
That is
\begin{align*}
(r_i,\tilde{r}_j)=0
\;\text{ and }\;
(Ap_i,\tilde{p}_j)=0
\;\text{ for }\;i\neq j.
\end{align*}
\end{itemize}
\end{frame}

\subsection{Quasi-minimal residual (QMR) method}
% Slide 70
\begin{frame}{Quasi-minimal residual (QMR) method}
\begin{itemize}
\item The BiCG method is notoriously unstable (Gutknecht \& Strako\v{s}, 2000) and it often displays irregular convergence behaviors, i.e., no monotone decrease of residual norm, unlike GMRES.
\item The QMR method (Freund \& Nachtigal, 1991) can be viewed as an extension of the GMRES method in the sense that it builds iterates as
\begin{align*}
&\text{Find }\;
x_m\in x_0+\mathcal{K}_m(A,r_0)\\
&\text{such that }\;
\|r_m\|_2:=\|b-Ax_m\|_2=\min_{x\in x_0+\mathcal{K}_m(A,r_0)}\|b-Ax\|_2
\end{align*}
with the important difference that the basis of $\mathcal{K}_m(A,r_0)$ is produced by bi-orthogonalization.
\item[] For a given $V_{m+1}$ such that $\text{range}(V_m)=\mathcal{K}_m(A,r_0)$, similarly as with GMRES, we have
\begin{align*}
\hspace{-.3cm}
r_m:=b-Ax_m=r_0-AV_m\tilde{y}=\beta v_1-V_{m+1}\underline{T_m}\tilde{y}=V_{m+1}(\beta e_1^{m+1}-\underline{T_m}\tilde{y}).
\end{align*}
\end{itemize}\smallskip
\tiny{Gutknecht, M. H. \& Strako\v{s}, Z. (2000). Accuracy of two three-term and three two-term recurrences for Krylov space solvers, SIAM Journal on Matrix Analysis and Applications, 22, 213–229.}\tinyskip\\
\tiny{Freund, R. W. \& Nachtigal, N. M. (1991). QMR: A quasi-minimal residual method for non-Hermitian linear systems, SIAM Journal: Numer. Math. 60, pp. 315–339.}
\end{frame}

% Slide 71
\begin{frame}{Quasi-minimal residual (QMR) method, cont'd\textsubscript{1}}
\begin{itemize}
\item[] The main difference with a basis produced by Arnoldi is that $V_{m+1}$ is \textit{not orthogonal}.
Thus, we are left with
\begin{align*}
\|r_m\|_2=\|V_{m+1}(\beta e_1^{(m+1)}-\underline{T_m}\tilde{y})\|_2
\end{align*}
Although we have
\begin{align*}
\|r_m\|_2\leq\|V_{m+1}\|_2\cdot\|\beta e_1^{(m+1)}-\underline{T_m}\tilde{y}\|_2
\end{align*}
Like in GMRES, we still form the iterate by minimizing $\|\beta e_1^{(m+1)}-\underline{T_m}y\|_2$, which here, is referred to as the \textit{quasi-residual norm}, hence the name of \textit{quasi-minimal residual} method.
\item Because of the tridiagonal structure of $\underline{T_m}$, minimizing the quasi-residual norm is a bit simpler than minimizing the residual norm in GMRES.
\item[] In particular, updating the QR factorization of the tridiagonal requires only up to three applications of Givens rotations.
\end{itemize}
\end{frame}

% Slide 72
\begin{frame}{Quasi-minimal residual (QMR) method, cont'd\textsubscript{2}}
\begin{itemize}
\item The least-squares problem $\min_{y\in\mathbb{R}^j}\|\beta e_1^{(j+1)}-\underline{T_j}y\|_2$ is, once again, solved by making use of a QR decomposition of $\underline{T_j}$. We have\vspace{-.1cm}
\begin{align*}
\underline{R_j}=
\begin{bmatrix}R_j\\0_{1\times j}\end{bmatrix}=
G^{(j+1)}_j\dots G_1^{(j+1)}\underline{T_j}=
G^{(j+1)}_jG^{(j+1)}_{j-1}G_{j-2}^{(j+1)}\underline{T_j}=
Q_{j+1}\underline{T_j}
\end{align*}
and $\underline{g_j}:=\beta Q_{j+1}e_1^{(j+1)}$, so that the least-squares problem is recast in a banded triangular linear system:\vspace{-.1cm}
\begin{align*}
R_j\tilde{y}=\underline{g_j}[1:j]
\end{align*}
where $R_j$ has a bandwidth of three.
$R_j$ and $\underline{g_j}$ are updated as follows, with minimal effort, given $R_{j-1}$ and $\underline{g_{j-1}}$:\vspace{-.1cm}
\begin{align*}
\underline{R_j}=
\begin{bmatrix}
\underline{R_{j-1}}&G_j^{(j+1)}[1:j,1:j+1]\begin{bmatrix}G_{j-1}^{(j)}G_{j-2}^{(j)}t_{1:j,j}\\\beta_j\end{bmatrix}\\
0_{1\times{j-1}}&0
\end{bmatrix}
\end{align*}
so that updating $\underline{R_j}$ boils down to computing\vspace{-.1cm}
\begin{align*}
\underline{R_j}[1:j+1,j]=G_{j}^{(j+1)}G_{j-1}^{(j+1)}G_{j-2}^{(j+1)}t_{1:j+1,j}.
\end{align*}
\end{itemize}
\end{frame}

% Slide 73
\begin{frame}{Quasi-minimal residual (QMR) method, cont'd\textsubscript{3}}
\begin{itemize}
\item[] and $\underline{g_j}$ is updated as follows:
\begin{align*}
\underline{g_j}=
\begin{bmatrix}
\gamma_1\\
\vdots\\
\gamma_{j-1}\\
c_j\gamma_j\\
-s_j\gamma_j
\end{bmatrix}
\;\text{ where }\;
\begin{bmatrix}
\gamma_1\\
\vdots\\
\gamma_{j}
\end{bmatrix}:=\underline{g_{j-1}}
\end{align*}
with\vspace{-.4cm}
\begin{align*}
s_j:=\frac{t_{j+1,j}}{\sqrt{\left(t_{jj}^{(j-1)}\right)^2+t_{j+1,j}^2}}
\;\text{ and }\;
c_j:=\frac{t_{j+1,j}^{(j-1)}}{\sqrt{\left(t_{jj}^{(j-1)}\right)^2+t_{j+1,j}^2}}
\end{align*}
in which $\underline{T_j}^{(j)}:=\underline{R_j}$.
\item Finally, given $R_j\tilde{y}=\underline{g_j}[1:j]$, we obtain
\begin{align*}
\hspace{-.25cm}
r_j=
V_{j+1}(\beta e^{(j+1)}_1-\underline{T_j}\tilde{y})=
V_{j+1}\begin{bmatrix}0_{j\times 1}\\\underline{g_j}[j+1]\end{bmatrix}
\;\text{ so that }\;
\|r_j\|_2=|\underline{g_j}[j+1]|.
\end{align*}
\end{itemize}
\end{frame}

% Slide 74
\begin{frame}{Quasi-minimal residual (QMR) method, cont'd\textsubscript{4}}
\begin{itemize}
\item Finally, the QMR iteration is given as follows:\vspace{-.3cm}
\begin{algorithm}[H]
\footnotesize
\caption{QMR$:(x_0,\varepsilon)\mapsto x_j$}
\begin{algorithmic}[1]
\STATE{\color{gray}{// Allocate $\underline{T}\in\mathbb{R}^{(m+1)\times m}$ and $\underline{g}\in\mathbb{R}^{m+1}$}}
\STATE{$r_0:=b-Ax_0;$ $\beta:=\|r_0\|_2;$ $\underline{g}:=[\beta,0,\dots,0]^T;$ $v_1:=r_0/\beta$}
\STATE{Pick $\tilde{r}_0$ such that $(r_0,\tilde{r}_0)\neq 0$}
\COMMENT{E.g., $\tilde{r}_0:=r_0$}
\STATE{$w_1:=\beta\tilde{r}_0/(r_0,\tilde{r}_0)$}
\FOR{$j=1,2\dots$}
\STATE{Get $v_{j+1}$ and $t_{1:j+1,j}$ from iteration of two-sided Lanczos}
\STATE{\color{gray}{// Apply $G_{j-2}^{(j+1)}$ to $t_{1:j+1,j}$.}}
\IF{$j>2$}
\STATE{
$\begin{bmatrix}t_{j-2,j}\\t_{j-1,j}\end{bmatrix}
:=
\begin{bmatrix}c&s\\-s&c\end{bmatrix}
\begin{bmatrix}t_{j-2,j}\\t_{j-1,j}\end{bmatrix}$
where
$\begin{cases}
s:=t_{j-1,j-2}/(t_{j-2,j-2}^2+t_{j-1,j-2}^2)^{1/2}\\
c:=t_{j-2,j-2}/(t_{j-2,j-2}^2+t_{j-1,j-2}^2)^{1/2}
\end{cases}$
}
\ENDIF
\STATE{\color{gray}{// Apply $G_{j-1}^{(j+1)}$ to $t_{1:j+1,j}$.}}
\IF{$j>1$}
\STATE{
$\begin{bmatrix}t_{j-1,j}\\t_{jj}\end{bmatrix}
:=
\begin{bmatrix}c&s\\-s&c\end{bmatrix}
\begin{bmatrix}t_{j-1,j}\\t_{jj}\end{bmatrix}$
where
$\begin{cases}
s:=t_{j,j-1}/(t_{j-1,j-1}^2+t_{j,j-1}^2)^{1/2}\\
c:=t_{j-1,j-1}/(t_{j-1,j-1}^2+t_{j,j-1}^2)^{1/2}
\end{cases}$
}
\ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}
\end{itemize}
\end{frame}

% Slide 75
\begin{frame}{Quasi-minimal residual (QMR) method, cont'd\textsubscript{5}}
\begin{itemize}
\item Finally, the QMR iteration is given as follows:\vspace{-.3cm}
\setcounter{algorithm}{10}
\begin{algorithm}[H]
\footnotesize
\caption{QMR$:(x_0,\varepsilon)\mapsto x_j$}
\begin{algorithmic}[1]
\setcounter{ALC@line}{11}
\STATE{\hspace{\algorithmicindent}\color{gray}{// Apply $G_{j}^{(j+1)}$ to $\underline{g}[1:j+1]$ and $t_{1:j+1,j}$}}
\STATE{\hspace{\algorithmicindent}
$\begin{bmatrix}\underline{g}[j]\\\underline{g}[j+1]\end{bmatrix}
:=
\begin{bmatrix}c&s\\-s&c\end{bmatrix}
\begin{bmatrix}\underline{g}[j]\\0\end{bmatrix}$
where
$\begin{cases}
s:=t_{j+1,j}/(t_{jj}^2+t_{j+1,j}^2)^{1/2}\\
c:=t_{jj}/(t_{jj}^2+t_{j+1,j}^2)^{1/2}
\end{cases}$}
\STATE{\hspace{\algorithmicindent}$t_{jj}:=c\cdot t_{jj}+s\cdot t_{j+1,j};$ $t_{j+1,j}:=0$}
\STATE{\hspace{\algorithmicindent}$p_j:=(v_j-\tau_{j-1}^{(2)}p_{j-1}-\tau_{j-2}^{(3)}p_{j-2})/\tau_j^{(1)}$ where $p_0:=0$ and $p_{-1}:=0$}
\STATE{\hspace{\algorithmicindent}$x_j:=x_{j-1}+\underline{g}[j]p_j$}
\STATE{\hspace{\algorithmicindent}\textbf{if} $|\underline{g}[j+1]|<\varepsilon\|b\|_2$ \textbf{then} Stop}
\COMMENT{Stop if $\|r_j\|_2<\varepsilon\|b\|_2$}
\end{algorithmic}
\end{algorithm}
$\vspace{-.9cm}$\\
\item[] The QMR usually exhibits a much smoother convergence behavior than BiCG.
\end{itemize}
\end{frame}

\section{Transpose-free methods}

\subsection{Matrix polynomials and Krylov subspaces}

% Slide 76
\begin{frame}{Matrix polynomials}
\begin{itemize}
\item Let $A\in\mathbb{R}^{n\times n}$, and consider the scalar \textbf{polynomial} of \textit{degree} $m$ given by
\begin{align*}
p_m:
&\,\mathbb{C}\rightarrow\mathbb{C}\\
&\,t\mapsto a_0+a_1t+a_2t^2+\dots+a_mt^m.
\end{align*}
That is, $a_m\neq 0$.
An associated \textbf{matrix polynomial} is then given by 
\begin{align*}
p_m:
&\,\mathbb{R}^{n\times n}\rightarrow\mathbb{R}^{n\times n}\\
&\,A\mapsto a_0I_n+a_1A+a_2A^2+\dots+a_mA^m.
\end{align*}
$\vspace{-.6cm}$\\
\begin{theorem}[Eigenvalues of matrix polynomials]
Let $p:\mathbb{C}\rightarrow\mathbb{C}$ be a scalar polynomial, and $\theta\in\mathbb{C}$ be an eigenvalue of $A\in\mathbb{R}^{n\times n}$ with an associated eigenvector $y\in\mathbb{C}^n$.
Then, $p(\theta)$ is an eigenvalue of $p(A)$, and $y$ is an associated eigenvector, i.e., $p(A)y=p(\theta)y$.
\end{theorem}
\begin{theorem}[Cayley-Hamilton theorem]
Let $P_A(t):=\det(A_n-t I_n)$ denote the (scalar) characteristic polynomial of $A\in\mathbb{R}^{n\times n}$, then $P_A(A)=0_{n\times n}$.
\end{theorem}
\end{itemize}
\end{frame}

% Slide 77
\begin{frame}{Matrix polynomials, cont'd}
\begin{itemize}
\item The Cayley-Hamilton theorem guarantees that, for any matrix $A\in\mathbb{R}^{n\times n}$, there is a polynomial $p$ of degree no greater than $n$ such that $p(A)=0$.
\item[] A polynomial whose value is zero at the matrix is called the \textbf{annihilating polynomial}.
\item Since $p(A)=0$ implies $\alpha p(A)=0$ for all $\alpha\in\mathbb{C}$, we may always normalize a polynomial so that its highest-order term is 1.
Such polynomials are called \textbf{monic polynomials}.
\begin{theorem}[Minimum polynomial of a matrix]
\begin{itemize}\normalsize
\item[-] For a matrix $A\in\mathbb{R}^{n\times n}$, there exists a unique monic polynomial $q_A$ of \textbf{minimum degree}, no greater than $n$, that annihilates the matrix $A$, i.e., $q_A(A)=0_{n\times n}$.\\
\item[-] The unique monic polynomial $q_A$ of minimum degree that annihilates the matrix $A$ is called the \textbf{minimal polynomial} of $A$.
\end{itemize}
\end{theorem}
\item \textit{Similar matrices} have the \textit{same minimal polynomial}.
\end{itemize}
\end{frame}

% Slide 78
\begin{frame}{Krylov subspaces and matrix polynomials}
\begin{itemize}
\item All Krylov subspace methods introduced for the solving of linear systems construct iterates of the form $x_m\in x_0+\mathcal{K}_m(A,r_0)$ where, we recall that
\begin{align*}
\mathcal{K}_m(A,r_0)=\text{span}\{r_0,Ar_0,\dots,A^{m-1}r_0\}
\end{align*}
so that, for every such iterate $x_m$, there exists a polynomial $p_{m-1}$ of degree $m-1$ such that
\begin{align*}
x_m=x_0+p_{m-1}(A)r_0.
\end{align*}
Moreover, for the residual associated to such iterates, we have
\begin{align*}
r_m:=b-Ax_m=r_0-Ap_{m-1}(A)r_0
\end{align*}
so that there exists a polynomial of degree no greater than $m$, which we denote by $\varphi_m$, such that
\begin{align*}
r_m=\varphi_m(A)r_0.
\end{align*}
We refer to $\varphi_m$ as the \textbf{residual polynomial}.
\end{itemize}
\end{frame}

\subsection{Conjugate gradient squared (CGS) method} 
% Slide 79
\begin{frame}{Conjugate gradient squared (CGS) method}
\begin{itemize}
\item While both the BiCG and QMR methods offer alternatives to solve non-symmetric linear systems on the basis of short-recurrence relations, they do both require to be able to compute $x\mapsto A^Tx$.
\item[] The CGS method (Sonneveld, 1989) was introduced as a means to to approximate the solution of non-symmetric linear systems, on the basis on short-recurrence relations, without the need to be able to evaluate $x\mapsto Ax.\hspace{-1cm}$
\item The CGS method is derived from the perspective of BiCG iterates, that is,
\begin{align*}
x_j\in x_0+\mathcal{K}_j(A,r_0)
\;\text{ such that }\;
r_j:=b-Ax_j\perp\mathcal{K}_j(A^T,\tilde{r_0})
\end{align*}
for which we saw that, there exists a \textbf{residual polynomial} $\varphi_j$ of degree no greater than $j$, and such that
\begin{align*}
r_j=\varphi_j(A)r_0.
\end{align*}
Without loss of generality, we assume $\varphi_j(0)=1$.
\end{itemize}\smallskip
\tiny{Sonneveld, P. (1989). CGS: A fast Lanczos-type solver for nonsymmetric linear systems, SIAM Journal on
Scientific and Statistical Computing, 10 , 36–52.}
\end{frame}

% Slide 80
\begin{frame}{Conjugate gradient squared (CGS) method, cont'd\textsubscript{1}}
\begin{itemize}
\item Furthermore, there exists another polynomial $\psi_j$ of degree no greater than $j$ such that the BiCG search direction $p_{j+1}$ is given by\vspace{-.1cm}
\begin{align*}
p_{j+1}=\psi_j(A)r_0.
\end{align*}
\item The BiCG dual vectors $\tilde{r}_j$ and $\tilde{p}_{j+1}$ being updated after the same schemes as those of the vectors $r_j$ and $p_{j+1}$, respectively, except with $A^T$ instead of $A$, we then have\vspace{-.1cm}
\begin{align*}
\tilde{r}_j=\varphi(A^T)\tilde{r}_0
\;\text{ and }\;
\tilde{p}_{j+1}=\psi_j(A^T)\tilde{r}_0
\;\text{ for }\;
j=1,2,\dots,m.
\end{align*}
\item The diagonal and super-diagonal components of the tridiagonal, $\alpha_j$ and $\beta_j$, respectively, formed by the BiCG iteration, can then be recast as follows:\vspace{-.1cm}
\begin{align*}
\alpha_j&\,=
\frac{(r_{j-1},\tilde{r}_{j-1})}{Ap_j,\tilde{p}_j}=
\frac{(\varphi_{j-1}(A)r_0,\varphi_{j-1}(A^T)\tilde{r}_0)}{(A\psi_{j-1}(A)r_0,\psi_{j-1}(A^T)\tilde{r}_0)}=
\frac{(\varphi_{j-1}^2(A)r_0,\tilde{r}_0)}{(A\psi_{j-1}^2(A)r_0,\tilde{r}_0)},\\
\beta_j&\,=\frac{(r_j,\tilde{r}_j)}{(r_{j-1},\tilde{r}_{j-1})}=
\frac{(\varphi_{j}(A)r_0,\varphi_{j}(A^T)\tilde{r}_0)}{(\varphi_{j-1}(A)r_0,\varphi_{j-1}(A^T)\tilde{r}_0)}=
\frac{(\varphi_{j}^2(A)r_0,\tilde{r}_0)}{(\varphi_{j-1}^2(A)r_0,\tilde{r}_0)}
\end{align*}
which indicates that it is possible to compute $x_{j+1}$ and $r_{j+1}$ without any evaluation of $x\mapsto A^Tx$.
\end{itemize}
\end{frame}

% Slide 81
\begin{frame}{Conjugate gradient squared (CGS) method, cont'd\textsubscript{2}}
\begin{itemize}
\item The problem we are left with is to find update formulae for\vspace{-.1cm}
\begin{align*}
\boxed{\varphi_j^2(A)r_0}
\;\text{ and }\;
\boxed{\psi_j^2(A)r_0}\,.
\end{align*}
$\vspace{-.45cm}$\\
\item The update formula for the BiCG residual is recast into\vspace{-.1cm}
\begin{align*}
r_j=&\,r_{j-1}-\alpha_jAp_j\\
\varphi_j(A)r_0=&\varphi_{j-1}(A)r_0-\alpha_jA\psi_{j-1}(A)r_0
\end{align*}
$\vspace{-.55cm}$\\
which, as it holds irrespective of $r_0$, leads to\vspace{-.1cm}
\begin{align}\label{eq:06}
\varphi_j(A)=\varphi_{j-1}(A)-\alpha_jA\psi_{j-1}(A)
\;\text{ where }\;
\varphi_{0}(A)=\psi_{0}(A)=I_n.
\end{align}
$\vspace{-.55cm}$\\
Irrespective of the polynomial $p$, we have $Ap(A)=p(A)A$, so that\vspace{-.1cm}
\begin{align}\label{eq:07}
\varphi_j^2(A)=\varphi_{j-1}^2(A)+\alpha_j^2A^2\psi^2_{j-1}(A)-2\alpha_jA\varphi_{j-1}(A)\psi_{j-1}(A).
\end{align}
$\vspace{-.55cm}$\\
\item Similarly, from the update formula for the BiCG search direction, we get\vspace{-.1cm}
\begin{align}
p_{j+1}=&\,r_j+\beta_jp_j\nonumber\\
\psi_j(A)r_0=&\,\varphi_j(A)r_0+\beta_j\psi_{j-1}(A)r_0\nonumber\\
\psi_j(A)=&\,\varphi_j(A)+\beta_j\psi_{j-1}(A)\label{eq:08}
\end{align}
$\vspace{-1.1cm}$\\
\begin{align}
\hspace{-.2cm}
\text{so that we obtain }\psi^2_j(A)=\varphi_j^2(A)+\beta_j^2\psi_{j-1}^2(A)+2\beta_j\varphi_j(A)\psi_{j-1}(A).\label{eq:09}
\end{align}
\end{itemize}
\end{frame}

% Slide 82
\begin{frame}{Conjugate gradient squared (CGS) method, cont'd\textsubscript{3}}
\begin{itemize}
\item The cross-term of Eq.~\eqref{eq:07} is developed as follows using Eq.~\eqref{eq:08}:
\begin{align}
\varphi_{j-1}(A)\psi_{j-1}(A)
=&\,\varphi_{j-1}(A)(\varphi_{j-1}(A)+\beta_{j-1}\psi_{j-2}(A))\nonumber\\
=&\,\varphi_{j-1}^2(A)+\beta_{j-1}\varphi_{j-1}(A)\psi_{j-2}(A).\label{eq:10}
\end{align}
Using Eqs.~\eqref{eq:06} and \eqref{eq:08}, we get the following expression for the cross-term of Eq.~\eqref{eq:09}:
\begin{align}
\varphi_{j}(A)\psi_{j-1}(A)
=&\,(\varphi_{j-1}(A)-\alpha_{j}A\psi_{j-1}(A))\psi_{j-1}(A)\nonumber\\
=&\,\varphi_{j-1}(A)\psi_{j-1}(A)-\alpha_{j}A\psi_{j-1}^2(A)\nonumber\\
=&\,\varphi_{j-1}(A)(\varphi_{j-1}(A)+\beta_{j-1}\psi_{j-2}(A))-\alpha_{j}A\psi_{j-1}^2(A)\nonumber\\
=&\,\varphi_{j-1}^2(A)+\beta_{j-1}\varphi_{j-1}(A)\psi_{j-2}(A)-\alpha_{j}A\psi_{j-1}^2(A)\label{eq:11}
\end{align}
where $\beta_0:=0$.
\end{itemize}
\end{frame}

% Slide 83
\begin{frame}{Conjugate gradient squared (CGS) method, cont'd\textsubscript{4}}
\begin{itemize}
\item We are now equipped to develop the update formulae of $\varphi_j^{2}(A)$ and $\psi_j^2(A)\!:\hspace{-1cm}$
\begin{itemize}\normalsize
\item[-] First, using Eq.~\eqref{eq:06}, $\phi_0(A)=\psi_0(A)=I_n$ and Eq.~\eqref{eq:08}, we obtain:
\begin{align*}
\begin{cases}
\varphi_1^2(A)=(\varphi_0(A)-\alpha_1A\psi_0(A))^2=(I_n-\alpha_1A)^2\\
\varphi_1(A)\psi_0(A)=\varphi_1(A)=\varphi_0(A)-\alpha_1A\psi_0(A)=I_n-\alpha_1A\\
\psi_1^2(A)=(\varphi_1(A)+\beta_1\psi_0(A))^2=(\varphi_1(A)+\beta_1I_n)^2
\end{cases}\!\!.
\end{align*}
\item[-] Then using Eqs.~\eqref{eq:07} with Eq.~\eqref{eq:10}, Eq.~\eqref{eq:11}, and Eq.~\eqref{eq:09}, respectively, for $j=2,3,\dots,m$, we get:
\begin{align*}
\begin{cases}
\varphi_j^2(A)=\varphi_{j-1}^2(A)+\alpha_j^2A^2\psi^2_{j-1}(A)\\
\hspace{1.48cm}-2\alpha_jA\left(\varphi_{j-1}^2(A)+\beta_{j-1}\varphi_{j-1}(A)\psi_{j-2}(A)\right)\\
\varphi_j(A)\psi_{j-1}(A)=\varphi_{j-1}^2(A)+\beta_{j-1}\varphi_{j-1}(A)\psi_{j-2}(A)-\alpha_{j}A\psi_{j-1}^2(A)\\
\psi^2_j(A)=\varphi_j^2(A)+\beta_j^2\psi_{j-1}^2(A)+2\beta_j\varphi_j(A)\psi_{j-1}(A)
\end{cases}\!\!.
\end{align*}
\end{itemize}
\end{itemize}
\end{frame}

% Slide 84
\begin{frame}{Conjugate gradient squared (CGS) method, cont'd\textsubscript{5}}
\begin{itemize}
\item Let us define
\begin{align*}
\hat{r}_j:=\varphi_j^2(A)r_0,\;\;
\hat{p}_{j+1}:=\psi_j^2(A)r_0
\;\text{ and }\;
\hat{q}_j:=\varphi_j(A)\psi_{j-1}(A)r_0.
\end{align*}
Using the update formulae from the last slide, we get
\begin{align*}
\hat{r}_j
=&\,\varphi_{j-1}^2(A)r_0+\alpha_j^2A^2\psi^2_{j-1}(A)r_0\\
&\,-2\alpha_jA\left(\varphi_{j-1}^2(A)+\beta_{j-1}\varphi_{j-1}(A)\psi_{j-2}(A)\right)r_0\\
=&\,\hat{r}_{j-1}+\alpha_j^2A^2\hat{p}_j-2\alpha_jA\left(\hat{r}_{j-1}+\beta_{j-1}\hat{p}_{j-1}\right)\\
=&\,\hat{r}_{j-1}+\alpha_jA\left(\alpha_jA\hat{p}_j-2\hat{r}_{j-1}-2\beta_{j-1}\hat{p}_{j-1}\right).
\end{align*}
\begin{align*}
\text{As well as,}\;\;\;\;
\hat{q}_j
=&\,\varphi_j(A)\psi_{j-1}(A)r_0\\
=&\,\varphi_{j-1}^2(A)r_0+\beta_{j-1}\varphi_{j-1}(A)\psi_{j-2}(A)r_0-\alpha_{j}A\psi_{j-1}^2(A)r_0\\
=&\,\hat{r}_{j-1}+\beta_{j-1}\hat{q}_{j-1}-\alpha_{j}A\hat{p}_j.
\end{align*}
\begin{align*}
\hspace{-1.75cm}
\text{and }\;\;\;\;
\hat{p}_{j+1}
=&\,\varphi_j^2(A)r_0+\beta_j^2\psi_{j-1}^2(A)r_0+2\beta_j\varphi_j(A)\psi_{j-1}(A)r_0\\
=&\,\hat{r}_j+\beta_j^2\hat{p}_j+2\beta_j\hat{q}_j.
\end{align*}
\end{itemize}
\end{frame}

% Slide 85
\begin{frame}{Conjugate gradient squared (CGS) method, cont'd\textsubscript{6}}
\begin{itemize}
\item Still using the update formulae for $\varphi_j^2(A)$ and $\psi_j^2(A)$, we get:
\begin{align*}
\alpha_j=
\frac{(\varphi_{j-1}^2(A)r_0,\tilde{r}_0)}{(A\psi_{j-1}^2(A)r_0,\tilde{r}_0)}=
\frac{(\hat{r}_{j-1},\tilde{r}_0)}{(A\hat{p}_j,\tilde{r}_0)}
\end{align*}
as well as
\begin{align*}
\beta_j=
\frac{(\varphi_{j}^2(A)r_0,\tilde{r}_0)}{(\varphi_{j-1}^2(A)r_0,\tilde{r}_0)}=
\frac{(\hat{r}_{j},\tilde{r}_0)}{(\hat{r}_{j-1},\tilde{r}_0)}.
\end{align*}
\item For the sake of brevity, let $u_j:=\hat{r}_j+\beta_j\hat{q}_j$, so that we have:
\begin{align*}
\begin{cases}
\hat{q}_j=
u_{j-1}-\alpha_jA\hat{p}_j,\\
\hat{r}_j=
\hat{r}_{j-1}+\alpha_jA(\alpha_jA\hat{p}_j-2u_{j-1})\\
\hspace{.45cm}=\hat{r}_{j-1}+\alpha_jA(u_{j-1}-\hat{q}_j-2u_{j-1})\\
\hspace{.45cm}=\hat{r}_{j-1}-\alpha_jA(\hat{q}_j+u_{j-1}),\\
\hat{p}_{j+1}=
u_j+\beta_j^2\hat{p}_j+\beta_j\hat{q}_j.
\end{cases}
\end{align*}
\end{itemize}
\end{frame}

% Slide 86
\begin{frame}{Conjugate gradient squared (CGS) method, cont'd\textsubscript{7}}
\begin{itemize}
\item If the BiCG method converges, then $\|r_j\|_2=\|\varphi_j(A)r_0\|_2$ tends to zero. 
\item[] Then, one might expect that $\|\hat{r}_j\|_2=\|\varphi_j^2(A)r_0\|_2$ tends faster to zero.
\item[] Hence, in an attempt to accelerate convergence, the CGS iterate $x_j$ is defined so as to yield
\begin{align*}
b-Ax_j=\hat{r}_j.
\end{align*}
Given our update formula for $\hat{r}_j$, we get:
\begin{align*}
b-Ax_j=&\,\hat{r}_{j-1}-\alpha_jA(\hat{q}_j+u_{j-1})\\
Ax_j=&\,b-\hat{r}_{j-1}+\alpha_jA(\hat{q}_j+u_{j-1})\\
Ax_j=&\,b-(b-Ax_{j-1})+\alpha_jA(\hat{q}_j+u_{j-1})\\
Ax_j=&\,Ax_{j-1}+\alpha_jA(\hat{q}_j+u_{j-1})
\end{align*}
so that 
\begin{align*}
\boxed{x_j=x_{j-1}+\alpha_j(\hat{q}_j+u_{j-1})}\,.
\end{align*}
\end{itemize}
\end{frame}

% Slide 87
\begin{frame}{Conjugate gradient squared (CGS) method, cont'd\textsubscript{8}}
\begin{itemize}
\item Eventually, we obtain the following algorithm:\vspace{-.3cm}
\begin{algorithm}[H]
\small
\caption{CGS$:(x_0,\varepsilon)\mapsto x_j$}
\begin{algorithmic}[1]
\STATE{$r_0:=b-Ax_0$}
\STATE{Pick $\tilde{r}_0$ such that $(r_0,\tilde{r}_0)\neq 0$}
\COMMENT{E.g., $\tilde{r}_0=r_0$}
\STATE{$\hat{p}_1:=r_0;$ $\hat{r}_0:=r_0;$ $u_0:=r_0$}
\FOR{$j=1,2\dots$}
\STATE{$\alpha_j:=(\hat{r}_{j-1},\tilde{r}_0)/(A\hat{p}_j,\tilde{r}_0)$}
\STATE{$\hat{q}_j:=u_{j-1}-\alpha_jA\hat{p}_j$}
\STATE{$x_j:=x_{j-1}+\alpha_j(\hat{q}_j+u_{j-1})$}
\STATE{$\hat{r}_j:=\hat{r}_{j-1}-\alpha_jA(\hat{q}_j+u_{j-1})$}
\STATE{\textbf{if} $\|\hat{r}_j\|_2<\varepsilon\|b\|_2$ \textbf{then} Stop}
\STATE{$\beta_j:=(\hat{r}_j,\tilde{r}_0)/(\hat{r}_{j-1},\tilde{r}_0)$}
\STATE{$u_j:=\hat{r}_j+\beta_j\hat{q}_j$}
\STATE{$\hat{p}_{j+1}:=u_j+\beta_j^2\hat{p}_j+\beta_j\hat{q}_j$}
\ENDFOR
\end{algorithmic}
\end{algorithm}
$\vspace{-.8cm}$\\
\item[-] A CGS iteration entails two matrix-vector products, which is similar to BiCG, the difference being that CGS does not need to evaluate $x\mapsto A^Tx$.
\item[-] When it converges, CGS often does so about twice as fast as BiCG. 
\end{itemize}
\end{frame}

% Slide 88
\begin{frame}{Conjugate gradient squared (CGS) method, cont'd\textsubscript{9}}
\begin{itemize}
\item[-] However, as the residual polynomial is squared, i.e., $\hat{r}_j=\varphi_j^2(A)r_0$ where $r_j=\varphi_j(A)r_0$, if the residual $r_j$ increases in BiCG, then it does so even more significantly in CGS.
\item[] As a result, CGS convergence curves can exhibit important oscillations, sometimes leading to numerical instability.
\end{itemize}
\end{frame}

\subsection{Bi-conjugate gradient stabilized (BiCGSTAB) method}
% Slide 89
\begin{frame}{Bi-conjugate gradient stabilized (BiCGSTAB) method}
\begin{itemize}
\item The CGS method, which is based on squaring the BiCG residual polynomial, i.e., $\hat{r}_j:=\varphi_j^2(A)r_0$, is prone to substantial build-up of rounding error, possibly even overflow.
\item The BiCGSTAB method (van der Vorst, 1992) is a variant of CGS developed to remedy unwanted oscillations, hence the name of BiCG stabilized.
\item[] BiCGSTAB iterates are defined so as to yield a residual of the form
\begin{align*}
r_j=\phi_j(A)\varphi_j(A)r_0
\end{align*}
where $\varphi_j$ is, still, the \textit{residual polynomial of} the \textit{BiCG} method, and $\phi_j$ is a \textit{new $j$-th degree polynomial} introduced to remedy those potentially spurious oscillations, and defined as follows:
\begin{align*}
\phi_0(A)=I_n
\;\text{ and }\;
\phi_j(A)=(I_n-\omega_jA)\phi_{j-1}(A)
\;\text{ for }\;
j=1,2,\dots
\end{align*}
where $\omega_j$ is chosen so as to minimize the residual norm.
\end{itemize}\smallskip
\tiny{van der Vorst, H. A. (1992). Bi-CGSTAB: A fast and smoothly converging variant of Bi-CG for the solution
of nonsymmetric linear systems. SIAM Journal on Scientific and Statistical Computing, 13, 631–644.}
\end{frame}

% Slide 90
\begin{frame}{$\hspace{-.15cm}$Bi-conjugate gradient stabilized (BiCGSTAB) method, cont'd\textsubscript{1}$\hspace{-1cm}$}
\begin{itemize}
\item[] Then, the search direction is defined as 
\begin{align*}
p_{j+1}=\phi_j(A)\psi_j(A)r_0
\;\text{ for }\;
j=1,2,\dots
\end{align*}
where the polynomial $\psi_j$ is the \textit{search direction polynomial of CGS}.
\item[]We thus have the following update formulae:
\begin{align}
\begin{cases}
\varphi_j(A)=\varphi_{j-1}(A)-\alpha_jA\psi_{j-1}(A)\\
\psi_j(A)=\varphi_j(A)+\beta_j\psi_{j-1}(A)\\
\phi_j(A)=(I_n-\omega_jA)\phi_{j-1}(A)
\end{cases}\label{eq:12}
\text{for }j=1,2,\dots
\end{align}
where $\varphi_0(A)=\psi_0(A)=\phi_0(A)=I_n$.
\item We can then develop the following update formula for the polynomial of the BiCGSTAB residual:
\begin{align}
\phi_j(A)\varphi_j(A)
&\,=(I_n-\omega_jA)\phi_{j-1}(A)\left(\varphi_{j-1}(A)-\alpha_jA\psi_{j-1}(A)\right)\nonumber\\
&\,=(I_n-\omega_jA)\left(\phi_{j-1}(A)\varphi_{j-1}(A)-\alpha_jA\phi_{j-1}(A)\psi_{j-1}(A)\right).\label{eq:13}
\end{align}
\end{itemize}
\end{frame}

%Slide 91
\begin{frame}{$\hspace{-.15cm}$Bi-conjugate gradient stabilized (BiCGSTAB) method, cont'd\textsubscript{2}$\hspace{-1cm}$}
\begin{itemize}
%\item The update formula for the polynomial of the BiCGSTAB search direction is obtained as follows:\vspace{-.075cm}
%\begin{align}
%\phi_j(A)\psi_j(A)
%=&\,\phi_j(A)\left(\varphi_j(A)+\beta_j\psi_{j-1}(A)\right)\nonumber\\
%=&\,\phi_j(A)\varphi_j(A)+\beta_j\phi_j(A)\psi_{j-1}(A)\nonumber\\
%=&\,\phi_j(A)\varphi_j(A)+\beta_j(I_n-\omega_jA)\phi_{j-1}(A)\psi_{j-1}(A).\label{eq:14}
%\end{align}
\item From $r_j=\phi_j(A)\varphi_j(A)r_0$, Eq.~\eqref{eq:13} and $p_{j+1}=\phi_j(A)\psi_j(A)r_0$, we get the following residual update formula:
\begin{align*}
r_j
=&\,(I_n-\omega_jA)\left(\phi_{j-1}(A)\varphi_{j-1}(A)-\alpha_jA\phi_{j-1}(A)\psi_{j-1}(A)\right)r_0\\
=&\,(I_n-\omega_jA)\left(\phi_{j-1}(A)\varphi_{j-1}(A)r_0-\alpha_jA\phi_{j-1}(A)\psi_{j-1}(A)r_0\right)\\
=&\,(I_n-\omega_jA)\left(r_{j-1}-\alpha_jAp_j\right).
\end{align*}
\item From $p_{j+1}=\phi_j(A)\psi_j(A)r_0$, $r_j=\phi_j(A)\varphi_j(A)r_0$ and Eq.~\eqref{eq:12}, we get the following expression for the update of the search direction:
\begin{align*}
p_{j+1}
=&\,\phi_j(A)\left(\varphi_j(A)+\beta_j\psi_{j-1}(A)\right)r_0\\
=&\,\phi_j(A)\varphi_j(A)r_0+\beta_j\phi_j(A)\psi_{j-1}(A)r_0\\
=&\,r_j+\beta_j(I_n-\omega_jA)\phi_{j-1}(A)\psi_{j-1}(A)r_0\\
=&\,r_j+\beta_j(I_n-\omega_jA)p_j.
\end{align*}
\end{itemize}
\end{frame}

%Slide 92
\begin{frame}{$\hspace{-.15cm}$Bi-conjugate gradient stabilized (BiCGSTAB) method, cont'd\textsubscript{3}$\hspace{-1cm}$}
\begin{itemize}
\item Similarly as for BiCG and CGS, we have
\begin{align*}
\alpha_j=
\frac{(\varphi_{j-1}(A)r_0,\varphi_{j-1}(A^T)\tilde{r}_0)}{(A\psi_{j-1}(A)r_0,\psi_{j-1}(A^T)\tilde{r}_0)}
\;\text{ and }\;
\beta_j=
\frac{(\varphi_{j}(A)r_0,\varphi_{j}(A^T)\tilde{r}_0)}{(\varphi_{j-1}(A)r_0,\varphi_{j-1}(A^T)\tilde{r}_0)}.
\end{align*}
However, unlike with CGS, we do not intend to compute the squared polynomials $\varphi_{j}^2(A)$ and $\psi_{j}^2(A)$.
We proceed as follows.
\item[-] First, from the update formulae for $\phi_j$ and $\psi_j$ in Eq.~\eqref{eq:12}, we have
\begin{align*}
\varphi_j(A^T)=-\alpha_jA^T\varphi_{j-1}(A^T)+\varphi_{j-1}(A^T)-\alpha_j\beta_{j-1}A^T\psi_{j-2}(A^T),
\end{align*}
which implies that the highest-order term of $\varphi_j(A^T)$ is the same as that of $-\alpha_jA^T\varphi_{j-1}(A^T)$.
Thus, proceeding by induction, we find that this term is 
\begin{align*}
(-1)^j\alpha_j\alpha_{j-1}\cdots\alpha_1(A^T)^j.
\end{align*}
\item[-] Let us then restate the orthogonality of BiCG residuals with their duals as follows:
\begin{align*}
\left(\varphi_i(A)r_0,\varphi_j(A^T)\tilde{r}_0\right)=0
\;\text{ for }\;i\neq j.
\end{align*}
As this holds for all $j\neq i$, this implies $\left(\varphi_i(A)r_0,(A^T)^j\tilde{r}_0\right)=0$ for $i\neq j$.
\end{itemize}
\end{frame}

%Slide 93
\begin{frame}{$\hspace{-.15cm}$Bi-conjugate gradient stabilized (BiCGSTAB) method, cont'd\textsubscript{4}$\hspace{-1cm}$}
\begin{itemize}
\item[] As a result, the only term of $\varphi_j(A^T)$ which contributes to the non-zero part of $\left(\varphi_j(A)r_0,\varphi_j(A^T)\tilde{r}_0\right)$ is the highest-order one.
Thus, we have:
\begin{align}\label{eq:14}
\left(\varphi_j(A)r_0,\varphi_j(A^T)\tilde{r}_0\right)
=
(-1)^j\alpha_j\alpha_{j-1}\cdots\alpha_1
\left(\varphi_j(A)r_0,(A^T)^j\tilde{r}_0\right).
\end{align}
\item[-] Secondly, from the update formula of $\phi_j$ in Eq.~\eqref{eq:12}, we have:
\begin{align*}
\phi_j(A^T)=(I_n-\omega_jA)\phi_{j-1}(A^T)=-\omega_jA^T\phi_{j-1}(A^T)+\phi_{j-1}(A^T),
\end{align*}
which indicates that the highest-order term of $\phi_j(A^T)$ is the same as that of $-\omega_jA^T\phi_{j-1}(A^T)$.
Thus, by induction again, we get that this term is
\begin{align*}
(-1)^j\omega_j\omega_{j-1}\cdots\omega_1(A^T)^j.
\end{align*}
\item[-] As we have previously stated that $\left(\varphi_i(A)r_0,(A^T)^j\tilde{r}_0\right)=0$ for all $i\neq j$, we have that the only term of $\phi_j(A^T)$ which contributes to the non-zero part of $\left(\varphi_j(A)r_0,\phi_j(A^T)\tilde{r}_0\right)$ is the highest-order one.
Therefore, we have:
\begin{align}\label{eq:15}
\left(\varphi_j(A)r_0,\phi_j(A^T)\tilde{r}_0\right)
=
(-1)^j\omega_j\omega_{j-1}\cdots\omega_1
\left(\varphi_j(A)r_0,(A^T)^j\tilde{r}_0\right).
\end{align}
\end{itemize}	
\end{frame}

%Slide 94
\begin{frame}{$\hspace{-.15cm}$Bi-conjugate gradient stabilized (BiCGSTAB) method, cont'd\textsubscript{5}$\hspace{-1cm}$}
\begin{itemize}
\item[-] Now, by combining Eqs.~\eqref{eq:14} and \eqref{eq:15}, we obtain
\begin{align}\label{eq:16}
\left(\varphi_j(A)r_0,\varphi_j(A^T)\tilde{r}_0\right)
=
\frac{\alpha_j\alpha_{j-1}\cdots\alpha_1}{\omega_j\omega_{j-1}\cdots\omega_1}
\left(\varphi_j(A)r_0,\phi_j(A^T)\tilde{r}_0\right).
\end{align}
Consequently, using Eq.~\eqref{eq:16}, the formula for the $\beta_j$ can be recast as follows:
\begin{align*}
\beta_j
=&\,\frac{(\varphi_{j}(A)r_0,\varphi_{j}(A^T)\tilde{r}_0)}{(\varphi_{j-1}(A)r_0,\varphi_{j-1}(A^T)\tilde{r}_0)}\\
=&\,\frac{\alpha_j}{\omega_j}\cdot\frac{\left(\varphi_j(A)r_0,\phi_j(A^T)\tilde{r}_0\right)}{\left(\varphi_{j-1}(A)r_0,\phi_{j-1}(A^T)\tilde{r}_0\right)}\\
=&\,\frac{\alpha_j}{\omega_j}\cdot\frac{\left(\phi_j(A)\varphi_j(A)r_0,\tilde{r}_0\right)}{\left(\phi_{j-1}(A)\varphi_{j-1}(A)r_0,\tilde{r}_0\right)}\\
=&\,\frac{\alpha_j}{\omega_j}\cdot\frac{(r_j,\tilde{r}_0)}{(r_{j-1},\tilde{r}_0)}.
\end{align*}
\end{itemize}	
\end{frame}

%Slide 95
\begin{frame}{$\hspace{-.15cm}$Bi-conjugate gradient stabilized (BiCGSTAB) method, cont'd\textsubscript{6}$\hspace{-1cm}$}
\begin{itemize}
\item[-] In order to find an adequate formula for $\alpha_j$, we now work on simplifying 
\begin{align*}
\left(A\psi_{j-1}(A)r_0,\psi_{j-1}(A^T)\tilde{r}_0\right).
\end{align*}
From the update formula of $\psi_j$ given in Eq.~\eqref{eq:12}, we get:
\begin{align*}
\psi_j(A^T)=\varphi_j(A^T)+\beta_j\psi_{j-1}(A^T),
\end{align*}
which indicates that the highest-order term of $\psi_j(A^T)$ is the same as that of $\varphi_j(A^T)$.
We recall this term is
\begin{align*}
(-1)^j\alpha_j\alpha_{j-1}\cdots\alpha_1(A^T)^j.
\end{align*}
\item[-] We then restate the $A$-orthogonality of BiCG search directions with their duals as follows:
\begin{align*}
\left(A\psi_i(A)r_0,\psi_j(A^T)\tilde{r}_0\right)=0
\;\text{ for }\;
i\neq j.
\end{align*}
As this holds for all $j\neq i$, this implies $\left(A\psi_i(A)r_0,(A^T)^j\tilde{r}_0\right)=0$ for $i\neq j$.
\end{itemize}	
\end{frame}

%Slide 96
\begin{frame}{$\hspace{-.15cm}$Bi-conjugate gradient stabilized (BiCGSTAB) method, cont'd\textsubscript{7}$\hspace{-1cm}$}
\begin{itemize}
\item[] Therefore, the only term of $\psi_j(A^T)$ which contributes to the non-zero part of $\left(A\psi_j(A)r_0,\psi_j(A^T)\tilde{r}_0\right)$ is the highest order.
Thus, we have:
\begin{align}\label{eq:17}
\left(A\psi_j(A)r_0,\psi_j(A^T)\tilde{r}_0\right)=
(-1)\alpha_j\alpha_{j-1}\cdots\alpha_1
\left(A\psi_j(A)r_0,(A^T)^j\tilde{r}_0\right).
\end{align}
\item[-] Analogously, we can show that
\begin{align}\label{eq:18}
\left(A\psi_j(A)r_0,\phi_j(A^T)\tilde{r}_0\right)=
(-1)\omega_j\omega_{j-1}\cdots\omega_1
\left(A\psi_j(A)r_0,\phi_j(A^T)\tilde{r}_0\right).
\end{align}
$\vspace{-.62cm}$\\
\item[-] Then, upon combining Eqs.~\eqref{eq:17} and \eqref{eq:18}, we obtain:
\begin{align}\label{eq:19}
\left(A\psi_j(A)r_0,\psi_j(A^T)\tilde{r}_0\right)=
\frac{\alpha_j\alpha_{j-1}\cdots\alpha_1}{\omega_j\omega_{j-1}\cdots\omega_1}
\left(A\psi_j(A)r_0,\phi_j(A^T)\tilde{r}_0\right).
\end{align}
\item[-] Finally, an update formula for $\alpha_j$ is obtained as follows by combining Eqs.~\eqref{eq:14}, \eqref{eq:15} and \eqref{eq:19}:
\begin{align*}
\alpha_j
=&\,\frac{(\varphi_{j-1}(A)r_0,\varphi_{j-1}(A^T)\tilde{r}_0)}{(A\psi_{j-1}(A)r_0,\psi_{j-1}(A^T)\tilde{r}_0)}\\
=&\,\frac{(\varphi_{j-1}(A)r_0,\phi_{j-1}(A^T)\tilde{r}_0)}{(A\psi_{j-1}(A)r_0,\phi_{j-1}(A^T)\tilde{r}_0)}
\end{align*}
\end{itemize}	
\end{frame}

%Slide 97
\begin{frame}{$\hspace{-.15cm}$Bi-conjugate gradient stabilized (BiCGSTAB) method, cont'd\textsubscript{8}$\hspace{-1cm}$}
\begin{itemize}
\item[]so that\vspace{-.3cm}
\begin{align*}
\alpha_j
=\frac{(\phi_{j-1}(A)\varphi_{j-1}(A)r_0,\tilde{r}_0)}{(A\phi_{j-1}(A)\psi_{j-1}(A)r_0,\tilde{r}_0)}
=\frac{(r_{j-1},\tilde{r}_0)}{(Ap_{j-1},\tilde{r}_0)}.
\end{align*}
$\vspace{-.45cm}$\\
\item In summary, we have obtained the following updating formula:\vspace{-.15cm}
\begin{align*}
\begin{cases}
r_j=(I_n-\omega_jA)(r_{j-1}-\alpha_jAp_j)\;\text{ where }\;\alpha_j=(r_{j-1},\tilde{r}_0)/(Ap_j,\tilde{r}_0)\\
p_{j+1}=r_j+\beta_j(I_n-\omega_jA)p_j\;\text{ where }\;\beta_j=\alpha_j(r_j,\tilde{r}_0)/\left(\omega_j(r_{j-1},\tilde{r}_0)\right)
\end{cases}
\end{align*}
$\vspace{-.3cm}$\\
for $j=1,2\dots$ where $p_1:=r_0$.
\item Using the update formulae found for $r_j$ and $p_{j+1}$, we can find the update formula of the BiCGSTAB iterate as follows:\vspace{-.2cm}
\begin{align*}
b-Ax_j=&\,r_j\\
b-Ax_j=&\,(I_n-\omega_jA)(r_{j-1}-\alpha_jAp_j)\\
b-Ax_j=&\,r_{j-1}-\alpha_jAp_j-\omega_jA(r_{j-1}-\alpha_jAp_j)\\
Ax_j=&\,b-r_{j-1}+\alpha_jAp_j+\omega_jA(r_{j-1}-\alpha_jAp_j)\\
Ax_j=&\,b-(b-Ax_{j-1})+\alpha_jAp_j+\omega_jA(r_{j-1}-\alpha_jAp_j)
\end{align*}
$\vspace{-.6cm}$\\
so that $\boxed{x_j=x_{j-1}+\alpha_jp_j+\omega_j(r_{j-1}-\alpha_jAp_j)}$.
\end{itemize}	
\end{frame}

%Slide 98
\begin{frame}{$\hspace{-.15cm}$Bi-conjugate gradient stabilized (BiCGSTAB) method, cont'd\textsubscript{9}$\hspace{-1cm}$}
\begin{itemize}
\item All what remains to do is to define $\omega_j$.
As previously mentioned, our goal is to pick $\omega_j$ so as to minimize the residual norm $\|r_j\|_2$, that is
\begin{align*}
\omega_j=\arg\min_{\omega\in\mathbb{R}}\|(I_n-\omega A)(r_{j-1}-\alpha_jAp_j)\|_2.
\end{align*}
For this, let $q_j:=r_{j-1}-\alpha_jAp_j$, so that we aim at finding
\begin{align*}
\min_{\omega\in\mathbb{R}}\|(I_n-\omega A)q_j\|_2
\end{align*}
which yields 
\begin{align*}
\omega_j=\frac{(q_j,Aq_j)}{(Aq_j,Aq_j)}.
\end{align*}
\end{itemize}	
\end{frame}

%Slide 99
\begin{frame}{$\hspace{-.28cm}$Bi-conjugate gradient stabilized (BiCGSTAB) method, cont'd\textsubscript{10}$\hspace{-1cm}$}
\begin{itemize}
\item Eventually, BiCGSTAB iterations are given as follows:\vspace{-.3cm}
\begin{algorithm}[H]
\small
\caption{BiCGSTAB$:(x_0,\varepsilon)\mapsto x_j$}
\begin{algorithmic}[1]
\STATE{$r_0:=b-Ax_0$}
\STATE{Pick $\tilde{r}_0$ such that $(r_0,\tilde{r}_0)\neq 0$}
\COMMENT{E.g., $\tilde{r}_0=r_0$}
\STATE{$p_1:=r_0$}
\FOR{$j=1,2\dots$}
\STATE{$\alpha_j:=(r_{j-1},\tilde{r}_0)/(Ap_j,\tilde{r}_0)$}
\STATE{$q_j:=r_{j-1}-\alpha_jAp_j$}
\STATE{$\omega_j:=(q_j,Aq_j)/(Aq_j,Aq_j)$}
\STATE{$x_j:=x_{j-1}+\alpha_jp_j+\omega_jq_j$}
\STATE{$r_j:=q_j-\omega_jAq_j$}
\STATE{\textbf{if} $\|\tilde{r}_j\|_2<\varepsilon\|b\|_2$ \textbf{then} Stop}
\STATE{$\beta_j:=(\alpha_j/\omega_j)\cdot(r_j,\tilde{r}_0)/(r_{j-1},\tilde{r}_0)$}
\STATE{$p_{j+1}:=r_j+\beta_j(p_j-\omega_jAp_j)$}
\ENDFOR
\end{algorithmic}
\end{algorithm}
\end{itemize}	
\end{frame}

\section{Summary}
% Slide 100
\begin{frame}{Flowchart of Krylov subspace-based linear iterative solvers}
\begin{itemize}
\item The following flowchart can be used for practical solver selection: 
\begin{center}
\includegraphics[height=7.5cm]{images/krylov-summary.pdf}
\end{center}
\end{itemize}
\end{frame}

% Slide 101
\begin{frame}{Things we did not talk about}
\begin{itemize}
\item Breakdowns.
\item Convergence theories.
\item Effects of finite precision.
\item Preconditioning (Lecture~14).
\item Restarting strategies (Lecture~15).
\item Block variants for multiple simultaneously available right-hand sides.
\item Communication-avoiding variants.
\end{itemize}
\end{frame}

\section{Homework problems}

% Slide 102
\begin{frame}{Homework problems}\vspace{.1cm}
Turn in \textbf{your own} solution to \textbf{Pb.$\,$26}:\vspace{.15cm}\\
\begin{minipage}[t]{0.1\textwidth}
\textbf{Pb.$\,$26}
\end{minipage}
\begin{minipage}[t]{0.89\textwidth}
Recall that the approximation $\tilde{x}\in x_0+\mathcal{K}$ to the solution of $Ax=b$ s.t. $b-A\tilde{x}\perp\mathcal{L}$ is formed by $\tilde{x}=x_0+V_m(W_m^HAV_m)^{-1}W_m^Hr_0$, which requires that $W_m^HAV_m$ is not singular, where $\mathcal{K}=\text{range}(V_m)$ and $\mathcal{L}=\text{range}(W_m)$.
Show that $W_m^HAV_m$ is not singular if and only if no vector of the subspace $A\mathcal{K}$ is orthogonal to the constraints subspace $\mathcal{L}$, i.e., $A\mathcal{K}\cap \mathcal{L}^\perp=\{0\}$.
Do so considering the dot product as the inner product.
\end{minipage}\vspace{.15cm}
\begin{minipage}[t]{0.1\textwidth}
\textbf{Pb.$\,$27}
\end{minipage}
\begin{minipage}[t]{0.89\textwidth}
For any $y\in\mathbb{F}^n$ and approximation space $\mathcal{K}\subset\mathbb{F}^n$, show that among all $x\in\mathcal{K}$, $\|y-x\|_2$ is minimized if and only if $y-x\perp \mathcal{K}$.
\end{minipage}\vspace{.15cm}
\begin{minipage}[t]{0.1\textwidth}
\textbf{Pb.$\,$28}
\end{minipage}
\begin{minipage}[t]{0.89\textwidth}
Using the result of Pb.$\,$27, prove that the GMRES iterate defined as\vspace{-.2cm}
\begin{align*}
x_m\in x_0+\mathcal{K}_m(A,r_0)
\text{ such that }
r_m:=b-Ax_m\perp A\mathcal{K}_m(A,r_0)\vspace{-.2cm}
\end{align*}
is equivalently given as the constrained minimizer of residual norm:\vspace{-.2cm}
\begin{align*}
\|b-Ax_m\|_2=\min_{x\in x_0+\mathcal{K}_m(A,r_0)}\|b-Ax\|_2.
\end{align*}
\end{minipage}
\end{frame}

% Slide 103
\begin{frame}{Homework problem}\vspace{.1cm}
\begin{minipage}[t]{0.1\textwidth}
\textbf{Pb.$\,$29}
\end{minipage}
\begin{minipage}[t]{0.89\textwidth}
Show that, if $A\in\mathbb{R}^{n\times n}$ is SPD, then it admits an LU decompostion without pivoting.\\
\end{minipage}
\begin{minipage}[t]{0.1\textwidth}
\textbf{Pb.$\,$30}
\end{minipage}
\begin{minipage}[t]{0.89\textwidth}
Let $A\in\mathbb{R}^{n\times n}$ be SPD and $x_j\in x_0+\mathcal{K}_j(A,r_0)$ denote the CG iterate approximating the solution of $Ax=b$.
Show that $x_j$ minimizes the $A$-norm of the error over the search space, i.e.,
\begin{align*}
\|x-x_j\|_A=\min_{y\in x_0+\mathcal{K}_j(A,r_0)}\|x-y\|_A
\;\text{ where }\; 
\|x\|_A:=(Ax,x)^{1/2}.
\end{align*}
\end{minipage}
\end{frame}

\section{Practice session}
% Slide 104
\begin{frame}{Practice session}
\begin{enumerate}
\item Implement and test the FOM method based on MGS orthogonalization.
\item Implement and test the GMRES method based on CGS2 orthogonalization.
\item Implement and test the CG algorithm.
\item Implement and test the MINRES method.
\item Implement and test the SYMMLQ method.
\item Implement and test the BiCG method.
\item Implement and test the QMR method.
\item Implement and test the GCR method.
\item Implement and test the BiCGSTAB method.
\end{enumerate}
\end{frame}

\section{References}
% Slide 105
\begin{frame}{References}
\begin{itemize}
\item Bai, Z. Z., \& Pan, J. Y. (2021). Matrix analysis and computations. Society for Industrial and Applied Mathematics.
\item Saad, Y. (2003). Iterative methods for sparse linear systems. Society for Industrial and Applied Mathematics.
\end{itemize}
\end{frame}

\end{document}

