\documentclass[t,usepdftitle=false]{beamer}

\input{~/Dropbox/Git/tex-beamer-custom/preamble.tex}

\title[NLA for CS and IE -- Lecture 06]{Numerical Linear Algebra\\for Computational Science and Information Engineering}
\subtitle{\vspace{.3cm}Lecture 06\\Introduction to Direct Methods for Sparse Linear Systems}
\hypersetup{pdftitle={NLA-for-CS-and-IE\_Lecture06}}

\date[Summer 2025]{Summer 2025}

\author[nicolas.venkovic@tum.de]{Nicolas Venkovic\\{\small nicolas.venkovic@tum.de}}
\institute[]{Group of Computational Mathematics\\School of Computation, Information and Technology\\Technical University of Munich}

\titlegraphic{\vspace{0cm}\includegraphics[height=1.1cm]{~/Dropbox/Git/logos/TUM-logo.png}}

\begin{document}
	
\begin{frame}[noframenumbering, plain]
	\maketitle
\end{frame}
	
\myoutlineframe
	
\section{Solving sparse triangular linear systems\\{\small Section 9.3 in Darve \& Wotters (2021)}}

% Slide 01
\begin{frame}{When $L$ is sparse and $b$ is dense}
\begin{itemize}
\item We want to solve $Lx=b$ where $L$ is a \textbf{sparse lower-triangular} matrix with \textbf{non-zero diagonal} entries.
\item Remember how to proceed when $L$ is dense:\vspace{.1cm}
\begin{itemize}
\item[1.] $x_1=b_1/l_{11}$\vspace{.1cm}
\item[2.] $x_2=(b_2-l_{21}x_1)/l_{22}$
\item[]$\vdots$
\item[$i$.] $x_i=\left(b_i - \sum_{j=1}^{i-1} l_{ij}x_j\right)/l_{ii}$
\item[]$\vdots$
\item[$n$.] $x_n=\left(b_n - \sum_{j=1}^{n-1} l_{nj}x_j\right)/l_{nn}$
\end{itemize}
\vspace{.1cm}
\item When $L$ is sparse, we simply need to \textbf{skip} the \textbf{zero components} $l_{ij}$ \textbf{in each summand}.
\item We will see in practice session that this can easily be implemented.
\item The \textbf{final form of} the \textbf{implementation depends on} the sparse matrix \textbf{data structure} used to store $L$.
\end{itemize}
\end{frame}

% Slide 02
\begin{frame}{When both $L$ and $b$ are sparse}
\begin{itemize}
\item \textbf{When} $L$ \textbf{and} $b$ \textbf{are sparse}, then \textbf{the solution} $x$ \textbf{may be sparse}.
\item Ideally, we would like to solve for $x$ as follows:
\vspace{.1cm}
\begin{itemize}
\item[1.] for $i=1,\dots,n$ :\vspace{.1cm}
\item[2.]\hspace{.4cm}if $x_i\neq 0$ :\vspace{.1cm}
\item[3.]\hspace{.8cm}$x_i\leftarrow b_i/\ell_{ii}$\vspace{.1cm}
\item[4.]\hspace{.8cm}for $j=1,\dots,i-1$ :\vspace{.1cm} 
\item[5.]\hspace{1.2cm}if $\ell_{ij}\neq 0$ :\vspace{.1cm}
\item[5.]\hspace{1.6cm}$x_i\leftarrow x_i-\ell_{ij}x_j/\ell_{ii}$
\end{itemize}
\vspace{.1cm}
\item But iterating over the non-zero components of $x$ \textbf{requires to know the structure of} $x$.
\item For any non-zero $x_i$, we have either or both
\begin{itemize}
\item[(a)] $b_i\neq 0$ \vspace{.1cm}
\item[(b)] there is some $j<i$ such that $\ell_{ij}\neq 0$ and $x_j\neq 0$.
\end{itemize}
\end{itemize}
\end{frame}

% Slide 03
\begin{frame}{When both $L$ and $b$ are sparse, cont'd\textsubscript{1}}
\begin{itemize}
\item Let $G=(V,E)$ be the graph associated with $L$, then we denote by $X\!\subset\!V$ the minimal set of vertices so that either or both (a) and (b) hold.\vspace{.05cm}\\
That is, $X\subset V$ is the minimal set such that:\\
\begin{center}
$b_i\neq 0\implies i\in X$ and
$\ell_{ij}\neq 0$ and $j\in X\implies i\in X$. 
\end{center}
\vspace{-.05cm}
\begin{definition}[Reachability \& Reach]
\begin{itemize}
\item[-] A vertex $i\in V$ in a directed graph $G=(V, E)$ is \textbf{reachable from a vertex} $j\in V$, if there is a directed path from $j$ to $i$ in $G$.
That is, if there is a sequence of edges $(j,i_1),(i_1,i_2),\dots,(i_{k-1},i_k),(i_k,i)$ where all the edges are in $E$.\vspace{-.1cm}
\item[-] The set of vertices $i\in V$ reachable from a vertex $j\in V$ is the \textbf{reach} of $j$.
\end{itemize}
\end{definition}
\end{itemize}
\vspace*{-.075cm}
\begin{minipage}{0.35\textwidth}
\begin{itemize}
\vspace{.6cm}
\item If $j\in X$, then\\
every vertex in\\
the reach of $j$,\\
is also in $X$.
\end{itemize}
\end{minipage}$\hspace{-1cm}$
\begin{minipage}{0.6\textwidth}
\includegraphics[height=3.5cm]{images/node-reach.png}
\end{minipage}
\end{frame}

% Slide 04
\begin{frame}{When both $L$ and $b$ are sparse, cont'd\textsubscript{2}}
\begin{itemize}
\item Then, if we let $B\subset V$ be the set of vertices $i\in V$ such that $b_i\neq 0$, then $X$ is the \textbf{set of vertices reachable from} $B$. 
\item Consequently, the set $X$ can be found by operating on the graph $\!G\!=\!(V,E)\!\!\!$ associated with $L$.\vspace{.1cm}\\
Namely, the set $X$ can be found using a \textbf{depth-first traversal} (\textbf{DFS}, i.e., for depth-first search) from every vertex in $B$.
\item Depth-first traversal starts from some node $j$, and explores as far as possible along each branch in the graph before backtracking.
\item We will see an implementation of depth-first traversal in the practice session.
\item Procedure to solve $Lx=b$ where both $L$ and $b$ are sparse is as follows:
\begin{block}{Linear solve of $Lx=b$ where both $L$ and $b$ are sparse}
\begin{itemize}
\item[1.] Define the set $B$ from the sparsity pattern of $b$.\vspace{-.1cm}
\item[2.] Find the set $X$ of non-zero $x$ components using DFS on $B$.\vspace{-.1cm}
\item[3.] Run modified version of the algorithm to solve $Lx=b$ with a sparse $L$, but compute $x_i$ only if $i\in X$.
\end{itemize}
\end{block}
\end{itemize}
\end{frame}

\section{Cholesky factorization\\{\small Section 9.4 in Darve \& Wotters (2021)}}

% Slide 05
\begin{frame}{Up-looking Cholesky algorithm}
\begin{itemize}
\item Now that we know how to solve sparse trangular systems, we can use this to obtain a sparse Cholesky factorization.
\item In particular, the \textbf{up-looking Cholesky} algorithm performs a Cholesky factorization by doing a \textbf{series of sparse triangular solves}.
\item Proceeding by construction, assume the $(n-1)$-dimensional leading block $L^\prime$ of the Cholesky factor $L$ of $A$ is already known, leading the following structure of the $LL^T=A$ factorization:
\vspace{.1cm}
\begin{center}
\includegraphics[height=2.5cm]{images/sparse-cholesky-block.png}
\end{center}
First, we have $[L^\prime,0_{(n-1)\times 1}]\begin{bmatrix}x\\w\end{bmatrix}=b$ which simplifies to $L^\prime x=b$.\vspace{.1cm}\\
Second, we have $[x^T\,w]\begin{bmatrix}x\\w\end{bmatrix}=a$ and $w>0$ so that $w=\sqrt{a-x^Tx}$.
\end{itemize}
\end{frame}

% Slide 06
\begin{frame}{Up-looking Cholesky algorithm, cont'd\textsubscript{1}}
\begin{itemize}
\item This leads to the following algorithm:
\begin{block}{Up-looking Cholesky algorithm}
Given a sparse SPD matrix $A\in\mathbb{R}^{n\times n}$, initialize $L^\prime:=\sqrt{a_{11}}$.\vspace{.1cm}\\
For $k=2,\dots,n$:\vspace{-.1cm}
\begin{itemize}
\item[-] Let the leading $k$-by-$k$ block of $A$ be written as $\begin{bmatrix}A^\prime & b\\b^T&a\end{bmatrix}$ where $A^\prime$ is the $(k-1)$-dimensional leading block, $b$ is $(k-1)$-by-1 and $a$ is a scalar.
\item[-] Solve for $x\in\mathbb{R}^{k-1}$ such that $L^\prime x=b$ where $L^\prime$ and $b$ are sparse.
\item[-] Compute $w:=\sqrt{a-x^Tx}$, and update
\begin{align*}
L^\prime:=
\begin{bmatrix}
L^\prime&0\\
x^T&w
\end{bmatrix}
\end{align*}
\end{itemize}
Return $L:=L^\prime$
\end{block}
Consequently, the sparse Cholesky factor $L$ of the sparse matrix $A$ is formed by performing $n-1$ sparse triangular solves of sizes $1,\dots,n-1$.
\end{itemize}
\end{frame}

% Slide 07
\begin{frame}{Up-looking Cholesky algorithm, cont'd\textsubscript{2}}
\begin{itemize}
\vspace{-.5cm}
\item Consider a matrix $A$ with the following non-zero pattern:
\raisebox{-.8cm}{\includegraphics[height=1.8cm]{images/non-zero-pattern.png}}
\item Since the 2-by-2 leading block $A^\prime$ of $A$ is diagonal, so is the corresponding 2-by-2 Cholesky factor $L^\prime$ such that $L^\prime {L^\prime}^T=A^\prime$.\\
\vspace{.1cm}
Let the vector
\raisebox{-.4cm}{\includegraphics[height=1cm]{images/sparse-b-2.png}}
complete the 3-by-3 leading block $\begin{bmatrix}A^\prime&b\\b^T&a\end{bmatrix}$ of $A$.\vspace{.1cm}\\
Then, the up-looking Cholesky algorithm requires that we do the sparse triangular solve of $L^\prime x=b$.\vspace{-.35cm}
\begin{center}
\raisebox{-.2cm}{\includegraphics[height=1.5cm]{images/triangular-system-block-2-by-2.png}}\hspace{1cm}
\includegraphics[height=2.5cm]{images/reach-block-2-by-2.png}
\includegraphics[height=1cm]{images/sparse-x-2.png}
\end{center}
\vspace{.05cm}
\item The Cholesky factor $L^\prime:=\begin{bmatrix}L^\prime&0\\x^T&w\end{bmatrix}$ of $\begin{bmatrix}A^\prime&b\\b^T&a\end{bmatrix}$  has structure \raisebox{-.5cm}{\includegraphics[height=1.2cm]{images/sparse-L-3.png}}.
\end{itemize}
\end{frame}

% Slide 08
\begin{frame}{Up-looking Cholesky algorithm, cont'd\textsubscript{3}}
\begin{itemize}
\vspace{-.5cm}
\item Consider a matrix $A$ with the following non-zero pattern:
\raisebox{-.8cm}{\includegraphics[height=1.8cm]{images/non-zero-pattern.png}}
\item 
Let the vector
\raisebox{-.7cm}{\includegraphics[height=1.5cm]{images/sparse-b-3.png}}
complete the 4-by-4 leading block $\begin{bmatrix}A^\prime&b\\b^T&a\end{bmatrix}$ of $A$.\vspace{.1cm}\\
Then, the up-looking Cholesky algorithm requires that we do the sparse triangular solve of $L^\prime x=b$.\vspace{-.35cm}
\begin{center}
\raisebox{-.2cm}{\includegraphics[height=1.7cm]{images/triangular-system-block-3-by-3.png}}\hspace{1cm}
\includegraphics[height=2.5cm]{images/reach-block-3-by-3.png}
\raisebox{-.25cm}{\includegraphics[height=1.7cm]{images/sparse-x-3.png}}
\end{center}
\vspace{.05cm}
\item The Cholesky factor $L^\prime:=\begin{bmatrix}L^\prime&0\\x^T&w\end{bmatrix}$ of $\begin{bmatrix}A^\prime&b\\b^T&a\end{bmatrix}$  has structure \raisebox{-.6cm}{\includegraphics[height=1.4cm]{images/sparse-L-4.png}}.
\end{itemize}
\end{frame}

% Slide 09
\begin{frame}{Up-looking Cholesky algorithm, cont'd\textsubscript{4}}
\begin{itemize}
\vspace{-.5cm}
\item Consider a matrix $A$ with the following non-zero pattern:
\raisebox{-.8cm}{\includegraphics[height=1.8cm]{images/non-zero-pattern.png}}
\item 
Let the vector
\raisebox{-.85cm}{\includegraphics[height=1.7cm]{images/sparse-b-4.png}}
complete the decomposition $\begin{bmatrix}A^\prime&b\\b^T&a\end{bmatrix}$ of $A$.\vspace{.1cm}\\
Then, the up-looking Cholesky algorithm requires that we do the sparse triangular solve of $L^\prime x=b$.\vspace{-.35cm}
\begin{center}
\raisebox{-.35cm}{\includegraphics[height=1.9cm]{images/triangular-system-block-4-by-4.png}}\hspace{.5cm}
\includegraphics[height=2.5cm]{images/reach-block-4-by-4.png}
\raisebox{-.15cm}{\includegraphics[height=1.7cm]{images/sparse-x-4.png}}
\end{center}
\vspace{-.1cm}
\item The Cholesky factor $L^\prime:=\begin{bmatrix}L^\prime&0\\x^T&w\end{bmatrix}$ of $\begin{bmatrix}A^\prime&b\\b^T&a\end{bmatrix}$  has structure \raisebox{-.6cm}{\includegraphics[height=1.7cm]{images/sparse-L-5.png}}.$\hspace{-2cm}$
\end{itemize}
\end{frame}

% Slide 10
\begin{frame}{Up-looking Cholesky algorithm, cont'd\textsubscript{5}}
\begin{itemize}
\item The up-looking Cholesky algorithm yield a factor with the following non-zero pattern:\\
\begin{center}
\includegraphics[height=2.5cm]{images/sparse-L.png}
\end{center}
\vspace{.1cm}
Note that the sparsity of $L$ resembles that of $A$, with additional \textbf{fill-ins}:\vspace{.2cm}\\
\begin{center}
\includegraphics[height=1.8cm]{images/A-vs-L.png}
\end{center}
\item While the up-looking algorithm is better than performing a dense Cholesky factorization, it does \textbf{require many DFS} in graphs.
\item We'll now try to do better than the up-looking algorithm.
\end{itemize}
\end{frame}

% Slide 11
\begin{frame}{Elimination tree}
\begin{itemize}
\item The \textbf{graph associated with} the sparsity pattern of \textbf{a Cholesky factor} $L$ has a special property which allows to \textbf{ignore many of its edges} and \textbf{retain the same reach}.
\item Consider what happens when we \textbf{ignore all the non-zero entries of $L$ below the first subdiagonal non-zero component}. E.g.,\vspace{.15cm}
\begin{center}
\includegraphics[height=1.8cm]{images/sparsified-L.png}
\end{center}
Removing these entries results in a \textbf{sparsification of the associated graph}:
\begin{center}
\includegraphics[height=2.3cm]{images/sparsified-graph.png}
\end{center}
\end{itemize}
\end{frame}

% Slide 12
\begin{frame}{Elimination tree, cont'd}
\begin{itemize}
\item[]
\begin{center}
\includegraphics[height=2.3cm]{images/sparsified-graph.png}
\end{center}
\item[] \textbf{Two general properties} are observed:
\begin{enumerate}
\item The \textbf{reach} of every vertex remains \textbf{unchanged} by sparsification.\vspace{.1cm}
\item \textbf{Every vertex} of the sparsified graph \textbf{has at most one edge} leading \textbf{out} of it.\vspace{.1cm}\\
I.e., \textbf{if} the graph is \textbf{connected}, then it is a \textbf{directed tree}.\vspace{.1cm}\\
\end{enumerate}
Remarks:
\begin{itemize}
\item The sparsified graph is called an \textbf{elimination tree}.\vspace{.1cm}
\item The elimination tree \textbf{may be disconnected}, in which case it is a forest, but even then, it will be called an elimination tree.
\item The elimination tree is an important data structure that \textbf{can be used to simplify} all \textbf{reach calculations} in a sparse Cholesky factorization.
\end{itemize}
\end{itemize}
\end{frame}

% Slide 13
\begin{frame}{Non-zero pattern of $L$}
\begin{itemize}
\item Say we aim to compute the $i$-th line of the Cholesky factor $L$ of an SPD $A$.
\item We are equipped with $L^\prime:=L[1:i-1,1:i-1]$:
\begin{center}
\includegraphics[height=5cm]{images/non-zero-L.png}
\end{center}
\item We saw the non-zero entries of $L[i,1:i-1]$ are the non-zero entries of the solution $x$ of the above system with right-hand side $b:=A[1:i-1,i]$.
\item Remember from our sparse triangular solves, $x_j=\ell_{ij}$ is non-zero either if\\ 
(a) $b_j=a_{ij}\neq 0$,
or if (b) $\exists\, k<j$ so that both $\ell_{jk}\neq 0$ and $x_k=\ell_{ik}\neq 0$.
\end{itemize}
\end{frame}

% Slide 14
\begin{frame}{Non-zero pattern of $L$, cont'd}
\begin{itemize}
\item Therefore, the pattern of non-zero values of $L$ is characterized as follows:
\begin{block}{Graph of (possible) non-zero entries of $L$}
Let $j<i$, then $\ell_{ij}$ is non-zero if\vspace{-.1cm}
\begin{itemize}
\item[(a)$\!\!$] $a_{ij}\neq 0$, or\vspace{-.1cm}
\item[(b)$\!\!$] there is a column index $k<j$ such that $\ell_{jk}$ and $\ell_{ik}$ are non-zero.\vspace{-.1cm}
\end{itemize}
We denote by $G_{ch}$ the graph with fewest edges that respect (a) and (b).\vspace{.1cm}\\
That is, $G_{ch}$ is the minimal graph such that $a_{ij}\neq 0\implies(j,i)\in G_{ch}$ and $(j,k),(i,k)\in G_{ch}\implies (j,i)\in G_{ch}$.
\end{block}
\item The graph $G_{ch}$ is a superset of the non-zero pattern of the Cholesky factor $L$ of $A$.\vspace{.1cm}\\
It can be that $(j,i)\in G_{ch}$ but $\ell_{ij}$ numerically cancels out.
However, if so, a tiny perturbation of $A$ with fixed sparsity is enough to make $\ell_{ij}\neq 0$.\vspace{.1cm}\\
Therefore, the graph $G_{ch}$ is best referred to as the graph of possible non-zero entries of the Cholesky factor $L$.
\end{itemize}
\end{frame}

% Slide 15
\begin{frame}{Definition of the elimination tree}
\begin{itemize}
\item The elimination tree can be defined as follows:
\begin{block}{Elimination tree}
Let $A$ be an SPD matrix, and $G_{ch}$ be the graph representing the non-zero entries of the Cholesky factor $L$ of $A$.\vspace{.1cm}\\
The elimination tree is obtained as follows.\vspace{.1cm}\\
For each node $i$ in $G_{ch}$:\vspace{-.1cm}
\begin{itemize}
\item[-] Let $V_i$ be the set of nodes $j$ of $G_{ch}$ for which there is an edge $(i,j)\in G_{ch}$, i.e., $V_i$ is the set of out-neighbors of $i$. 
Let $p_i=\min V_i$ be the smallest-indexed node in $V_i$.\vspace{-.1cm}
\item[-] Remove the edges $(i,j)$ for all $j\in V_i\setminus \{p_i\}$ from $G_{ch}$, i.e., remove all the the edges leaving $i$ except for $(i,p_i)$.\vspace{-.1cm}
\end{itemize}
The \textbf{elimination tree is} what's \textbf{left} of $G_{ch}$.
\end{block}
Example: \vspace{-.55cm}
\begin{center}
\includegraphics[height=1.8cm]{images/sparsified-graph.png}
\end{center}
\end{itemize}
\end{frame}

% Slide 16
\begin{frame}{Properties of the elimination tree}
\begin{itemize}
\item Since for each vertex $i$ in $G_{ch}$, the elimination tree is formed by removing all but one out-neighbors, each vertex is left with at most one single out-neighbor, and the elimination tree is indeed a tree, or at least a forest.
\item Consider the following example for a graph $G_{ch}$ of non-zero entries of the Cholesky factor $L$:
\vspace{.1cm}
\begin{center}
\includegraphics[height=1.8cm]{images/redundant-edge.png}
\end{center}
As $G_{ch}$ is, the reach of $k$ is $j,i$.\vspace{.05cm}\\
If $k<j<i$, the elimination tree is formed by removing the edge $(k,i)$.\vspace{.05cm}
Then, the reach of $k$ in the elimination tree is still $j,i$.\vspace{-.1cm}
\begin{theorem}[Conservation of reach]
For a given graph $G_{ch}$ of non-zero entries of a Cholesky factor $L$ of $A$, for any $1\leq i\leq n$, the reach of the corresponding elimination tree is the same as the reach of $i$ in $G_{ch}$.
\end{theorem}
\end{itemize}
\end{frame}

% Slide 17
\begin{frame}{Computing the elimination tree from $A$}
\begin{itemize}
\item Since the elimination tree has the same reach as $G_{ch}$, but is sparser than $G_{ch}$, it can be used to more efficiently identify the non-zero entries of the Cholesky factor.\vspace{.1cm}\\
For that, we need to figure out how to efficiently compute the elimination tree from the given sparsity pattern of $A$.
\item The idea behind computing the elimination tree of $A$ is to proceed one vertex at a time, maintaining a forest which contains all the vertices added so far.
$\!\!$The elimination tree shall be obtained once all the vertices are added.$\hspace{-1cm}$
\item Suppose we have a forest which has all the vertices $1,\dots,i-1$ at the correct place.
To proceed with the $i$-th vertex, if $a_{ik}\neq 0$ for some $k<i$, then we'll want $i$ to be in the reach of $k$.
In order to avoid potential redundant edges, we should then connect $i$ to whichever vertex $j$ which is at the leaf of the tree containing $k$.
\end{itemize}
\end{frame}

% Slide 18
\begin{frame}{Computing the elimination tree from $A$, cont'd}
\begin{itemize}
\item The pseudocode of the algorithm to build the elimination tree from the sparsity pattern of $A$ is given by
\vspace{.1cm}
\begin{itemize}
\item[1.] Initialize a forest $\mathcal{F}=\emptyset$ :\vspace{.1cm}
\item[2.] For $i=1,\dots,n$ :\vspace{.1cm}
\item[3.]\hspace{.4cm}Add vertex $i$ to $\mathcal{F}$\vspace{.1cm}
\item[4.]\hspace{.4cm}For all $k<i$ such that $a_{ik\neq 0}$ :\vspace{.1cm}
\item[5.]\hspace{.8cm}Find vertex $j$ at the leaf of $k$'s tree\vspace{.1cm} 
\item[6.]\hspace{.8cm}Add the edge $(j,i)$ to $\mathcal{F}$
\end{itemize}
\vspace{.15cm}
Taking the same sparse matrix $A$ as earlier, the elimination tree is then built as follows:
\begin{center}
\vspace{.1cm}
\includegraphics[trim={0 0 5cm .9cm},clip,height=1.9cm]{images/recall-elimination-tree.png}
\includegraphics[height=1.8cm]{images/elimination-tree.png}
\end{center}
We see that the same elimination tree is obtained as before.
\end{itemize}
\end{frame}

% Slide 19
\begin{frame}{Summary}
To compute a sparse Cholesky factor $L$ of a sparse matrix $A$, we
\begin{enumerate}
\item Build the elimination tree of $A$, at cost $\mathcal{O}(|A|)$, where $|A|$ is the number of non-zero entries in $A$.
\item Find the graph $G_{ch}$ of possible non-zero entries of $L$ using reaches of the elimination tree.
\item Perform the up-looking Cholesky factorization to build $L$.
\end{enumerate}
\vspace{.1cm}
Pseudocode of the up-looking Cholesky factorization to build row $k$ of $L$:
\begin{itemize}
\item[1.] $L[k,1:k]:=A[k,1:k]$ \vspace{.1cm}
\item[2.] For each $j<k$ such that $\ell_{kj}\neq 0$:\vspace{.1cm}
\item[3.]\hspace{.4cm}$\ell_{kj}\leftarrow\ell_{kj}/\ell_{jj}$\vspace{.1cm}
\item[4.]\hspace{.4cm}For each $i>j$ such that $\ell_{ij}\neq 0$ :\vspace{.1cm}
\item[5.]\hspace{.8cm} $\ell_{ki}\leftarrow\ell_{ki}-\ell_{ij}/\ell_{kj}$
\end{itemize}
\end{frame}


\section{Nested dissesction\\{\small Section 9.5 in Darve \& Wotters (2021)}}

% Slide 20
\begin{frame}{Reducing fill-ins in $L$}
\begin{itemize}
\item While the row and column permutations of a matrix do not really impact the solution of a linear system (i.e., $P_rAP_c\cdot P_c^Tx=P_rb$), they can have a significant impact on the sparsity pattern, i.e., the graph $G_{ch}$ of the Cholesky factor:
\begin{center}
\vspace{-.3cm}
\includegraphics[height=4cm]{images/permutations-fill-ins.png}
\end{center}
Even though only one row and one column are permuted between $A_1$ and $A_2$, the difference between the numbers of fill-ins in $L_1$ and $L_2$ is very significant.
\item \textbf{How should a matrix be ordered to reduce the number of fill-ins in the Cholesky factor $L$?}
\end{itemize}
\end{frame}

% Slide 21
\begin{frame}{One step of nested dissection}
\begin{itemize}
\item \textbf{Nested dissection} is a strategy for ordering a matrix $A$ in a way that \textbf{closely minimizes} the number of \textbf{fill-ins} in $L$.
\item \textbf{Nested dissection} is a \textbf{recursive method based on graph partitioning}.
\item Consider the symmetric matrix $A$ with an associated graph $G$.\vspace{.1cm}\\
Let the vertices of $G$ be decomposed in the disjoint union of $V_1,V_2$ and $S$, so that there are no edges between vertices of $V_1$ and $V_2$.\vspace{.1cm}\\
If $G_1$ and $G_2$ are the induced graph on $V_1$ and $V_2$, respectively, then we have
\begin{center}
\vspace{0cm}
\includegraphics[height=3cm]{images/separator.png}
\end{center}
where $S$ is referred to as a \textbf{separator}.
\end{itemize}
\end{frame}

% Slide 22
\begin{frame}{One step of nested dissection, cont'd\textsubscript{1}}
\begin{itemize}
\item A \textbf{node separator set} $S$ partitions the graph $G$ of $A$ into three disjoint sets of vertices $V_1,V_2$ and $S$ such that none of the nodes of $V_1$ are connected to any of the nodes of $V_2$, and vice-versa.
\item The removal of $S$ from the graph $G$ leads to two subgraphs $G_1$ and $G_2$, disconnected from each other. 
\item Consider what happens when we order the vertices as 
\begin{align*}
(\text{vertices of }G_1,\text{vertices of }G_2, S)
\end{align*}
we obtain the matrix
\begin{center}
\vspace{0cm}
\includegraphics[height=3.5cm]{images/once-dissected-A.png}
\end{center}
which is structurally close to the $A_1$ matrix with small number of fill-ins.
\end{itemize}
\end{frame}

% Slide 23
\begin{frame}{One step of nested dissection, cont'd\textsubscript{2}}
\begin{itemize}
\item[]
\begin{center}
\includegraphics[height=3.5cm]{images/once-dissected-A.png}
\vspace{-.15cm}
\end{center}
\item[] As a result of the block diagonal structure due to $G_1$ and $G_2$, the Cholesky factor $L$ of the reordered matrix will preserve a block diagonal structure.\vspace{.05cm}\\
If the blue blocks are dense, the sparsity of $L$ is exactly given by that of $A$.\vspace{.05cm}\\
In general, each block of $L$ will be sparse.\vspace{.05cm}\\
From here, we will proceed as follows:
\item \textbf{Entries in $S$}. For these entries, we give up and accept whatever fill-ins happen. 
Thus we want $S$ to be as small as possible.
\item \textbf{Entries in $G_1$ and $G_2$.}
For these entries, we will recurse on the blocks $G_1\times G_1$ and $G_2\times G_2$, i.e., find small separators $S_1$ and $S_2$ for $G_1$ and $G_2$, respectively, and so on.
\end{itemize}
\end{frame}

% Slide 24
\begin{frame}{Nested dissection}
\begin{itemize}
\item The basic idea of \textbf{nested dissection} is to \textbf{recursively} apply the procedure we just described, and yield a \textbf{nested dissection ordering} of the graph nodes. 
\begin{center}
\vspace{-.3cm}
\includegraphics[height=5cm]{images/pseudocode-nested-dissection.png}
\end{center}
\item The description of \texttt{find\_separator\_set} is beyond the scope of this class.\vspace{.05cm}\\
A good separator set has as few nodes as possible, and it decomposes the graph in roughly equally sized subgraphs.\vspace{.05cm}\\
Finding a good separator set is actually a NP-hard problem.
\end{itemize}
\end{frame}

% Slide 25
\begin{frame}{Nested dissection, cont'd}
\begin{itemize}
\item In the matrix, the recursive process of nested dissection looks like this:
\begin{center}
\vspace{.1cm}
\includegraphics[height=3cm]{images/recursive-dissection-matrices.png}
\end{center}
\item[] As we can see, if good separators are chosen, the "down-and-right-arrow" patterns shows up at all scales, and we can guarantee that more and more entries of $L$ will be zero.
\end{itemize}
\end{frame}

\section{Homework problems}

% Slide 26
\begin{frame}{Homework problem}\vspace{.1cm}
Turn in \textbf{your own} solution to the following problem:\vspace{.15cm}\\
\begin{minipage}[t]{0.1\textwidth}
\textbf{Pb.$\,$16}
\end{minipage}
\begin{minipage}[t]{0.89\textwidth}
Find the non-zero pattern of the Cholesky factor $L$ for the following matrix:
\begin{align*}
A=
\begin{bmatrix}
a_{11}&a_{12}&0     &a_{14}\\
a_{12}&a_{22}&0     &a_{24}\\
0     &0     &a_{33}&0     \\
a_{14}&a_{24}&0     &a_{44}    
\end{bmatrix}.
\end{align*}
Show your work using the up-looking Cholesky factorization algorithm.
\end{minipage}\vspace{.15cm}
\end{frame}

\end{document}
