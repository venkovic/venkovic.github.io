\documentclass[t,usepdftitle=false]{beamer}

\input{../../../tex-beamer-custom/preamble.tex}

\title[NLA for CS and IE -- Lecture 11]{Numerical Linear Algebra\\for Computational Science and Information Engineering}
\subtitle{\vspace{.3cm}Lecture 11\\Arnoldi and Lanczos Procedures}
\hypersetup{pdftitle={NLA-for-CS-and-IE\_Lecture11}}

\date[Summer 2025]{Summer 2025}

\author[nicolas.venkovic@tum.de]{Nicolas Venkovic\\{\small nicolas.venkovic@tum.de}}
\institute[]{Group of Computational Mathematics\\School of Computation, Information and Technology\\Technical University of Munich}

\titlegraphic{\vspace{0cm}\includegraphics[height=1.1cm]{../../../logos/TUM-logo.png}}

\begin{document}
	
\begin{frame}[noframenumbering, plain]
	\maketitle
\end{frame}
	
\myoutlineframe
	
% Slide 01
\begin{frame}{Krylov subspace methods for few eigenpairs}
\begin{itemize}
\item So far we've seen:
\begin{itemize}\normalsize
\item \textbf{Power iterations}, \textbf{inverse iterations} and \textbf{Rayleigh quotient iterations} to compute a single eigenpair
\item \textbf{QR iterations}, the \textbf{divide-and-conquer method} and the \textbf{method of bissection} to compute all the eigenpairs of a small-to-medium size and dense matrix
\item \textbf{LOBPCG} to compute a few extremal generalized eigenpairs of a large, possibly sparse matrix pencil $(A,B)$.
\end{itemize}
\item \textbf{Krylov subspace methods} are another set of iterative methods to compute a few eigenpairs of a large matrix $A$
\begin{itemize}\normalsize
\item We assume that the mapping $x\mapsto Ax$ can be operated efficiently, possibly because $A$ is sparse\vspace{.1cm}
\item We denote two methods in particular:
\begin{itemize}\normalsize
\item[-] The \textbf{Arnoldi} process is meant for non-symmetric matrices, and\vspace{.1cm}
\item[-] The \textbf{Lanczos} process, which was introduced later for symmetric matrices.
\end{itemize}
\end{itemize}
\end{itemize}
\end{frame}
		
	
\section{Arnoldi process\\{\small Section 6.1 in Darve \& Wootters (2021)}}

% Slide 02
\begin{frame}{Arnoldi process as a Krylov subspace method}
\begin{itemize}
\item Given a vector $v\in\mathbb{R}^n$, the $k$-th Krylov subspace of $A\in\mathbb{R}^{n\times n}$ is 
\begin{align*}
\mathcal{K}_k(A,v):=\mathrm{span}\{v,Av,\dots,A^{k-1}v\}.
\end{align*}
\item The Arnoldi process which we present in this section is a procedure to generate an orthogonal $Q_k:=[q_1,\dots,q_k]$, i.e., $Q_k^TQ_k=I_k$ such that 
\begin{align*}
\mathrm{span}\{q_1,\dots,q_k\}=\mathcal{K}_k(A,v).
\end{align*}
The orthonormal basis in the columns of $Q_k$ is such that $H_k:=Q_k^TAQ_k$ is an upper Hessenberg matrix. 
\item We present two different ways to derive Arnoldi procedures:
\begin{enumerate}\normalsize
\item Deduction of Arnoldi iteration from the $AQ=QH$ relation where $Q\in\mathbb{R}^{n\times n}$ is orthogonal and $H\in\mathbb{R}^{n\times n}$ is Hessenberg.\vspace{.1cm}
\item Orthogonalization of $Aq_k$ against $q_1,\dots,q_k$.
\end{enumerate}
\item Later, we see that approximate eigenpairs of $A$ can be sought within the Kylov subspace $\mathcal{K}_k(A,v)$.
\end{itemize}
\end{frame}

% Slide 03
\begin{frame}{Reduction to Hessenberg form}
\begin{itemize}
\item Householder transformations can be used to transform a matrix into Hessenberg form.
That is, there exists an orthogonal matrix $Q\in\mathbb{R}^{n\times n}$ s.t.
\begin{align*}
Q^TAQ=H
\end{align*}
\vspace{-.5cm}\\
is upper Hessenberg.
\item As with any similarity transformation, the eigenvalues of $H$ are the same as those of $A$, which can be exploited to find eigenvalues of $A$.
\item Instead of considering the full Hessenberg matrix $H$, we approximate eigenpairs of $A$ with eigenpairs of a leading block $H_k$ of $H$ with $k\ll n$.\vspace{.1cm}
\begin{center}
\includegraphics[height=2.5cm]{images/hessenberg-reduction.png}
\end{center}
\item As it turns out, the eigenpairs of the leading block $H_k$ of $H$ are good approximations of some eigenpairs of $A$.
\end{itemize}
\end{frame}

% Slide 04
\begin{frame}{Deducting the Arnoldi process from $AQ=QH$}
\begin{itemize}
\item Computing the leading block $H_k$ of $H$ is called the \textbf{Arnoldi process}.\\
One possible form of this process is similar to the Gram-Schmidt procedure.
\item Because $Q$ is orthogonal, we can rewrite $Q^TAQ=H$ as $AQ=QH$.
\item We'd like to design an iterative procedure to recover $Q$ and $H$.\vspace{.1cm}\\
\begin{itemize}\normalsize
\item[-] Suppose that we already have the $k$ first columns of $Q$, and the first $k-1$ columns of $H$. How can we recover the next columns of $Q$ and $H$?
\begin{center}
\includegraphics[height=5.25cm]{images/hessenberg-current.png}
\end{center}
\end{itemize}
\end{itemize}
\end{frame}

% Slide 05
\begin{frame}{Deducting the Arnoldi process from $AQ=QH$, cont'd\textsubscript{1}}
\begin{itemize}
\item The first thing we observe is that we can recover the entries $h_{ik}$ for $i\leq k$ using what we know, because $q_i^TAq_k=h_{ik}$.\\
Now that we know $h_{ik}$ for $i\leq k$, we focus on recovering $h_{k+1,k}$ and $q_{k+1}$.
To do this, from the $k$-th column of $AQ=QH$, we write
\begin{align*}
Aq_k=h_{1k}q_1+\dots+h_{kk}q_k+h_{k+1,k}q_{k+1}.
\end{align*}
\begin{center}
\includegraphics[height=5.25cm]{images/arnoldi-progress.png}
\end{center}
\end{itemize}
\end{frame}
	
% Slide 06
\begin{frame}{Deducting the Arnoldi process from $AQ=QH$, cont'd\textsubscript{2}}
\begin{itemize}
\item[] of which we can compute the left-hand side $Aq_k$, and the only part of the right-hand side that we don't know is the vector $h_{k+1,k}q_{k+1}$. So we can solve for it. Denote
\begin{align*}
r_k:=Aq_k-h_{1k}q_1-\dots-h_{kk}q_k=h_{k+1,k}q_{k+1}
\end{align*}
where $q_{k+1}$ has unit norm, so that
\begin{align*}
h_{k+1,k}=\|r_k\|_2,
\;q_{k+1}=r_k/h_{k+1,k}.
\end{align*}
(Note that we could also choose $h_{k+1,k}=-\|r_k\|_2$, which would lead to a different sign choice for $q_{k+1}$. This choice is arbitrary.)\vspace{.1cm}\\
Thus we have figured out $q_{k+1}$ and $h_{ik}$ for $i\leq k+1$.
This is what we wanted to know, and we can now proceed to the step to obtain $q_{k+2}$ and $h_{i,k+1}$ for $i\leq k+2$.
\end{itemize}
\end{frame}	
	
% Slide 07
\begin{frame}{Deducting the Arnoldi process from $AQ=QH$, cont'd\textsubscript{3}}
\begin{itemize}
\item Consequently, we have the following iteration:
	\smallskip
	\vspace{.1cm}
	\begin{center}
	$\hspace{-5cm}$Arnoldi recurrence relation:\tinyskip\\
	\small
	\fbox{\begin{varwidth}{10cm}
	Suppose we have $q_1,\dots,q_k$ and $h_{:,j}$ for $j<k$. 
	Then we can find $q_{k+1}$ and $h_{:,k}$ as follows:\tinyskip\\
	$\hspace*{.5cm}$for $i\leq k:$\tinyskip\\
	$\hspace*{1cm}h_{ik}:=q_i^TAq_k$\tinyskip\\
	$\hspace*{.5cm}r_k:=Aq_k-\sum_{i=1}^kh_{ik}q_i$\tinyskip\\
	$\hspace*{.5cm}h_{k+1,k}:=\|r_k\|_2$\tinyskip\\
	$\hspace*{.5cm}q_{k+1}:=\frac{r_k}{h_{k+1,k}}$
	\end{varwidth}}\end{center}
	\medskip
\item Performing this iteration starting from a given vector $q_1$, we get a method to calculate the columns of the matrices $H$ and $Q$ that satisfy $Q^TAQ=H$. This is, in a nuttshell, the Arnoldi process.
\end{itemize}
\end{frame}		
	
% Slide 08
\begin{frame}
\frametitle{Alternative way to define Arnoldi procedures}	
\begin{itemize}
\item Given a vector $X_{:,1}$, the Arnoldi procedure is defined by 
\begin{align*}
\mathrm{Arnoldi}:
(X_{:,1},k)\in\mathbb{R}^{n}\times\mathbb{N}
\mapsto
Q=[q_1,\dots,q_k]\in\mathbb{R}^{n\times k}
\end{align*}
s.t. $Q^TQ=I_k$ and $\mathrm{span}\{q_1,\dots,q_k\}=\mathrm{span}\{q_1,Aq_1,\dots,A^{k-1}q_1\}$ where $q_1:=X_{:,1}/\|X_{:,1}\|_2$.\vspace{.1cm}
\item $\!\!$We are interested by the QR decomposition $X\!\!=\!QR$ such that $X_{:,j}\!:=\!Aq_{j-1}\hspace{-1cm}$\\
$\!\!$for $j=2,\dots,k$.
$X$ is defined column-by-column with respect to $Q$, so that\\
$\!\!$the Gram-Schmidt procedure is particularly well adapted.\vspace{.1cm}
\item Let $\Pi^{(j)}$ be a projector onto $\mathrm{Span}\{q_1,\dots,q_j\}^\perp$, then Arnoldi$(X_{:,1},k)$ is given by the following GS procedure:
\begin{algorithm}[H]
\small
\caption{$\mathrm{Arnoldi}:(X_{:,1},k)\mapsto Q$}
\begin{algorithmic}[1]
\STATE{$q_1:=X_{:,1}/\|X_{:,1}\|_2$}
\FOR{$j=2,\dots,k$}
\STATE{$X_{:,j}:=Aq_{j-1}$}
\STATE{$q_{j}:=\Pi^{(j-1)}X_{:,j}$}
\STATE{$q_{j}:=q_{j}/\|q_{j}\|_2$}
\ENDFOR
\end{algorithmic}
\end{algorithm}		
\end{itemize}
\end{frame}   

% Slide 09
\begin{frame}
\frametitle{Matrices of interest and notation}	
\begin{itemize}
\item From the orthogonality of $Q$, the $QR$ decomposition of $X$ is such that $Q^TX=R$. 
Given that $X_{:,j}=Aq_{j-1}$ for $j=2,\dots,k$, we have $R_{ij}=Q_{:,i}^TX_{:,j}=q_i^TAq_{j-1}$ for $(i,j)\in[1,k]\times[2,k]$.
%\item Given that $q_j\propto\Pi^{(j-1)}Aq_{j-1}$ and $\Pi^{(j)}:\mathbb{R}^n\rightarrow\mathrm{Span}\{q_1,\dots,q_j\}^\perp$, we have $R_{ij}=q_{i}^TAq_{j-1}=0$ for $i>j$, so that $R$ is upper triangular.
\vspace{.1cm}
\item In the Arnoldi procedure, we are interested in some of the components of $R$.
In particular, we wish to compute the matrix defined by $H:=Q^TAQ$.
The components of $H$ are given by $h_{ij}=q_i^TAq_j$.
%\item Given that $q_j\propto\Pi^{(j-1)}Aq_{j-1}$ and $\Pi^{(j)}:\mathbb{R}^n\rightarrow\mathrm{Span}\{q_1,\dots,q_j\}^\perp$, we have $H_{ij}=q_i^TAq_j=0$ for $i>j$, i.e., $H$ is upper Hessenberg.
\vspace{.1cm}
\item So as to explicitly state the dimension of $Q$ during intermediate states $j<k$ of the Arnoldi algorithm, we write $Q_j:=[q_1,\dots,q_j]$.
Similarly, we denote the corresponding matrix by $H_j:=Q_j^TAQ_j$.
\vspace{.1cm}
\item Some properties of the Arnoldi procedure rely on the matrix defined by $\underline{H}_j:=Q_{j+1}^TAQ_j$.
\end{itemize}
\end{frame}   

% Slide 10
\begin{frame}
\frametitle{CGS-based Arnoldi procedure}	
\begin{itemize}
\item For the CGS-based Arnoldi procedure, we let $\Pi^{(j)}:=I_n-Q_jQ_j^T$.
\item We obtain the following algorithm:
\begin{algorithm}[H]
\small
\caption{CGS-based Arnoldi$:(X_{:,1},k)\mapsto Q_k$}
\begin{algorithmic}[1]
\STATE{$q_{1}:=X_{:,1}/\|X_{:,1}\|_2$}
\FOR{$j=2,\dots,k$}
\STATE{$X_{:,j}:=Aq_{j-1}$}
\STATE{$H_{1:j-1,j-1}:=Q_{j-1}^TX_{:,j}$}
\STATE{$q_{j}:=X_{:,j}-Q_{j-1}H_{1:j-1,j-1}$}
\STATE{$q_{j}:=q_{j}/\|q_{j}\|_2$}
\ENDFOR
\end{algorithmic}
\end{algorithm}		
\vspace{-.4cm}
\item Let $\|q_j\|_2$ be computed after line~5, then, after line~6, we have
\begin{align*}
\|q_j\|_2q_j=&\;(I_n-Q_{j-1}Q_{j-1}^T)Aq_{j-1}\\
\|q_j\|_2q_j^Tq_j=&\;q_j^T(I_n-Q_{j-1}Q_{j-1}^T)Aq_{j-1}\\
\|q_j\|_2=&\;q_j^TAq_{j-1}\\
\|q_j\|_2=&\;h_{j,j-1}.
\end{align*}
\end{itemize}
\end{frame}   

% Slide 10
\addtocounter{framenumber}{-1}
\begin{frame}
\frametitle{CGS-based Arnoldi procedure}	
\begin{itemize}
\item For the CGS-based Arnoldi procedure, we let $\Pi^{(j)}:=I_n-Q_jQ_j^T$.
\item We obtain the following algorithm:
\begin{algorithm}[H]
\small
\caption{CGS-based Arnoldi$:(X_{:,1},k)\mapsto Q_k$}
\begin{algorithmic}[1]
\STATE{$q_{1}:=X_{:,1}/\|X_{:,1}\|_2$}
\FOR{$j=2,\dots,k$}
\STATE{$X_{:,j}:=Aq_{j-1}$}
\STATE{$H_{1:j-1,j-1}:=Q_{j-1}^TX_{:,j}$}
\STATE{$q_{j}:=X_{:,j}-Q_{j-1}H_{1:j-1,j-1}$}
\STATE{$h_{j,j-1}:=\|q_{j}\|_2$}\COMMENT{$H_{j+1:k,j-1}:=0$}
\STATE{$q_{j}:=q_{j}/H_{j,j-1}$}
\ENDFOR
\end{algorithmic}
\end{algorithm}		
\end{itemize}
\end{frame}   

% Slide 11
\begin{frame}
\frametitle{Hessenberg matrices and property of the Arnoldi algorithm}	
\begin{itemize}
\item From lines~4-7 of the algorithm, we have
\begin{align*}
h_{j,j-1}q_j=&\;(I_n-Q_{j-1}Q_{j-1}^T)Aq_{j-1},\\
h_{j,j-1}q_i^Tq_j=&\;q_i^T(I_n-Q_{j-1}Q_{j-1}^T)Aq_{j-1}.
\end{align*}
Let $i>j$, then we have $q_i^TAq_{j-1}=0$ so that $h_{ij}=0$ for $i>j+1$, i.e., $H_j$ is upper Hessenberg.	
\vspace{.1cm}	
\item We have $Aq_j=\sum_{i=1}^{j+1}h_{ij}q_i$.\\
\vspace{.1cm}
\emph{Proof}\,:
From lines 4-7 of the algorithm, we have
\begin{align*}
h_{j,j-1}q_j=&\;Aq_{j-1}-Q_{j-1}Q_{j-1}^TAq_{j-1}\\
h_{j+1,j}q_{j+1}=&\;Aq_{j}-Q_{j}Q_{j}^TAq_{j}\\
h_{j+1,j}q_{j+1}=&\;Aq_{j}-Q_{j}H_{1:j,j}
\end{align*}
\vspace{-.2cm}
so that we can write
\vspace{-.2cm}
\begin{align*}
Aq_{j}=
\left[\begin{matrix}q_1&\dots&q_j\end{matrix}\right]
\left[\begin{matrix}h_{1j}\\\vdots\\h_{jj}\end{matrix}\right]+h_{j+1,j}q_{j+1}.
\hspace{.4cm}\qedsymbol{}
\end{align*}
\end{itemize}
\end{frame} 

% Slide 12
\begin{frame}
\frametitle{The Arnoldi relation}	
\begin{itemize}
\item Writing down the components of $Q_jH_j$ and $AQ_j$ leads us to
\vspace{-.2cm}
\begin{align*}
\left[\begin{matrix}
\sum_{i=1}^{j}h_{i1}q_i&\dots&\sum_{i=1}^{j}h_{ij}q_i\end{matrix}\right]=&\;Q_jH_j,\\
\left[\begin{matrix}Aq_1&\dots&Aq_j\end{matrix}\right]=&\;AQ_j.
\end{align*}
\item Then, using the fact that $Aq_j=\sum_{i=1}^{j+1}h_{ij}q_i$ and that $H_j$ is upper Hessenberg, we have $h_{j+1,i}=0$ for $i=1,\dots,j-1$ so that
\vspace{-.2cm}
\begin{align*}
\left[\begin{matrix}Aq_1&\dots&Aq_j\end{matrix}\right]=
&\;\left[\begin{matrix}
\sum_{i=1}^{j}h_{i1}q_i&\dots&\sum_{i=1}^{j}h_{ij}q_i\end{matrix}\right]+\\
&\;\left[\begin{matrix}
0&\dots&0&h_{j+1,j}q_{j+1}\end{matrix}\right]
\end{align*}
\vspace{-.5cm}\\
which can be written as $\boxed{AQ_j=Q_jH_j+h_{j+1,j}q_{j+1}e_j^T}$ where $e_j$ is the $j$-th column of the $j$-dimensional identity matrix. 
Note the difference with the relation $AQ=QH$ obtained only when $j=n$.
\item Similarly, we have
\vspace{-.25cm}
\begin{align*}
\left[\begin{matrix}Aq_1&\dots&Aq_j\end{matrix}\right]=
&\;\left[\begin{matrix}
\sum_{i=1}^{j+1}h_{i1}q_i&\dots&\sum_{i=1}^{j+1}h_{ij}q_i\end{matrix}\right]
\end{align*}
\vspace{-.35cm}\\
which can be written as $\boxed{AQ_j=Q_{j+1}\underline{H}_j}$.
\end{itemize}
\end{frame} 

% Slide 13
\begin{frame}
\frametitle{CGS-based Arnoldi procedure}	
\begin{itemize}
\item Approximate solutions in $\mathcal{K}_k(A,r_0)$ for linear systems and eigenvalue problems have residuals which depend on the product $AQ_k$.
\item Exploit the Arnoldi relation $AQ_k=Q_{k+1}\underline{H}_k$ for faster computation.
\item The Arnoldi algorithm is reformulated as follows so as to compute $\underline{H}_k$ at the $k$-th iteration:
\begin{algorithm}[H]
\small
\caption{CGS-based Arnoldi}%: $(X_{:,1},m)\mapsto(Q_{m+1},\underline{H}_m)$}
\begin{algorithmic}[1]
\STATE{$q_{1}:=X_{:,1}/\|X_{:,1}\|_2$}
\FOR{$j=1,\dots,k$}
\STATE{$X_{:,j+1}:=Aq_{j}$}
\STATE{$H_{1:j,j}:=Q_{j}^TX_{:,j+1}$}
\STATE{$q_{j+1}:=X_{:,j+1}-Q_{j}H_{1:j,j}$}
\STATE{$H_{j+1,j}:=\|q_{j+1}\|_2$}\COMMENT{$H_{j+2:k+1,j}:=0$}
\STATE{$q_{j+1}:=q_{j+1}/H_{j+1,j}$}
\ENDFOR
\end{algorithmic}
\end{algorithm}		
\item The approach of the book of Darve and Wootters (2021) we presented is equivalent to CGS-based Arnoldi.		
\end{itemize}
\end{frame}   

% Slide 14
\begin{frame}
\frametitle{MGS-based Arnoldi procedure}	
\begin{itemize}
\item MGS-based Arnoldi $\implies$ $\Pi^{(i)}:=(I_n-q_{i}q_{i}^T)\dots(I_n-q_{1}q_{1}^T)$.
\item We obtain the following algorithm:
\begin{algorithm}[H]
\small
\caption{MGS-based Arnoldi$:(X_{:,1},k)\mapsto Q_k$}
\begin{algorithmic}[1]
\STATE{$q_{1}:=X_{:,1}/\|X_{:,1}\|_2$}
\FOR{$j=2,\dots,k$}
\STATE{$q_{j}:=Aq_{j-1}$}
\FOR{$i=1,\dots,j-1$}
\STATE{$q_{j}:=q_j-q_{i}q_{i}^Tq_j$}
\ENDFOR
\STATE{$q_{j}:=q_{j}/\|q_{j}\|_2$}
\ENDFOR
\end{algorithmic}
\end{algorithm}		
\item For all $(i,j)\in[1,j-1]\times[2,k]$, prior to executing line 5, we have
\begin{align*}
q_{j}=(I_n-q_{i-1}q_{i-1}^T)\dots(I_n-q_{1}q_{1}^T)Aq_{j-1}
\end{align*}
so that, assuming perfect orthogonality of $Q_j$, we have
\begin{align*}
q_{i}^Tq_{j}=&\;q_{i}^T(I_n-q_{i-1}q_{i-1}^T)\dots(I_n-q_{1}q_{1}^T)Aq_{j-1}=q_{i}^TAq_{j-1}=h_{i,j-1}.
\end{align*}
\end{itemize}
\end{frame}   

% Slide 15
\begin{frame}
\frametitle{MGS-based Arnoldi procedure}	
\begin{itemize}
\item Also, when computing $\|q_j\|_2$ prior to line~7, we have $\|q_j\|_2=h_{j,j-1}$.
\item We obtain the following algorithm:
\begin{algorithm}[H]
\small
\caption{MGS-based Arnoldi$:(X_{:,1},k)\mapsto Q_k$}
\begin{algorithmic}[1]
\STATE{$q_{1}:=X_{:,1}/\|X_{:,1}\|_2$}
\FOR{$j=2,\dots,k$}
\STATE{$q_{j}:=Aq_{j-1}$}
\FOR{$i=1,\dots,j-1$}
\STATE{$H_{i,j-1}:=q_{i}^Tq_j$}
\STATE{$q_{j}:=q_j-H_{i,j-1}q_{i}$}
\ENDFOR
\STATE{$H_{j,j-1}:=\|q_{j}\|_2$}\COMMENT{$H_{j+1:k,j-1}:=0$}
\STATE{$q_{j}:=q_{j}/h_{j,j-1}$}
\ENDFOR
\end{algorithmic}
\end{algorithm}		
\vspace{-.4cm}
\item All the properties we showed for the CGS-based Arnoldi procedure remain valid for MGS-based Arnoldi.
\end{itemize}
\end{frame}   

% Slide 16
\begin{frame}
\frametitle{MGS-based Arnoldi procedure}	
\begin{itemize}
\item Similarly as before, we want the upper Hessenberg matrix $\underline{H}_k$ to be computed at the end of the $k$-th iteration.
\item Consequently, the Arnoldi algorithm is reformulated as follows:
\begin{algorithm}[H]
\small
\caption{MGS-based Arnoldi}%$:(X_{:,1},m)\mapsto (Q_{m+1},\underline{H}_m)$}
\begin{algorithmic}[1]
\STATE{$q_{1}:=X_{:,1}/\|X_{:,1}\|_2$}
\FOR{$j=1,\dots,k$}
\STATE{$q_{j+1}:=Aq_{j}$}
\FOR{$i=1,\dots,j$}
\STATE{$H_{ij}:=q_{i}^Tq_{j+1}$}
\STATE{$q_{j+1}:=q_{j+1}-H_{ij}q_{i}$}
\ENDFOR
\STATE{$H_{j+1,j}:=\|q_{j+1}\|_2$}\COMMENT{$H_{j+2:k+1,j}:=0$}
\STATE{$q_{j+1}:=q_{j+1}/h_{j+1,j}$}
\ENDFOR
\end{algorithmic}
\end{algorithm}		
\item MGS-based Arnoldi is the most commonly used implementation of Arnoldi process.
\item Other variants include CGS2 and Householder-based Arnoldi.
\end{itemize}
\end{frame}   

\section{Arnoldi Rayleigh-Ritz for dominant eigenpairs}

% Slide 17
\begin{frame}{Arnoldi procedure with Rayleigh-Ritz vectors}
\begin{itemize}
\item \textbf{Eigenvectors with eigenvalues whose norms are the largest} among the spectrum of $A$ tend to be \textbf{well approximated by Rayleigh-Ritz} projections, as explained by Parlett (1998) and Saad (2011).
\item Rayleigh-Ritz projections are \textbf{commonly defined with respect to Krylov subspaces} whose bases $Q_k$ are obtained by a Arnoldi procedure:
\begin{itemize}\normalsize
\item Then, a \textbf{Rayleigh-Ritz vector} $y\in\text{range}(Q_k)$ approximates an eigenvector of $A$ with the \textbf{Ritz value} $\lambda$ such that $Ay-\lambda y\perp \text{range}(Q_k).\hspace{-1cm}$\\
That is, we \textbf{search for} $(\lambda,\hat{y})\in\mathbb{C}\times\mathbb{C}^{k}\setminus\{0\}$ s.t.$\!$ $z^H\left(Ay-\lambda y\right)=0$
$\forall\; z\in\text{range}(Q_k)$ with $\!y=Q_k\hat{y}$.
This \textbf{simplifies to}
\begin{align*}
Q_k^T\left(AQ_k\hat{y}-\lambda Q_k\hat{y}\right)=&\;0\\
H_k\hat{y}-\lambda\hat{y}=&\;0
\;\;\implies\;\;
\boxed{H_k\hat{y}=\lambda\hat{y}}
\end{align*}
where use was made of the Arnoldi relation and $Q_k^TQ_k=I_k$.
\item $nev<k$ \textbf{dominant eigenpairs} $\{(\lambda_\ell,\hat{y}_\ell)\}_{\ell=1}^{nev}$ \textbf{of} $H_k$ are \textbf{used to appro-}$\hspace{-1cm}$\\
\textbf{-ximate$\!$ dominant eigenpairs of} $\!A$ \textbf{with} $\!\{(\lambda_\ell,y_\ell)\}_{\ell=1}^{nev}$ where $y_\ell\!:=\!Q_k\hat{y}_\ell.\hspace{-1cm}$
\end{itemize}
\end{itemize}
\medskip
\tiny{B. N. Parlett, The symmetric eigenvalue problem. Society for Industrial and Applied Mathematics (1998).}\tinyskip\\
\tiny{Y. Saad, Numerical methods for large eigenvalue problems: revised edition. Society for Industrial and Applied Mathematics (2011).}
\end{frame}

% Slide 18
\begin{frame}{Arnoldi procedure with Rayleigh-Ritz vectors, cont'd\textsubscript{1}}
\begin{itemize}
\item A desirable property of the Rayleigh-Ritz approximation $y_\ell$ is that \textbf{the Ritz value} $\theta_\ell$ \textbf{equates the corresponding Rayleigh quotient}:
\begin{align*}
y_\ell^HAy_\ell=(Q_k\hat{y}_\ell)^HAQ_k\hat{y}_\ell=\hat{y}^H_\ell Q_k^TAQ_k\hat{y}_\ell=
\hat{y}_\ell^HH_k\hat{y}_\ell=\lambda_\ell\hat{y}_\ell^H\hat{y}_\ell=\lambda_\ell
\end{align*}
where $\hat{y}_\ell$ is assumed to have unit length.
\item The \textbf{eigen-residual} $\tilde{r}_\ell:=Ay_\ell-\lambda_\ell y_\ell$ of the Rayleigh-Ritz vector $y_\ell$ is s.t.
\begin{align*}
\tilde{r}_\ell=&\;AQ_k\hat{y}_\ell-\lambda_\ell Q_k\hat{y}_\ell\\
=&\;Q_{k}H_m\hat{y}_\ell+h_{k+1,k}q_{k+1}e_k^T\hat{y}_\ell-\lambda_\ell Q_k\hat{y}_\ell\\
=&\;\lambda_\ell Q_k\hat{y}_\ell+h_{k+1,k}q_{k+1}e_k^T\hat{y}_\ell-\lambda_\ell Q_k\hat{y}_\ell\\
=&\;h_{k+1,k}q_{k+1}e_k^T\hat{y}_\ell
\end{align*}
\vspace{-1.1cm}\\
\begin{align*}
\hspace{.55cm}\boxed{\tilde{r}_\ell=\beta_{k,\ell}q_{k+1}}
\hspace{.5cm}\mathrm{where}\;\;\boxed{\beta_{k,\ell}:=h_{k+1,k}e_k^T\hat{y}_\ell}.
\end{align*}
\begin{itemize}\normalsize
\item Essentially, \textbf{the eigen-residuals} $\tilde{r}_1,\dots,\tilde{r}_k$ \textbf{of the Rayleigh-Ritz vectors} $y_1,\dots,y_k$ defined with respect to the \textbf{Krylov subspace} $\mathcal{K}_k(A,q_1)$ are \textbf{all parallel}, \textbf{along the Arnoldi vector} $q_{k+1}$.
\end{itemize}
\end{itemize}
\end{frame}

% Slide 19
\begin{frame}{Arnoldi procedure with Rayleigh-Ritz vectors, cont'd\textsubscript{2}}
\begin{itemize}
\item  From the fact that $\tilde{r}_\ell=\beta_{k,\ell}q_{k+1}$, the norm of the eigen-residual is such that $\|\tilde{r}_\ell\|_2^2=|\beta_{k,\ell}|^2q_{k+1}^Tq_{k+1}=|\beta_{k,\ell}|^2$ where $|\beta_{k,\ell}|=|h_{k+1,k}|\,|e_k^T\hat{y}_\ell|$.
\begin{itemize}\normalsize
\item Consequently, \textbf{a stopping criterion of the form} $\|\tilde{r}_\ell\|_2<\epsilon|\lambda_\ell|$ \textbf{can be checked efficiently} at every iteration \textbf{without} having to compute the \textbf{matrix-vector product} $Ay_\ell$ or even to assemble the vector $y_\ell:=Q_k\hat{y}_\ell$.
\end{itemize}
\item As explained earlier,
\begin{itemize}\normalsize
\item the \textbf{orthogonalization} which is at the root of the Arnoldi procedure has time complexity $\mathcal{O}(k^2n)$,
\item the \textbf{reduced eigensolve} of $H_m$ has time complexity $\mathcal{O}(k^3)$,
\item the \textbf{storage of the Arnoldi basis} in  $Q_k$ has space complexity $\mathcal{O}(kn)$ so that, \textbf{if convergence is not achieved} for some number $k$ of iterations, it is \textbf{necessary to start the Arnoldi procedure over} with a new initial vector $q_1$.
\end{itemize}
\item A \textbf{naive restart} of the Arnoldi procedure \textbf{can be highly detrimental to} the \textbf{convergence} of approximate eigenvectors. Some \textbf{care needs to be taken} so as \textbf{to reduce convergence slowdown}.
\end{itemize}
\end{frame}

\section{Shift-and-invert Arnoldi Rayleigh-Ritz for interior eigenpairs}

% Slide 20
\begin{frame}{Shift-and-invert spectral transformation}
	\begin{itemize}
	\item Rayleigh-Ritz pairs $(\lambda,y)$ converge first towards eigenpairs $(\theta,z)$ of $A$ with the largest value of $|\theta|$.
	\begin{itemize}\normalsize
	\item In practice, we may want to approximate an eigenpair with eigenvalue $\theta$ close to some $\sigma$, i.e., with small value of $|\sigma-\theta|$. E.g., $\sigma=0$.
	\item Rayleigh-Ritz approximations $(\lambda,y)$ of such eigenpairs $(\theta,z)$ in Krylov subspaces converge very slowly when $|\sigma|$ is small compared to the spectral radius of $A$.
	\end{itemize}
	\item The shift-and-invert spectral transformation was introduced by Ericsson and Ruhe (1980) as a means to circumvent this issue:
	\begin{itemize}\normalsize
	\item Consider the eigenvalue problem given by
	\begin{align*}
		(A-\sigma I_n)^{-1}w=\vartheta w 
	\end{align*}
	where it is assumed that $\sigma$ is not an eigenvalue of $A$. Then, we have
	\begin{align*}
		w=\vartheta (A-\sigma I_n)w\\
		w=\vartheta Aw- \vartheta\sigma w.
	\end{align*}
	\end{itemize}
	\end{itemize}
	\medskip
	\tiny{Ericsson, T., \& Ruhe, A. (1980). The spectral transformation Lanczos method for the numerical solution of large sparse generalized symmetric eigenvalue problems. Mathematics of Computation, 35(152), 1251-1268.}
\end{frame}

% Slide 21
\begin{frame}{Shift-and-invert spectral transformation, cont'd}
	\begin{itemize}
	\item[]
	\begin{itemize}\normalsize
	\item[] Since $\sigma$ is not an eigenvalue of the non-singular matrix $A$, the shift-and-invert operator $(A-\sigma I_n)^{-1}$ is not singular and $\vartheta\neq 0$ so that 
	\begin{align*}
		Aw=\left(\sigma+\frac{1}{\vartheta}\right)w.
	\end{align*}
	Essentially, $(\sigma+1/\vartheta,w)$ is an eigenpair of $A$.
	\item Now, if an Arnoldi procedure is applied to $(A-\sigma I_n)^{-1}$, the corresponding Rayleigh-Ritz pairs will first converge to the eigenpairs $(\vartheta,w)$ of the shift-and-invert operator with largest $|\vartheta|$.
	\item However, when $|\vartheta|$ is maximized, the magnitude of $\sigma-(\sigma+1/\vartheta)$ is minimized. Therefore, the Rayleigh-Ritz pairs of a shift-and-invert Arnoldi procedure will first converge to the eigenpairs of $A$ with eigenvalues closest to $\sigma$.
	\end{itemize}
   	\item Shift-and-invert operators are implemented in ARPACK to compute interior eigenpairs.
   	\item Shift-and-invert Arnoldi procedures rely on repetitive applications of the $(A-\sigma I_n)^{-1}$ operator.
	\end{itemize}
\end{frame}

\section{Arnoldi harmonic Ritz for interior eigenpairs}

% Slide 22
\begin{frame}{Harmonic Ritz approximation of interior eigenpairs}
	\begin{itemize}
	\item While shift-and-invert Arnoldi procedures allow fast convergence of Rayleigh-Ritz pairs towards interior eigenpairs, it comes at the cost of repeated applications of $(A-\sigma I_n)^{-1}$. However:
	\begin{enumerate}\normalsize
	\item[1.] Factorizing the shifted operator $A-\sigma I_n$ is not always possible.\vspace{.1cm}
	\item[2.] One may actually need to generate a basis for a Krylov subspace of $A$, and have little use for a basis of Krylov subspace of the shift-and-invert operator $(A-\sigma I_n)^{-1}$:
	\begin{itemize}\normalsize
	\item E.g., if interior eigenvectors of $A$ are needed to restart GMRES when solving $Ax=b$. 
	\end{itemize}
	\end{enumerate}
	\item As a means to bypass the need to apply shift-and-invert operators,\\ Morgan (1991) introduces a new projection method in which the shift-and-invert operator is applied implicitly:
	\begin{itemize}\normalsize
	\item Consider the case in which we are equipped with a basis for the search space $\text{range}(P)$ stored in the columns of $P$.\vspace{.025cm}
	\item Let $Q:=(A-\sigma I_n)P$, and consider the Rayleigh-Ritz pairs of the shift-and-invert operator $(A-\sigma I_n)^{-1}$ with respect to $\text{range}(Q)$.
	\end{itemize}
	\end{itemize}
	\smallskip
	\tiny{Morgan, R. B. (1991). Computing interior eigenvalues of large matrices. Linear Algebra and its Applications, 154, 289-309.}
\end{frame}

% Slide 23
\begin{frame}{Harmonic Ritz approximation of interior eigenpairs, cont'd\textsubscript{1}}
\begin{itemize}
\item[]
\begin{itemize}\normalsize
\item[] That is, consider the pair $(\vartheta,Q\hat{y})$ such that\vspace{-.075cm}
\begin{align*}
Q^H(A-\sigma I_n)^{-1}Q\hat{y}=&\;\vartheta Q^HQ\hat{y},
\end{align*}
$\vspace{-.6cm}$\\	
which develops into the reduced generalized eigenvalue problem
\begin{align*}
P^T(A-\sigma I_n)^HP\hat{y}=&\;\vartheta P^T(A-\sigma I_n)^H(A-\sigma I_n)P\hat{y}
\end{align*}
$\vspace{-.6cm}$\\	
which does not require any application of the shift-and-invert operator.
\item Resulting from a Rayleigh-Ritz projection of the shift-and-invert operator $(A-\sigma I_n)^{-1}$, the pair $(\sigma+1/\vartheta,Q\hat{y})$ should be a good approximation with respect to $\text{range}(Q)$ of the eigenpair closest to $\sigma$.
	\item As good of an approximation $Q\hat{y}$ might be, $P\hat{y}=(A-\sigma I_n)^{-1}Q\hat{y}$ is the first power iterate of the shift-and-invert operator initiated with $Q\hat{y}$, so that $P\hat{y}$ should be an even slightly better approximation of the eigenvector with eigenvalue closest to $\sigma$.
	\item Stewart (2001) showed that solutions $(\theta,Q\hat{y})$ for which $Q\hat{y}$ has unit norm are such that$\|Ay_i\|\leq|\theta_i|$, so that it is guaranteed that $\|\tilde{r}_i\|_2$ is small if $\theta_i$ is near zero.
	\end{itemize}
	\end{itemize}
	\medskip
	\tiny{Morgan, R.B. (1991).$\!$ Computing interior eigenvalues of large matrices. Linear Algebra and its Applications, $\!$154, $\!$289-309.$\hspace{-1cm}$}\tinyskip\\
	\tiny{G. W. Stewart, Matrix Algorithms II: Eigensystems, SIAM, Philadelphia, (2001).}
\end{frame}

% Slide 24
\begin{frame}{Harmonic Ritz approximation of interior eigenpairs, cont'd\textsubscript{2}}
	\begin{itemize}
	\item[]
	\begin{itemize}\normalsize
	\item Consequently, Morgan (1991) proposes a Petrov-Galerkin projection and seeks for pairs $(\sigma+\lambda,y)$ to approximate eigenpairs of $A$ near $\sigma$ with respect to $\text{range}(P)$, leading to the following procedure:
	\begin{align*}
		\text{Find $\lambda$ and }
		y\in\text{range}(P)
		\text{ s.t. }
		(A-\sigma I_n)y-\lambda y\perp(A-\sigma I_n)\text{range}(P),
	\end{align*}
	which first converges to eigenpairs of $A$ near $\sigma$, thus motivating the selection of reduced generalized eigenpairs $(\lambda,\hat{y})$ with smallest values of $|\lambda|$ such that
	\begin{align*}
	P^T(A-\sigma I_n)^H(A-\sigma I_n)P\hat{y}=\lambda P^T(A-\sigma I_n)^HP\hat{y}.
	\end{align*}		
	\item The projection proposed by Morgan (1991) is first studied for symmetric matrices, then further analyzed and first referred to as harmonic Ritz by Paige et al. (1995) before being considered in the context of non-symmetric eigenvalue problems by Sleijpen and Van der$\!$ Vorst$\!$ (1996).$\hspace{-1cm}$
	\end{itemize}
	\end{itemize}
	\medskip
	\tiny{Morgan, R. B. (1991). Computing interior eigenvalues of large matrices. Linear Algebra and its Applications, 154, 289-309.}\tinyskip\\
	\tiny{Paige, C. C., Parlett, B. N., \& Van der Vorst, H. A. (1995). Approximate solutions and eigenvalue bounds from Krylov subspaces. Numerical linear algebra with applications, 2(2), 115-133.}\tinyskip\\
	\tiny{Sleijpen, G. L., \& Van der Vorst, H. A. (1996). A Jacobi–Davidson Iteration Method for Linear Eigenvalue Problems. Matrix, 17(2), 401-425.}
\end{frame}

% Slide 25
\begin{frame}{Harmonic Ritz approximation of interior eigenpairs, cont'd\textsubscript{3}}
	\begin{itemize}
	\item[]
	\begin{itemize}\normalsize
	\item To simplify what follows, let us define
	\begin{align*}
		G_1:=P^T(A-\sigma I_n)^H(A-\sigma I_n)P
		\text{ and }
		G_2:=P^T(A-\sigma I_n)P
	\end{align*}
	so that, assuming $\sigma$ is not an eigenvalue of $A$, the reduced eigenpair $(\lambda,\hat{y})$ is such that $G_2^{-H}G_1\hat{y}=\lambda \hat{y}$.
	\item It is well established that the Rayleigh quotient $\rho$ of $y$ with respect to $A$ is a better approximation of the eigenvalue of $A$ near $\sigma$ than $\sigma+\lambda$. The Rayleigh quotient can be efficiently computed as
	\begin{align*}
		\rho=\frac{y^HAy}{y^Hy}=\frac{\hat{y}^HP^TAP\hat{y}}{\hat{y}P^TP\hat{y}}
		=\sigma+\frac{\hat{y}^HP^T(A-\sigma I_n)P\hat{y}}{\hat{y}P^TP\hat{y}}
		=\sigma+\frac{\hat{y}^HG_2\hat{y}}{\hat{y}P^TP\hat{y}}
	\end{align*}
	so that, if $P^TP=I_k$ and $\hat{y}^H\hat{y}=1$, then we have $\boxed{\rho=\sigma+\hat{y}^HG_2\hat{y}}$.
	\item It is also common to monitor convergence through stopping criteria defined with respect to the residual
	\begin{align*}
		\hat{r}:=Ay-\rho y
	\end{align*}
	\end{itemize}
	\end{itemize}
\end{frame}

% Slide 26
\begin{frame}{Harmonic Ritz approximation of interior eigenpairs, cont'd\textsubscript{4}}
	\begin{itemize}
	\item[]
	\begin{itemize}\normalsize
	\item[] whose norm can also be efficiently computed as we have
	\begin{align*}
		\hat{r}^H\hat{r}
		=&\;(Ay-\rho y)^H(Ay-\rho y)\\
		=&\;((A-\sigma I_n)y+(\sigma-\rho)y)^H((A-\sigma I_n)y+(\sigma-\rho)y)\\
		=&\;y^H(A-\sigma I_n)^H(A-\sigma I_n)y+(\sigma-\rho)y^H(A-\sigma I_n)^Hy\\
		&\;+\overline{(\sigma-\rho)}y^H(A-\sigma I_n)y+\overline{(\sigma-\rho)}(\sigma-\rho)y^Hy\\
		=&\;\hat{y}^HG_1\hat{y}+(\sigma-\rho)\hat{y}^HG_2^H\hat{y}+\overline{(\sigma-\rho)}\hat{y}^HG_2\hat{y}+\overline{(\sigma-\rho)}(\sigma-\rho)y^Hy
	\end{align*}
	where, once again, we assume $P^TP=I_k$ and $\hat{y}^H\hat{y}=1$ so that
	\begin{align*}
	\hat{r}^H\hat{r}
	=&\;\hat{y}^HG_1\hat{y}+(\sigma-\rho)\hat{y}^HG_2^H\hat{y}+\overline{(\sigma-\rho)}\hat{y}^HG_2\hat{y}+\overline{(\sigma-\rho)}(\sigma-\rho)\\
	=&\;\lambda\hat{y}^HG_2^H\hat{y}+(\sigma-\rho)\hat{y}^HG_2^H\hat{y}+\overline{(\sigma-\rho)}(\rho-\sigma)+\overline{(\sigma-\rho)}(\sigma-\rho)\\
	=&\;(\sigma+\lambda-\rho)\hat{y}^HG_2^H\hat{y}\\
	=&\;(\sigma+\lambda-\rho)\overline{\hat{y}^HG_2\hat{y}}
	\end{align*}
	which leads to $\boxed{\hat{r}^H\hat{r}=(\sigma+\lambda-\rho)\overline{(\rho-\sigma)}}\,$.
	\end{itemize}
	\end{itemize}
\end{frame}

% Slide 27
\begin{frame}{Harmonic Ritz approximation of interior eigenpairs, cont'd\textsubscript{5}}
	\begin{itemize}
	\item[]
	\begin{itemize}\normalsize
	\item The norm of the harmonic residual $\tilde{r}:=Ay-(\sigma+\lambda)y$ can also be used to monitor convergence.
	Still assuming $P^TP=I_k$ and $\hat{y}^H\hat{y}=1$, we then have
	\begin{align*}
		\tilde{r}^H\tilde{r}
		=&\;(Ay-(\sigma+\lambda)y)^H(Ay-(\sigma+\lambda)y)\\
		=&\;((A-\sigma I_n)y-\lambda y)^H((A-\sigma I_n)y-\lambda y)\\
		=&\;\hat{y}^HP^T(A-\sigma I_n)^H(A-\sigma I_n)P\hat{y}
		   -\lambda\hat{y}^HP^T(A-\sigma I_n)^HP\hat{y}\\
		 &\;-\overline{\lambda}\hat{y}^HP^T(A-\sigma I_n)P\hat{y}+\lambda\overline{\lambda}\\
		 =&\;\hat{y}^HG_1\hat{y}-\lambda\hat{y}^HG_2^H\hat{y}-\overline{\lambda}\hat{y}^HG_2\hat{y}+\lambda\overline{\lambda}\\
		 =&\;\lambda\hat{y}^HG_2^H\hat{y}-\lambda\hat{y}^HG_2^H\hat{y}-\overline{\lambda}\hat{y}^HG_2\hat{y}+\lambda\overline{\lambda}\\
		 =&\;\overline{\lambda}(\lambda-\hat{y}^HG_2\hat{y})
	\end{align*}
	where $\hat{y}^HG_2\hat{y}=\rho-\sigma$ so that
	\begin{align*}
		\boxed{\tilde{r}^H\tilde{r}=(\sigma+\lambda-\rho)\overline{\lambda}}\,.
	\end{align*}
	\end{itemize}
	\end{itemize}
\end{frame}

% Slide 28
\begin{frame}{Harmonic Ritz approximation of interior eigenpairs, cont'd\textsubscript{6}}
\begin{itemize}
\item Alternatively, the harmonic Ritz pairs may be formed from non-shifted procedures.
That is, let 
\begin{align*}
G_1:=(AP)^HAP
\;\text{ and }\;
G_2:=P^ TAP,
\end{align*}
then, the reduced eigenvalue problem of the shifted approximation 
\begin{align*}
y\in\text{range}(P)
\;\text{ such that }\;
(A-\sigma I_n)y-\lambda y
\perp
(A-\sigma I_n)\text{range}(P)
\end{align*}
is obtained as follows:
\begin{align*}
((A-\sigma I_n)P)^HAP\hat{y}=&\,(\sigma+\lambda)((A-\sigma I_n)P)^HP\hat{y}\\
[(AP)^HAP-\overline{\sigma}P^TAP]\hat{y}=&\,(\sigma+\lambda)(P^TA^HP-\overline{\sigma}I_k)\hat{y}\\
(G_1-\overline{\sigma}G_2)\hat{y}=&\,(\sigma+\lambda)(G_2^H-\overline{\sigma}I_k)\hat{y}\\
(G_1-\overline{\sigma}G_2)\hat{y}=&\,(\sigma+\lambda)(G_2-\sigma I_k)^H\hat{y}\\
(G_2-\sigma I_k)^{-H}(G_1-\overline{\sigma}G_2)\hat{y}=&\,(\sigma+\lambda)\hat{y}
\end{align*}
in which case we have $\rho=\hat{y}^HG_2\hat{y}$.
\end{itemize}
\end{frame}

% Slide 28
\begin{frame}{Arnoldi procedure with harmonic Ritz vectors}
\begin{itemize}
\item More can be said for the case in which the search space is Krylov and generated by an Arnoldi procedure, see Morgan and Zheng (1998).
\item Consider the shifted procedure \texttt{Arnoldi}($A-\sigma I_n$, $q_1$, $k$) $\mapsto(Q_{k+1},\underline{H}_k)$ which returns an orthonormal basis $Q_{k}:=[q_1\,\dots\,q_k]$ of $\mathcal{K}_{k}(A-\sigma I_n,q_1)$ such that $\!(A-\sigma I_n)Q_k=Q_{k+1}\underline{H}_k$ where $Q_{k+1}:=[Q_k\,q_{k+1}]$ as well as $\underline{H}_k=Q_{k+1}^T(A-\sigma I_n)Q_k$ and $H_k=Q_k^T(A-\sigma I_n)Q_k$.
\item Then, harmonic Ritz vectors $y\in\text{range}(Q_k)$ are such that 
\begin{align*}
(A-\sigma I_n)y-\lambda y\perp (A-\sigma I_n)\text{range}(Q_k)
\end{align*}
yields the following reduced generalized eigenvalue problem in which we search for non-trivial pairs $(\lambda,\hat{y})\in\mathbb{C}\times\mathbb{C}^{k}$ such that $y=Q_k\hat{y}$ and
\begin{align*}
Q_k^T(A-\sigma I_n)^H(A-\sigma I_n)Q_k\hat{y}=&\;\lambda Q_k^T(A-\sigma I_n)^HQ_k\hat{y}\\
\underline{H}^H_kQ_{k+1}^TQ_{k+1}\underline{H}_k\hat{y}=&\;\lambda H_k^H\hat{y}\\
\underline{H}^H_k\underline{H}_k\hat{y}=&\;\lambda H_k^H\hat{y}
\end{align*}
\end{itemize}
\medskip
\tiny{Morgan, R. B., \& Zeng, M. (1998). Harmonic projection methods for large non‐symmetric eigenvalue problems. Numerical linear algebra with applications, 5(1), 33-55.}
\end{frame}

% Slide 29
\begin{frame}{Arnoldi procedure with harmonic Ritz vectors, cont'd\textsubscript{1}}
\begin{itemize}
\item Reformulating the Arnoldi relation into 
\begin{align*}
(A-\sigma I_n)Q_k=Q_{k}H_k+h_{k+1,k}q_{k+1}e_k^T
\end{align*}
allows to rewrite the reduced eigenvalue problem of the harmonic Ritz projection as follows:
\begin{align*}
(H_k^HH_k+|h_{k+1,k}|^2e_ke_k^T)\hat{y}=\lambda H_k^H\hat{y}
\end{align*}
\vspace{-.65cm}
\begin{align*}
\hspace{.23cm}\boxed{(H_k+|h_{k+1,k}|^2fe_k^T)\hat{y}=\lambda\hat{y}}
\end{align*}
%\vspace{-.45cm}\\
where $f:=H_k^{-H}e_k\in\mathbb{C}^{k}$ and $e_k:=I_k[:,k]$.
\item Then, the harmonic eigen-residual $\tilde{r}:=Ay-(\sigma+\lambda )y$ of a given harmonic Ritz approximate eigenpair $(\sigma+\lambda,y)$ with $y:=Q_k\hat{y}$ is such that
\vspace{-.1cm}
\begin{align*}
\tilde{r}=&\;AQ_m\hat{y}-(\sigma+\lambda)Q_m\hat{y}\\
=&\;(A-\sigma I_n)Q_k\hat{y}-\lambda Q_k\hat{y}=(Q_{k}H_k+h_{k+1,k}q_{k+1}e_k^T)\hat{y}-\lambda Q_k\hat{y}\\
=&\;Q_k(\lambda I_n-|h_{k+1,k}|^2fe_k^T)\hat{y}+h_{k+1,k}q_{k+1}e_k^T\hat{y}-\lambda Q_k\hat{y}\\
=&\;h_{k+1,k}(e_k^T\hat{y})q_{k+1}-|h_{k+1,k}|^2(e_k^T\hat{y})Q_kf
\end{align*}
\end{itemize}
\end{frame}

% Slide 30
\begin{frame}{Arnoldi procedure with harmonic Ritz vectors, cont'd\textsubscript{2}}
\begin{itemize}
\item[] which can be written
\begin{align*}
\tilde{r}
=&\;h_{k+1,k}(e_k^T\hat{y})Q_{k+1}\begin{bmatrix}0_{k\times 1}\\1\end{bmatrix}-|h_{k+1,k}|^2(e_k^T\hat{y})Q_{k+1}\begin{bmatrix}f\\0\end{bmatrix}\\
=&\;h_{k+1,k}(e_k^T\hat{y})Q_{k+1}\begin{bmatrix}0_{k\times 1}\\1\end{bmatrix}+h_{k+1,k}(e_k^T\hat{y})Q_{k+1}\begin{bmatrix}-\overline{h_{k+1,k}}f\\0\end{bmatrix}\\
=&\;h_{k+1,k}(e_k^T\hat{y})Q_{k+1}\begin{bmatrix}-\overline{h_{k+1,k}}f\\1\end{bmatrix}\\
=&\;\beta_{k}Q_{k+1}s
\end{align*}
so that\vspace{-.35cm}
\begin{align*}
\boxed{\tilde{r}=\beta_{k}Q_{k+1}s}
\text{  where  }
\boxed{\beta_{k}:=h_{k+1,k}e_k^T\hat{y}}
\text{  and  }
\boxed{s:=\begin{bmatrix}-\overline{h_{k+1,k}}f\\1\end{bmatrix}}\,.
\end{align*}
\item The norm of $\tilde{r}$ is then given by $\boxed{\|\tilde{r}\|_2=|\beta_{k}|(|h_{k+1,k}|^2f^Hf+1)^{1/2}}\,$.
\end{itemize}
\end{frame}

% Slide 31
\begin{frame}{Arnoldi procedure with harmonic Ritz vectors, cont'd\textsubscript{3}}
\begin{itemize}
\item When precise eigenvalues are wanted, it is preferred to use the Rayleigh quotient rather than $\sigma+\lambda$. Assuming $\hat{y}$ has unit norm, so does $y:=Q_k\hat{y}$, and the Rayleigh quotient of $y$ is given by
\begin{align*}
\rho=\sigma+\hat{y}^HH_k\hat{y}=\sigma+\lambda-|h_{k+1,k}|^2(\hat{y}^Hf)(e_k^T\hat{y}).
\end{align*}
\item Moreover, the norm of the eigen-residual $\hat{r}:=Ay-\rho y$ is still such that 
\begin{align*}
\|\hat{r}\|_2^2=(\sigma+\lambda-\rho)\overline{(\rho-\sigma)}
\end{align*}
and that of the harmonic residual $\tilde{r}:=Ay-(\sigma+\lambda)y$ is still such that
\begin{align*}
\|\tilde{r}\|_2^2=(\sigma+\lambda-\rho)\overline{\lambda}.
\end{align*}
\end{itemize}
\end{frame}

% Slide 32
\begin{frame}{Arnoldi procedure with harmonic Ritz vectors, cont'd\textsubscript{4}}
\begin{itemize}
\item As mentioned before, we are interested by the case in which harmonic Ritz approximations are considered in the context of the non-shifted procedure $\mathtt{Arnoldi}(A,q_1,k)\mapsto(Q_{k+1},\underline{H}_k)$ which returns an orthonormal basis $Q_k:=[q_1\dots q_k]$ of $\mathcal{K}_k(A,q_1)$ such that $AQ_k=Q_{k+1}\underline{H}_k$ where $Q_{k+1}:=[Q_k\,q_{k+1}]$, $\underline{H}_k=Q_{k+1}^TAQ_k$ and $H_k=Q_k^TAQ_k$.
\begin{itemize}\normalsize
\item Then, the harmonic Ritz vector $y\in\text{range}(Q_k)$ is still such that
\begin{align*}
(A-\sigma I_n)y-\lambda y
\perp
(A-\sigma I_n)\text{range}(Q_k)
\end{align*}
but now yields the following reduced generalized eigenvalue problem:
\begin{align*}
((A-\sigma I_n)Q_k)^H(A-\sigma I_n)Q_k\hat{y}=&\;\lambda((A-\sigma I_n)Q_k)^HQ_k\hat{y}\\
\hspace*{-.5cm}
((A-\sigma I_n)Q_k)^HAQ_k\hat{y}-\sigma((A-\sigma I_n)Q_k)^HQ_k\hat{y}=&\;\lambda((A-\sigma I_n)Q_k)^HQ_k\hat{y}
\end{align*}
so that
\begin{align*}
((A-\sigma I_n)Q_k)^HAQ_k\hat{y}=&\;(\sigma+\lambda)((A-\sigma I_n)Q_k)^HQ_k\hat{y}
\end{align*}
\end{itemize}
\end{itemize}
\end{frame}

% Slide 33
\begin{frame}{Arnoldi procedure with harmonic Ritz vectors, cont'd\textsubscript{5}}
\begin{itemize}
\item[]
\begin{itemize}\normalsize
\item[] which develops as follows:
\begin{align*}
((AQ_k)^TAQ_k-\overline{\sigma}Q_k^TAQ_k)\hat{y}=&\;(\sigma+\lambda)(Q_k^TA^TQ_k-\overline{\sigma}I_k)\hat{y}\\
(H_k^TH_k+|h_{k+1,k}|^2e_ke_k^T-\overline{\sigma}H_k)\hat{y}=&\;(\sigma+\lambda)(H^T_k-\overline{\sigma}I_k)\hat{y}\\
((H_k^T-\overline{\sigma}I_k)H_k+|h_{k+1,k}|^2e_ke_k^T)\hat{y}=&\;(\sigma+\lambda)(H_k-\sigma I_k)^H\hat{y}\\
\hspace*{-.25cm}
((H_k-\sigma I_k)^HH_k+|h_{k+1,k}|^2e_ke_k^T)\hat{y}=&\;(\sigma+\lambda)(H_k-\sigma I_k)^H\hat{y}
\end{align*}
to finally yield
\begin{align*}
\boxed{(H_k+|h_{k+1,k}|^2fe_k^T)\hat{y}=(\sigma+\lambda)\hat{y}}
\text{ where }
\boxed{f:=(H_k-\sigma I_k)^{-H}e_k},
\end{align*}
so that the expression for $f$ differs from the shifted procedure.
Still, the harmonic Ritz pairs should converge first to the eigenpairs of $A$ closest to $\sigma$ so that, now, we should not retain the least dominant reduced eigenpairs, but rather those with eigenvalues closest to $\sigma$.
\end{itemize}
\end{itemize}
\end{frame}

% Slide 34
\begin{frame}{Arnoldi procedure with harmonic Ritz vectors, cont'd\textsubscript{6}}
\begin{itemize}
\item[]
\begin{itemize}\normalsize
\item Now, still assuming $\hat{y}^H\hat{y}=1$, the Rayleigh quotient is given by
\begin{align*}
\rho=y^HAy=\hat{y}^HQ_k^TAQ_k\hat{y}=\hat{y}^HH_k\hat{y}=\sigma+\lambda-|h_{k+1,k}|^2(\hat{y}^Hf)(e_k^T\hat{y}).
\end{align*}
\item Irrespective of the basis generated, as long as it's orthonormal, we already saw the residual given by $\hat{r}:=Ay-\rho y$ is such that
\begin{align*}
\hat{r}^H\hat{r}=(\sigma+\lambda-\rho)\overline{(\rho-\sigma)}.
\end{align*}
\item And the harmonic eigen-residual $\tilde{r}:=Ay-(\sigma+\lambda)y$ is such that\vspace{-.15cm}
\begin{align*}
\tilde{r}
=&\;AQ_k\hat{y}-(\sigma+\lambda)Q_k\hat{y}\\
%=(Q_kH_k+h_{k+1,k}q_{k+1}e_k^T)\hat{y}-(\sigma+\lambda)Q_k\hat{y}\\
=&\;Q_kH_k\hat{y}+h_{k+1,k}q_{k+1}e_k^T\hat{y}-(\sigma+\lambda)Q_k\hat{y}\\
=&\;(\sigma+\lambda)Q_k\hat{y}-|h_{k+1,k}|^2Q_kfe_k^T\hat{y}+h_{k+1,m}q_{k+1}e_k^T\hat{y}-(\sigma+\lambda)Q_k\hat{y}\\
=&\;-|h_{k+1,k}|^2(e_k^T\hat{y})Q_kf+h_{k+1,k}q_{k+1}e_m^T\hat{y}
\end{align*}
which, similarly as before, can be recast into\vspace{-.2cm}
\begin{align*}
\tilde{r}=\beta_kQ_{k+1}s
\;\text{ where }\;
\beta_k:=h_{k+1,k}e_k^T\hat{y}
\;\text{ and }\;
s=\begin{bmatrix}-\overline{h_{k+1,k}}f\\1\end{bmatrix},
\end{align*}
where the difference with shifted Arnoldi is the expression for $f$.
\end{itemize}
\end{itemize}
\end{frame}	
		
\section{Lanczos process\\{\small Section 6.2 in Darve \& Wootters (2021)}}

% Slide 35
\begin{frame}{Lanczos process for symmetric matrices}
\begin{itemize}
\item The Lanczos process is a specialized form of the Arnoldi process for \textbf{symmetric matrices}.
\item When $A$ is symmetric (i.e., $A = A^T$), the Hessenberg matrix $H_k = Q_k^T A Q_k$ is symmetric too.
Consequently, it is \textbf{tridiagonal}:
\begin{align*}
T_k =
\begin{bmatrix}
\alpha_1 & \beta_1 & 0 & \cdots & 0 \\
\beta_1 & \alpha_2 & \beta_2 & \cdots & 0 \\
0 & \beta_2 & \alpha_3 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & \alpha_k
\end{bmatrix}
\end{align*}
where $\alpha_i = q_i^T A q_i$ are the diagonal elements and $\beta_i = q_i^T A q_{i+1} = q_{i+1}^T A q_i$ are the off-diagonal elements.
\item This tridiagonal structure means that in the Arnoldi recurrence relation, most terms vanish:
\begin{align*}
Aq_j = \beta_{j-1}q_{j-1} + \alpha_j q_j + \beta_j q_{j+1}
\end{align*}
\item This $\!$three-term $\!$recurrence $\!$relation $\!$is the foundation of the Lanczos process.$\hspace{-1cm}$
\end{itemize}
\end{frame}

% Slide 36
\begin{frame}{Derivation of the Lanczos process}
\begin{itemize}
\item From the three-term recurrence relation, we can derive the Lanczos algorithm:
\begin{align*}
\beta_j q_{j+1} = Aq_j - \alpha_j q_j - \beta_{j-1}q_{j-1}
\end{align*}
\item Rearranging to compute $q_{j+1}$:
\begin{align*}
q_{j+1} = \frac{1}{\beta_j}(Aq_j - \alpha_j q_j - \beta_{j-1}q_{j-1})
\end{align*}
\item The coefficients are determined as:
\begin{align*}
\alpha_j = q_j^T A q_j 
\hspace{.5cm}\text{and}\hspace{.5cm}
\beta_j = \|Aq_j - \alpha_j q_j - \beta_{j-1}q_{j-1}\|_2
\end{align*}
\item This $\!$leads $\!$to $\!$a $\!$much $\!$simpler algorithm compared to the full Arnoldi process:$\hspace{-1cm}$
\begin{itemize}\normalsize
\item[-]we only need to maintain three vectors in memory at any time: $q_{j-1}$, $q_j$, and $q_{j+1}$.
\item[-]the work done remains constant as the iteration count increases.
\end{itemize}
\end{itemize}
\end{frame}

% Slide 37
\begin{frame}{Lanczos algorithm}
\begin{itemize}
\item The Lanczos algorithm can be formulated as follows:
\begin{algorithm}[H]
\small
\caption{Lanczos}
\begin{algorithmic}[1]
\STATE{Choose a starting vector $q_1$ with $\|q_1\|_2 = 1$}
\STATE{Set $\beta_0 = 0$ and $q_0 = 0$}
\FOR{$j = 1, 2, \ldots, k$}
\STATE{$v = Aq_j$}
\STATE{$\alpha_j = q_j^T v$}
\STATE{$v = v - \alpha_j q_j - \beta_{j-1} q_{j-1}$}
\STATE{$\beta_j = \|v\|_2$}
\STATE{$q_{j+1} = v / \beta_j$}
\ENDFOR
\end{algorithmic}
\end{algorithm}
\item After $k$ steps, we have:
\begin{itemize}\normalsize
\item[-] An orthonormal basis $Q_k = [q_1, q_2, \ldots, q_k]$ for the Krylov subspace $\mathcal{K}_k(A, q_1)$
\item[-] A tridiagonal matrix $T_k = Q_k^T A Q_k$ with diagonal elements $\alpha_i$ and off-diagonal elements $\beta_i$
\end{itemize}
\end{itemize}
\end{frame}

% Slide 38
\begin{frame}{The Lanczos relation}
\begin{itemize}
\item Similar to the Arnoldi relation, we have the Lanczos relation:
\begin{align*}
AQ_k = Q_k T_k + \beta_k q_{k+1}e_k^T
\end{align*}
where $T_k$ is the tridiagonal matrix.
\item We can also write:
\begin{align*}
AQ_k = Q_{k+1}\underline{T}_k
\end{align*}
where $\underline{T}_k$ is the $(k+1) \times k$ tridiagonal matrix:
\begin{align*}
\underline{T}_k =
\begin{bmatrix}
\alpha_1 & \beta_1 & 0 & \cdots & 0 \\
\beta_1 & \alpha_2 & \beta_2 & \cdots & 0 \\
0 & \beta_2 & \alpha_3 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & \alpha_k \\
0 & 0 & 0 & \cdots & \beta_k
\end{bmatrix}.
\end{align*}
\end{itemize}
\end{frame}

% Slide 39
\begin{frame}{Lanczos Rayleigh-Ritz for dominant eigenpairs}
\begin{itemize}
\item \textbf{Eigenvectors with eigenvalues whose norms are the largest} among the spectrum of $A$ are \textbf{well approximated by Rayleigh-Ritz} projections.
\item A \textbf{Rayleigh-Ritz vector} $y\in\text{range}(Q_k)$ approximates an eigenvector of $A$ with the \textbf{Ritz value} $\lambda$ such that $Ay-\lambda y\perp \text{range}(Q_k)$. That is, we \textbf{search for} $(\lambda,\hat{y})\in\mathbb{R}\times\mathbb{R}^{k}\setminus\{0\}$ s.t. $z^T\left(Ay-\lambda y\right)=0\;\forall\; z\in\text{range}(Q_k)$ with $\!y=Q_k\hat{y}$.
This \textbf{simplifies to}
\begin{align*}
Q_k^T\left(AQ_k\hat{y}-\lambda Q_k\hat{y}\right)=&\;0\\
T_k\hat{y}-\lambda\hat{y}=&\;0
\;\;\implies\;\;
\boxed{T_k\hat{y}=\lambda\hat{y}}
\end{align*}
where use is made of the Lanczos relation and $Q_k^TQ_k=I_k$.
\item The eigen-residual $\tilde{r}:=Ay-\lambda y$ can be computed as:
\begin{align*}
\tilde{r} = AQ_k \hat{y} - \lambda Q_k \hat{y} = \beta_k (e_k^T \hat{y}) q_{k+1}
\end{align*}
\item This means $\|\tilde{r}\|_2 = \beta_k |e_k^T \hat{y}|$, providing a simple way to assess convergence without explicitly computing $Ay$.
\end{itemize}
\end{frame}

% Slide 40
\begin{frame}{Lanczos Rayleigh-Ritz in finite precision}
\begin{itemize}
\item \textbf{Loss of orthogonality:} In finite precision arithmetic, the Lanczos vectors quickly lose orthogonality, which can lead to:
\begin{itemize}\normalsize
\item[-] Multiple copies of the same eigenvalue appearing (ghost eigenvalues)
\item[-] Inaccurate eigenvalue approximations
\end{itemize}
\end{itemize}
\begin{center}
\includegraphics[height=5.5cm]{images/lanczos-ghost.png}
\end{center}
\end{frame}

% Slide 41
\begin{frame}{Reorthogonalization strategies}
\begin{itemize}
\item Different strategies exist to circumvent the issue of loss of orthogonality in finite precision.
\begin{itemize}\normalsize
\item[-]\textbf{Full reorthogonalization}: Explicitly orthogonalize each new vector against all previous vectors.
\end{itemize}
\end{itemize}
\begin{center}
\includegraphics[height=5.5cm]{images/lanczos-reortho.png}
\end{center}
\end{frame}

% Slide 42
\begin{frame}{Reorthogonalization strategies, cont'd}
\begin{itemize}
\item[]
\begin{algorithm}[H]
\small
\caption{Lanczos with full reorthogonalization}
\begin{algorithmic}[1]
\STATE{Choose a starting vector $q_1$ with $\|q_1\|_2 = 1$}
\STATE{Set $\beta_0 = 0$ and $q_0 = 0$}
\FOR{$j = 1, 2, \ldots, k$}
\STATE{$v = Aq_j$}
\STATE{$\alpha_j = q_j^T v$}
\STATE{$v = v - \alpha_j q_j - \beta_{j-1} q_{j-1}$}
\FOR{$i = 1, 2, \ldots, j$}
\STATE{$v = v - (q_i^T v) q_i$} \COMMENT{Reorthogonalization step}
\ENDFOR
\STATE{$\beta_j = \|v\|_2$}
\STATE{$q_{j+1} = v / \beta_j$}
\ENDFOR
\end{algorithmic}
\end{algorithm}
\item[] Full reorthogonalization turns Lanczos back into Arnoldi.
Alternatives:
\begin{itemize}\normalsize
\item[-] \textbf{Selective reorthogonalization:} Only reorthogonalize when necessary, based on loss of orthogonality measures
\item[-] \textbf{Partial reorthogonalization:} Reorthogonalize against a subset of previous vectors
\end{itemize}
\end{itemize}
\end{frame}

% Slide 43
\begin{frame}{Lanczos harmonic Ritz for interior eigenpairs}
\begin{itemize}
\item \textbf{Eigenvectors with interior eigenvalues} are \textbf{better approximated by harmonic Ritz} projections.
\item A \textbf{harmonic Ritz vector} $y\in\text{range}(Q_k)$ approximates an eigenvector of $A$ with the \textbf{harmonic Ritz value} $\sigma+\lambda$ in the vicinity of $\sigma$ such that\vspace{-.1cm}
\begin{align*}
(A-\sigma I_n)y-\lambda y\perp (A-\sigma I_n)\text{range}(Q_k).
\end{align*}
Let the columns of $Q_k$ form an orthonormal basis of $\mathcal{K}_k(A,q_1)$.
Then, the \textbf{search for} for the pair $(\sigma+\lambda,\hat{y})$ is such that
\begin{align*}
Q_k^T(A-\sigma I_n)^T(A-\sigma I_n)Q_k\hat{y}=&\;\lambda Q_k^T(A-\sigma I_n)^TQ_k\hat{y}\\
((AQ_k)^TAQ_k-\sigma Q_k^TAQ_k)\hat{y}=&\;(\sigma+\lambda)(Q_k^TAQ_k-\sigma I_k)\hat{y}\\
(T_kT_k+\beta_k^2e_ke_k^T-\sigma T_k)\hat{y}=&\;(\sigma+\lambda)(T_k-\sigma I_k)\hat{y}\\
((T_k-\sigma I_k)T_k+\beta_k^2e_ke_k^T)\hat{y}=&\;(\sigma+\lambda)(T_k-\sigma I_k)\hat{y}\\
(T_k+\beta_k^2fe_k^T)\hat{y}=&\;(\sigma+\lambda)\hat{y}
\end{align*}
where $f:=(T_k-\sigma I_k)^{-1}e_k$.
\end{itemize}
\end{frame}

% Slide 44
\begin{frame}{Lanczos harmonic Ritz for interior eigenpairs}
\begin{itemize}
\item Note that $fe_k^T$ is not symmetric so that the \textbf{reduced eigenpairs of} the \textbf{harmonic Ritz procedure} are \textbf{generally complex}.
\item[] However, they do \textbf{converge towards} the \textbf{real eigenpairs}.
\item Similarly as with Arnoldi, the Rayleigh quotient is given by
\begin{align*}
\rho=\sigma+\lambda-\beta_{k}^2(\hat{y}^Hf)(e_k^T\hat{y})
\end{align*}
whereas the residual $\hat{r}:=Ay-\rho y$ is such that 
\begin{align*}
\hat{r}^H\hat{r}=(\sigma+\lambda-\rho)\overline{(\rho-\sigma)}.
\end{align*}
\end{itemize}
\end{frame}

% Slide 45
\begin{frame}{Summary of Krylov subspace methods}
\begin{itemize}
\item We studied two main Krylov subspace methods for eigenvalue problems:
\begin{itemize}\normalsize
\item[-] \textbf{Arnoldi process:} For general matrices, produces a Hessenberg matrix $H_k$, requires orthogonalization against all previously formed vectors
\item[-] \textbf{Lanczos process:} For symmetric matrices, produces a tridiagonal matrix $T_k$, relies on a three terms recurence formula
\end{itemize}
\item Both methods:
\begin{itemize}\normalsize
\item[-] Construct an orthonormal basis for the Krylov subspace $\mathcal{K}_k(A,v)$
\item[-] Can be used with either Rayleigh Ritz or harmonic Ritz projections
\end{itemize}
\item Key advantages of Krylov subspace methods:
\begin{itemize}\normalsize
\item[-] Only require matrix-vector products, ideal for large sparse matrices
\item[-] Can find several eigenvalues simultaneously
\end{itemize}
\item Modern implementations use:
\begin{itemize}\normalsize
\item[-] Restarting techniques to limit memory requirements and increasing computational cost of Arnoldi (will be covered in Lecture 15)
\item[-] Reorthogonalization strategies for numerical stability of Lanczos
\end{itemize}
\end{itemize}
\end{frame}
		
\section{Homework problems}

% Slide 46
\begin{frame}{Homework problem}\vspace{.1cm}
Turn in \textbf{your own} solution to \textbf{Pb.$\,$23}:\vspace{.15cm}\\
\begin{minipage}[t]{0.1\textwidth}
\textbf{Pb.$\,$23}
\end{minipage}
\begin{minipage}[t]{0.89\textwidth}
For the matrices 
\begin{align*}
A=
\begin{bmatrix}
2&1&0\\
1&2&1\\
0&1&2
\end{bmatrix}
\;\text{ and }\;
V=
\begin{bmatrix}
1&0\\0&1\\0&0
\end{bmatrix},
\end{align*}
(a) Find the Rayleigh Ritz pairs of $A$ with respect to $\text{range}(V)$.\vspace{.1cm}\\
(b) Assemble the reduced eigenvalue problem to solve in order to find the harmonic Ritz values of $A$ with respect to $\text{range}(V)$ for $\sigma=0$.
\end{minipage}\vspace{.15cm}
\begin{minipage}[t]{0.1\textwidth}
\textbf{Pb.$\,$24}
\end{minipage}
\begin{minipage}[t]{0.89\textwidth}
For the matrix
$A=\begin{bmatrix}2&3&0\\1&2&3\\0&1&2\end{bmatrix}$
and
$q_1=\begin{bmatrix}1\\0\\0\end{bmatrix}$,
use the Arnoldi process to build an orthonormal basis $Q_2=[q_1,q_2]$ of the Krylov subspace $\mathcal{K}_2(A,q_1)$, and compute the projected matrix $H_2=Q_2^TAQ_2$.
\end{minipage}\vspace{.15cm}
\end{frame}

\section{Practice session}
% Slide 47
\begin{frame}[fragile]{Practice session}
\begin{enumerate}
\item Implement MGS and CGS2-based Arnoldi procedures.
\item For both methods, check how close $Q_{k+1}^TAQ_k$ is to your Hessenberg matrix, and with what accuracy does your Arnoldi relation $AQ_k=Q_{k+1}\underline{H}_k$ hold.
Quantify also the loss of orthgonality of the basis, and compare runtimes.
For all this, use the matrix \texttt{G3\_circuit} from the SuiteSparse collection.
\item Implement a Rayleigh-Ritz procedure on top of your CGS2-based Arnoldi process.
For the matrix \texttt{nxp1} from the SuiteSparse collection, solve for the 5 largest eigenpairs using the reduced expressions derived in class to monitor convergence of residuals.
Verify your residual estimates and compare your converged eigenpairs with those obtained using \texttt{ArnoldiMethod.jl}.
\item Implement a haronic Ritz procedure on top of your CGS2-based Arnoldi process.
Still using the \texttt{nxp1} matrix, solve for the 2 eigenpairs closest to $\sigma=150$ using the reduced expressions derived in class to monitor convergence of residuals.
Verify your residual estimates.
\end{enumerate}
\end{frame}

% Slide 48
\begin{frame}[fragile]{Practice session, cont'd}
\begin{enumerate}
\setcounter{enumi}{4} 
\item Implement the Lanczos algorithm.
Check how close $Q_{k+1}^TAQ_k$ is to your tridiagonal matrix, and with what accuracy does your Lanczos relation $AQ_k=Q_{k+1}\underline{T}_k$ hold.
Quantify also the loss of orthogonality of the basis.
For all this, use the matrix \texttt{Kuu} from the SuiteSparse collection.
\item Implement a Rayleigh-Ritz procedure on top of your Lanczos process.
For the matrix \texttt{Kuu} from the SuiteSparse collection, solve for the 5 largest eigenpairs using the reduced expressions derived in class to monitor convergence of residuals.
Verify your residual estimates.
\item Implement a harmonic Ritz procedure on top of your Lanczos process.
For the matrix \texttt{Kuu} from the SuiteSparse collection, solve for the 2 eigenpairs closest to $\sigma=25$ using the reduced expressions derived in class to monitor convergence of residuals.
Verify your residual estimates.
\end{enumerate}
\end{frame}

\end{document}

