\documentclass[t,usepdftitle=false]{beamer}

\input{../../../tex-beamer-custom/preamble.tex}

\title[NLA for CS and IE -- Lecture 04]{Numerical Linear Algebra\\for Computational Science and Information Engineering}
\subtitle{\vspace{.3cm}Lecture 04\\Direct Methods for Dense Linear Systems}
\hypersetup{pdftitle={NLA-for-CS-and-IE\_Lecture04}}

\date[Summer 2025]{Summer 2025}

\author[nicolas.venkovic@tum.de]{Nicolas Venkovic\\{\small nicolas.venkovic@tum.de}}
\institute[]{Group of Computational Mathematics\\School of Computation, Information and Technology\\Technical University of Munich}

\titlegraphic{\vspace{0cm}\includegraphics[height=1.1cm]{../../../logos/TUM-logo.png}}

\begin{document}
	
\begin{frame}[noframenumbering, plain]
	\maketitle
\end{frame}
	
\myoutlineframe
	
% Slide 01
\begin{frame}{Methods to solve linear systems}
\vspace{-.1cm}
\begin{block}{Problem}
Solve for $x$ such that $Ax=b$ where $A$ is an invertible matrix.
\end{block}
\vspace{-.075cm}
To solve this problem, we distinguish between two types of methods:\vspace{.05cm}
\begin{itemize} 
\item \textbf{Direct methods}: 
Deploy a predictable sequence of operations to yield the exact solution (assuming exact arithmetic).\vspace{.05cm}
\begin{itemize}
\item[-] \textbf{Gaussian elimination}: Efficiently solves isolated systems.\vspace{.05cm}
\item[-] \textbf{LU factorization}: Leverages $A=LU$, reusable for multiple right-hand sides.\vspace{.05cm}
\item[-] \textbf{Cholesky factorization}: Leverages $A=LL^H$ for Hermitian positive definite matrices, reusable for multiple right-hand sides.
\end{itemize}
\textit{In this lecture: Reminders, special cases, basic aspects of performance optimization, stability issues, and pivoting strategies.}\vspace{.05cm}
\item \textbf{Iterative methods}: Form successive approximations to the solution using $z\mapsto Az$ at each iteration.\vspace{.05cm}
\begin{itemize}
\item[-] \textbf{Stationary methods}: Use a consistent update formula, e.g., Jacobi, Gauss-Seidel, ...\vspace{.05cm}
\item[-] \textbf{Krylov subspace methods}: Build solution in expanding (Krylov) subspaces, e.g., CG, GMRES, ...
\end{itemize}
%\textit{Advantages: Memory-efficient for large, sparse systems; can provide good approximations quickly}
\end{itemize}
\end{frame}

\section{Gaussian elimination\\{\small Section 3.1 in Darve \& Wootters (2021)}}

% Slide 02
\begin{frame}{Solving isolated linear systems}
\normalsize
\begin{itemize}
\item Special matrices (more efficient than general case):
\begin{itemize}\normalsize
\item[-] \textbf{Diagonal} matrices: \textbf{Element-wise division} --- $n$ ops.\\
For $D=\mathrm{diag}(d_1,\dots,d_n)$, solve $Dx=b$ with $x_i=b_i/d_i$.\vspace{.05cm}
\item[-] \textbf{Tridiagonal} matrices: \textbf{Thomas algorithm} --- $O(n)$ ops.\\
A specialized form of the more general Gaussian elimination algorithm.\vspace{.05cm}
\item[-] \textbf{Lower triangular} matrices: \textbf{Forward substitution} --- $n^2$ ops.\\
Start from first equation, solve downwards.\vspace{.05cm}
\item[-] \textbf{Upper triangular} matrices: \textbf{Backward substitution} --- $n^2$ ops.\\
Start from last equation, solve upwards.
\end{itemize}
\item \textbf{Row echelon form}: 
\begin{itemize}\normalsize
\item[-] Matrix where the leading non-zero coefficient (\textbf{pivot}) of each row is strictly to the right of the pivot of the row above it.
\vspace{.05cm}
%\item[-] All rows consisting of only zeroes, if any, are at the bottom of the matrix.$\hspace{-1cm}$
%\item[-] Pivots are used to systematically eliminate variables in the solution process
\end{itemize}
\item General matrices: \textbf{Gaussian elimination} --- $O(n^3)$ ops.
\begin{itemize}\normalsize
\item[] Doolittle (\st{Crout}) variant:
\item[1.] \textbf{Forward elimination}: From $[A|b]$ to $[U|c]$ where $U$ is upper triangular.\\
Apply a sequence of row operations ({\color{red}\textbf{breakdown}} possible).\vspace{.05cm}
\item[2.] \textbf{Backward substitution}: If no breakdown happened, solve $Ux=c$.
\end{itemize}
\end{itemize}
\end{frame}

% Slide 03
\begin{frame}{Lower triangular matrices --- Forward substitution}
\begin{itemize}
\item Consider the system $Lx=b$ with lower triangular matrix\vspace{-.2cm}
\begin{align*}
L =\begin{bmatrix}
l_{11} & 0 & \cdots & 0 \\
l_{21} & l_{22} & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
l_{n1} & l_{n2} & \cdots & l_{nn}
\end{bmatrix}.
\end{align*}
$\vspace{-.4cm}$\\
\item The algorithm goes as follows:\vspace{.1cm}
%\hspace*{2cm}
%\begin{minipage}{0.9\textwidth}
\begin{itemize}
\item[1.] $x_1=b_1/l_{11}$
\item[2.] $x_2=(b_2-l_{21}x_1)/l_{22}$\vspace{-.1cm}
\item[]$\vdots$\vspace{-.1cm}
\item[$i$.] $x_i=\left(b_i - \sum_{j=1}^{i-1} l_{ij}x_j\right)/l_{ii}$\vspace{-.1cm}
\item[]$\vdots$\vspace{-.1cm}
\item[$n$.] $x_n=\left(b_n - \sum_{j=1}^{n-1} l_{nj}x_j\right)/l_{nn}$
\end{itemize}
%\end{minipage}
\item Operation count: $\displaystyle n\text{ divs.}+\frac{n(n-1)}{2}\text{ mults.}+\frac{n(n-1)}{2}\text{ adds.}=n^2\text{ ops.}\hspace{-1cm}$\vspace{.1cm}
\item {\color{red}\textbf{Breakdown}} happens iff $l_{ii} = 0$ for some $1\leq i\leq n$, i.e., \textbf{iff} $L$ is \textbf{singular}.
\end{itemize}
\end{frame}

% Slide 04
\begin{frame}{Upper triangular matrices --- Backward substitution}
\begin{itemize}
\item Consider the system $Ux=b$ with upper triangular matrix\vspace{-.2cm}
\begin{align*}
U =\begin{bmatrix}
u_{11} & u_{12} & \cdots & u_{1n} \\
0 & u_{22} & \cdots & u_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & u_{nn}
\end{bmatrix}.
\end{align*}
$\vspace{-.4cm}$\\
\item The algorithm goes as follows:\vspace{.1cm}
\begin{itemize}
\item[$n$.] $x_n=b_n/u_{nn}$
\item[$n\!-\!1$.] $x_{n-1}=(b_{n-1}-u_{n-1,n}x_n)/u_{n-1,n-1}$\vspace{-.1cm}
\item[]$\vdots$\vspace{-.1cm}
\item[$i$.] $x_i=\left(b_i - \sum_{j=i+1}^n u_{ij}x_j\right)/u_{ii}$\vspace{-.1cm}
\item[]$\vdots$\vspace{-.1cm}
\item[1.] $x_1=\left(b_1 - \sum_{j=2}^n u_{1j}x_j\right)/u_{11}$
\end{itemize}
\item Operation count: $\displaystyle n\text{ divs.}+\frac{n(n-1)}{2}\text{ mults.}+\frac{n(n-1)}{2}\text{ adds.}=n^2\text{ ops.}\hspace{-1cm}$\vspace{.1cm}
\item {\color{red}\textbf{Breakdown}} happens iff $u_{ii} = 0$ for some $1\leq i\leq n$, i.e., \textbf{iff} $U$ is \textbf{singular}.
\end{itemize}
\end{frame}

% Slide 05
\begin{frame}{General matrices --- Forward elimination}
\begin{itemize}
\item Forward elimination is deployed to try and transform
$[A|b]$ to $[U|c]$ where $U$ is an upper triangular matrix.
First, we do so without pivoting.\vspace{.1cm}
\item[-] First, we want to operate a transformation of the form
\small
\begin{align*}
\hspace{-.3cm}
\left[\begin{array}{ccccc|c}
a_{11} & a_{12} & a_{13} & \cdots & a_{1n} & b_1\vphantom{b^{(1)}}\\
{\color{red}a_{21}} & {\color{blue}a_{22}} & {\color{blue}a_{23}} & {\color{blue}\cdots} & {\color{blue}a_{2n}} & {\color{blue}b_2}\vphantom{b^{(1)}}\\
{\color{red}a_{31}} & {\color{blue}a_{32}} & {\color{blue}a_{33}} & {\color{blue}\cdots}& {\color{blue}a_{3n}} & {\color{blue}b_3}\vphantom{b^{(1)}}\\
{\color{red}\vdots} & {\color{blue}\vdots} & {\color{blue}\vdots} & {\color{blue}\ddots} & {\color{blue}\vdots} & {\color{blue}\vdots} \\
{\color{red}a_{n1}} & {\color{blue}a_{n2}} & {\color{blue}a_{n3}} & {\color{blue}\cdots}& {\color{blue}a_{nn}} & {\color{blue}b_n}\vphantom{b^{(1)}}
\end{array}\right]
\xrightarrow{(k=1)}
\left[\begin{array}{ccccc|c}
a_{11} & a_{12} & a_{13} & \cdots & a_{1n} & b_1 \\
{\color{red}0} & {\color{blue}a_{22}^{(1)}} & {\color{blue}a_{23}^{(1)}} &  {\color{blue}\cdots} & {\color{blue}a_{2n}^{(1)}} & {\color{blue}b_{2}^{(1)}} \\
{\color{red}0} & {\color{blue}a_{32}^{(1)}} & {\color{blue}a_{33}^{(1)}} &  {\color{blue}\cdots} & {\color{blue}a_{3n}^{(1)}} & {\color{blue}b_{3}^{(1)}}\\
{\color{red}\vdots} & {\color{blue}\vdots} & {\color{blue}\vdots} & {\color{blue}\ddots} & {\color{blue}\vdots} & {\color{blue}\vdots} \\
{\color{red}0} & {\color{blue}a_{n2}^{(1)}} & {\color{blue}a_{n3}^{(1)}} & {\color{blue}\cdots} & {\color{blue}a_{nn}^{(1)}} & {\color{blue}b_{n}^{(1)}}
\end{array}\right]
\end{align*}\normalsize
where ${\color{red}a_{21}},{\color{red}\dots},{\color{red}a_{n1}}$ are eliminated by setting ${\color{blue}b_{i}^{(1)}}:=b_i-m^{(1)}_{i}b_i$ and
\begin{align*}
\hspace{-.3cm}
{\color{blue}a_{ij}^{(1)}}
:=a_{ij}-m^{(1)}_{i}a_{1j}
\;\text{ where }\;
m^{(1)}_{i} := a_{i1}/a_{11}
\;\text{ for }\;
i,j\in\{2,\dots,n\}.
\end{align*}
This is equivalently done by $[A|b]\mapsto[G_1A|G_1b]$ where
\begin{center}
$\hspace{-.5cm}G_1=I_n\!-\!v^{(1)}e_1^T$
is a \textbf{Gauss transformation} matrix with structure
\raisebox{-.38cm}{\includegraphics[height=.9cm]{images/G1.jpg}}\vspace{-.15cm}
\end{center}
in which $\displaystyle v^{(1)}=[0\;\;\;m^{(1)}_{2}\;\;\dots\;\;m^{(1)}_{n}]^T$.\vspace{.15cm}
\end{itemize}
\tiny{Darve, E., \& Wootters, M. (2021). Numerical linear algebra with Julia. Society for Industrial and Applied Mathematics.}
\end{frame}

% Slide 06
\begin{frame}{General matrices --- Forward elimination, cont'd\textsubscript{1}}
\begin{itemize}
\item Forward elimination is deployed to try and transform
$[A|b]$ to $[U|c]$ where $U$ is an upper triangular matrix.
First, we do so without pivoting.\vspace{.1cm}
\item[-] Then, we want to operate a transformation of the form
\small
\begin{align*}
\hspace{-.45cm}
\left[\begin{array}{ccccc|c}
a_{11} & a_{12} & a_{13} & \cdots & a_{1n} & b_1\\
0 & a_{22}^{(1)} & a_{23}^{(1)} &  \cdots & a_{2n}^{(1)} & b_2^{(1)} \\
0 & {\color{red}a_{32}^{(1)}} & {\color{blue}a_{33}^{(1)}} &  \cdots & {\color{blue}a_{3n}^{(1)}} & {\color{blue}b_{3}^{(1)}}\\
\vdots & {\color{red}\vdots} & {\color{blue}\vdots} & {\color{blue}\ddots} & {\color{blue}\vdots} & {\color{blue}\vdots} \\
0 & {\color{red}a_{n2}^{(1)}} & {\color{blue}a_{n3}^{(1)}} & {\color{blue}\cdots} & {\color{blue}a_{nn}^{(1)}} & {\color{blue}b_{n}^{(1)}}
\end{array}\right]
\xrightarrow{(k=2)}
\left[\begin{array}{ccccc|c}
a_{11} & a_{12} & a_{13} & \cdots & a_{1n} & b_1 \\
0 & a_{22}^{(1)} & a_{23}^{(1)} &  \cdots & a_{2n}^{(1)} & b_2^{(1)} \\
0 & {\color{red}0} & {\color{blue}a_{33}^{(2)}} & {\color{blue}\cdots} & {\color{blue}a_{3n}^{(2)}} & {\color{blue}b_{3}^{(2)}} \\
\vdots & {\color{red}\vdots} & {\color{blue}\vdots} & {\color{blue}\ddots} & {\color{blue}\vdots} & {\color{blue}\vdots} \\
0 & {\color{red}0} & {\color{blue}a_{n3}^{(2)}} & {\color{blue}\cdots} & {\color{blue}a_{nn}^{(2)}} & {\color{blue}b_{n}^{(2)}}
\end{array}\right]
\end{align*}\normalsize
where $
{\color{red}a_{32}^{(1)}},{\color{red}\dots},{\color{red}a_{n2}^{(1)}}$ are eliminated by setting ${\color{blue}b_{i}^{(2)}}:=b_i^{(1)}-m^{(2)}_{i}b^{(1)}_i$ and
\begin{align*}
\hspace{-.3cm}
{\color{blue}a_{ij}^{(2)}}:=a^{(1)}_{ij}-m^{(2)}_{i}a^{(1)}_{2j}
\;\text{ where }\;
m^{(2)}_{i} := a^{(1)}_{i2}/a^{(1)}_{22}
\;\text{ for }\;
i,j\in\{3,\dots,n\}.
\end{align*}
This is equivalently done by $[G_1A|G_1b]\mapsto[G_2G_1A|G_2G_1b]$ where
\begin{center}
$\hspace{-.5cm}G_2=I_n\!-\!v^{(2)}e_2^T$
is a \textbf{Gauss transformation} matrix with structure
\raisebox{-.38cm}{\includegraphics[height=.9cm]{images/G2.jpg}}\vspace{-.15cm}
\end{center}
in which $\displaystyle v^{(2)}=[0\;\;0\;\;m^{(2)}_{3}\;\;\dots\;\;m^{(2)}_{n}]^T$.\vspace{.15cm}
\end{itemize}
\tiny{Darve, E., \& Wootters, M. (2021). Numerical linear algebra with Julia. Society for Industrial and Applied Mathematics.}
\end{frame}

% Slide 07
\begin{frame}{General matrices --- Forward elimination, cont'd\textsubscript{2}}
\begin{itemize}
\item Forward elimination is deployed to try and transform
$[A|b]$ to $[U|c]$ where $U$ is an upper triangular matrix.
First, we do so without pivoting.\vspace{.05cm}
\item[-] Eventually, the row-echelon form $[U|c]$ is obtained after the application of $n-1$ Gaussian transformations:\vspace{-.1cm}
\begin{align*}
[G_{n-1}\dots G_1A|G_{n-1}\dots G_1b]=[U|c]
\end{align*}
where $G_k=I_n-v^{(k)}e_k^T$ in which\vspace{.1cm}\\
\begin{center}
$\displaystyle v^{(k)}_i=\begin{cases}0&\text{for }1\leq i\leq k\\a_{ik}^{(k-1)}/a_{kk}^{(k-1)}&\text{for }k< i\leq n\end{cases}\;$
for $\;1<k\leq n-1$.
\end{center}
$\vspace{-.05cm}$\\
\item[-]Structurally speaking, $U$ is formed as follows:\vspace{.05cm}
\begin{figure}
\hspace{-.2cm}\includegraphics[height=1.9cm]{images/GA=U.jpg}
\end{figure}\vspace{-.15cm}
\end{itemize}
\tiny{Darve, E., \& Wootters, M. (2021). Numerical linear algebra with Julia. Society for Industrial and Applied Mathematics.}\vspace{.1cm}\normalsize
\begin{itemize}
\item[-] {\color{red}{\textbf{Breakdown}}} happens if either of the $a_{11},a_{22}^{(1)},\dots,a_{n-1,n-1}^{(n-2)}$ pivots is zero.
\end{itemize}
\end{frame}

% Slide 08
\begin{frame}{General matrices --- Forward elimination, cont'd\textsubscript{3}}
\begin{itemize}
\item Operation count of $G_{n-1}\dots G_1A$:
\begin{align*}
T_A(n):=&\,\sum_{k=1}^{n-1}\left(n-k\text{ divs.}+(n-k)^2\text{ mults.}+(n-k)^2\text{ adds.}\right)\\
=&\,\frac{(n-1)n}{2}\text{ divs.}
+\frac{n(2n^2-3n+1)}{6}\text{ mults.}\\
&\hspace{2.44cm}+\frac{n(2n^2-3n+1)}{6}\text{ adds.}\\
=&\,\frac{n(4n^2-3n-1)}{6}\text{ ops.}=O(n^3)\text{ ops.}
\end{align*}
\item Operation count of $G_{n-1}\dots G_1b$:
\begin{align*}
T_b(n):=&\,\sum_{k=1}^{n-1}\left(1\text{ div.}+1\text{ mult.}+1\text{ add.}\right)\\
=&\,(n-1)\text{ mults.}+(n-1)\text{ adds.}\\
=&\,2(n-1)\text{ ops.}=O(n)\text{ ops.}
\end{align*}
\end{itemize}
\end{frame}

% Slide 09
\begin{frame}{Tridiagonal matrices --- Forward elimination}
\begin{itemize}
\item Consider the system $Tx=b$ with tridiagonal matrix $T$. Then, assuming no breakdown happens, the forward elimination yields a bidiagonal matrix.
\item[-] The first set of row operations, i.e., $k=1$, yields
\small
\begin{align*}
\hspace{-.55cm}
\left[\begin{array}{ccccc|c}
             t_{11} & t_{12}               &        &           &        & b_1              \\
{\color{red}t_{21}} & {\color{blue}t_{22}} & t_{23} &           &        & {\color{blue}b_2}\\
                    & t_{32}               & t_{33} & t_{34}    &        & b_3              \\ 
                    &                      & \ddots & \ddots    & \ddots & \vdots           \\
                    &                      &        & t_{n,n-1} & t_{nn} & b_n
\end{array}\right]
\xrightarrow{(k=1)}
\left[\begin{array}{ccccc|c}
t_{11}         & t_{12}                     &        &        &           & b_1                     \\
{\color{red}0} & {\color{blue}t_{22}^{(1)}} & t_{23} &        &           & {\color{blue}b_2^{(1)}} \\
               & t_{32}                     & t_{33} & t_{34} &           & b_3                     \\
               &                            & \ddots & \ddots & \ddots    & \vdots                  \\
               &                            &        & t_{n,n-1} & t_{nn} & b_n 
\end{array}\right]
\end{align*}\normalsize
The application of forward elimination starts by
${\color{blue}b_{2}^{(1)}}:=b_2-m^{(1)}_{2}b_2$
and
\begin{align*}
t_{2j}^{(1)}\!:=t_{2j}\!-m_2^{(1)} t_{1j}
\text{ where } 
m_2^{(1)}\!:=t_{21}\!/t_{11}
\text{ for }
j\in\{2,\dots,n\}.
\end{align*}
Since $t_{13}=\dots=t_{1n}=0$, we have
\begin{align*}
{\color{blue}t_{22}^{(1)}}=t_{22}-m_2^{(1)}t_{12},
\;\text{ but }\;
t_{2j}^{(1)}=t_{2j}
\text{ for }j\in\{3,\dots,n\}.
\end{align*}
\end{itemize}
\end{frame}

% Slide 10
\begin{frame}{Tridiagonal matrices --- Forward elimination, cont'd}
\begin{itemize}
\item Consider the system $Tx=b$ with tridiagonal matrix $T$. Then, assuming no breakdown happens, the forward elimination yields a bidiagonal matrix.
\item[-] Then, the row operations for $k=2$ yield
\small
\begin{align*}
\hspace{-.55cm}
\left[\begin{array}{ccccc|c}
t_{11}         & t_{12}                     &                      &           &           & b_1\!\!\!\!\\
               & t_{22}^{(1)}               & t_{23}               &           &           & b_2^{(1)}\!\!\!\!\\
               & {\color{red}t_{32}}        & {\color{blue}t_{33}} & t_{34}    &           & {\color{blue}b_3}\!\!\!\!\\
               &                            & \ddots               & \ddots    & \ddots    & \vdots\!\!\!\!\\
               &                            &                      & t_{n,n-1} & t_{nn}    & b_n\!\!\!\! 
\end{array}\right]
\xrightarrow{(k=2)}
\left[\begin{array}{ccccc|c}
t_{11}         & t_{12}                     &        &        &           & b_1       \!\! \!\!               \\
               & t_{22}^{(1)} & t_{23} &        &           & b_2^{(1)} \!\!\!\!  \\
               & {\color{red}0}             & {\color{blue}t_{33}^{(1)}}  & t_{34} &           & {\color{blue}b_3^{(2)}} \!\!\!\!                      \\
               &                            & \ddots & \ddots & \ddots    & \vdots      \!\!\!\!              \\
               &                            &        & t_{n,n-1} & t_{nn} & b_n \!\!\!\!  
\end{array}\right]
\end{align*}\normalsize
Similarly, since $t_{24}=\dots=t_{2n}=0$, we have
\begin{align*}
{\color{blue}t_{33}^{(1)}}=t_{33}-m_3^{(2)}t_{23},
\;\text{ but }\;
t_{3j}^{(1)}=t_{3j}
\text{ for }j\in\{4,\dots,n\}.
\end{align*}
\item[-] And so on for $k=3,\dots,n-1$.
\item Operation count: $T_T(n)=3(n-1)\text{ ops.}$ and $T_b(n)=(n-1)\text{ ops.}$
\item \textbf{{\color{red}Breakdown}} happens iff $t_{ii}=0$ for some $1\leq i< n$.
\end{itemize}
\end{frame}

% Slide 11
\begin{frame}{Bidiagonal matrices --- Simplified backward substitution}
\begin{itemize}
\item Consider the system $Bx=b$ with (upper) bidiagonal matrix\vspace{-.2cm}
\begin{align*}
B =\begin{bmatrix}
b_{11}     & b_{12} &        &       \\
           & b_{22} & b_{23} &       \\
           &        & \ddots & \ddots\\
           &        &        & b_{nn}
\end{bmatrix}.
\end{align*}
$\vspace{-.4cm}$\\
\item The algorithm goes as follows:\vspace{.1cm}
\begin{itemize}
\item[$n$.] $x_n=b_n/b_{nn}$
\item[$n\!-\!1$.] $x_{n-1}=(b_{n-1}-b_{n-1,n}x_n)/b_{n-1,n-1}$\vspace{-.1cm}
\item[]$\vdots$\vspace{-.1cm}
\item[$i$.] $x_i=\left(b_i - b_{i,i+1}x_{i+1}\right)/b_{ii}$\vspace{-.1cm}
\item[]$\vdots$\vspace{-.1cm}
\item[1.] $x_1=\left(b_1 - b_{12}x_2\right)/b_{11}$
\end{itemize}
\item Operation count: $\displaystyle n\text{ divs.}+(n-1)\text{ mults.}+(n-1)\text{ adds.}=3n-2\text{ ops.}\hspace{-1cm}$\vspace{.1cm}
\item {\color{red}\textbf{Breakdown}} happens iff $b_{ii} = 0$ for some $1\leq i\leq n$, i.e., \textbf{iff} $B$ is \textbf{singular}.
\end{itemize}
\end{frame}
	
\section{LU factorization without pivoting\\{\small Section 3.1 in Darve \& Wootters (2021)}}

% Slide 12
\begin{frame}{LU factorization}
\begin{itemize}
\item So far, we considered forward elimination without pivoting.
\textbf{If no breakdown happens}, this process yields an \textbf{upper-triangular} matrix
\begin{align*}
U=G_{n-1}\cdots G_1A
\end{align*}
where the Gauss transformation matrix $G_k=I_n-v^{(k)}e_k^T$ is lower-triangular, with ones on the diagonal, thus non-singular.\\
\item You can verify that $G_{k}^{-1}=I_n+v^{(k)}e_k^T$.
\item Given the structure of $\displaystyle v^{(k)}=[0\;\;\cdots\;\;0\;\;m^{(k)}_{k+1}\;\;\dots\;\;m^{(k)}_{n}]^T$, we also have that $k<\ell$ implies $G_{k}^{-1}G_{\ell}^{-1}=I_n+v^{(k)}e_k^T+v^{(\ell)}e_\ell^T.\hspace{-1cm}$
\item Consequently, we have 
\begin{align*}
G_1^{-1}\cdots G_{n-1}^{-1}U=&\;A\\
\left(I_n+v^{(1)}e_1^T+\dots+v^{(n-1)}e_{n-1}^T\right)U=&\;A\\
LU=&\;A
\end{align*}
where $L:=G_1^{-1}\cdots G_{n-1}^{-1}$ is \textbf{lower-triangular}.
\end{itemize}
\end{frame}

% Slide 13
\begin{frame}{LU factorization, cont'd}
\begin{itemize}
\item The components below the diagonal of the $k$-th column of $L$ are given by the non-zero components of $v^{(k)}$, i.e., 
\begin{align*}
L=
\begin{bmatrix}
1         &        &               & \\
m_2^{(1)} & \ddots &               & \\
\vdots    &        & 1             & \\
m_n^{(1)} & \cdots & m_{n}^{(n-1)} & 1
\end{bmatrix}
\end{align*}
so that $L$ is a \textbf{by-product} of the \textbf{forward elimination} procedure, i.e., we have\vspace{-.3cm}
\begin{align*}
\hspace{-.4cm}
m_{i}^{(k)}=\frac{a^{(k-1)}_{ik}}{a^{(k-1)}_{kk}}
\end{align*}
$\vspace{-.2cm}$\\
where $a^{(k-1)}_{ij}$ are components of $A^{(k-1)}:=G_{k-1}\cdots G_1 A$, and $a_{ij}^{(0)}:=a_{ij}$.
\item If $A$ is non-singular, and the upper-triangular matrix $U$ is obtained by forward elimination without breakdown, then it can be shown that there is a unique lower-triangular matrix $L$ such that $LU=A$.
\end{itemize}
\end{frame}

% Slide 14
\begin{frame}{Solving linear systems with an LU factorization}
\begin{itemize}
\item Given an $LU$ factorization of an invertible matrix $A$:\vspace{.1cm}
\begin{center}
\includegraphics[height=1.9cm]{images/LU.jpg}
\end{center}
$\vspace{-.65cm}$\\
the linear system $Ax=b$ can be recast into $Lz=b$, where $Ux=z$, so that one can solve for $x$ in two steps:\vspace{.1cm}
\begin{center}
\includegraphics[height=2.9cm]{images/solve-with-LU.jpg}
\end{center}
$\vspace{-.7cm}$\\
\item Then, solving $Ax=b$ requires two triangular solves, i.e., a forward substitution, followed by a backward propagation, totaling $2n^2$ operations.
\end{itemize}
$\vspace{-.3cm}$\\
\tiny{Darve, E., \& Wootters, M. (2021). Numerical linear algebra with Julia. Society for Industrial and Applied Mathematics.}
\end{frame}

\section{Breakdown and instability of LU factorization\\{\small Section 3.1 and 3.3 in Darve \& Wootters (2021)}}

% Slide 15
\begin{frame}{Breakdown of LU factorization without pivoting}
\begin{itemize}
\item So far, we assumed \textbf{no breakdown} happens during forward elimination.
\item However, \textbf{breakdowns} do happen, \textbf{even} when using \textbf{exact arithmetic} and $A$ is \textbf{invertible}:\vspace{.2cm}\\
E.g., applying forward elimination to $A:=\!\begin{bmatrix}1&6&1&0\\0&1&9&0\\1&6&1&1\\0&0&1&0\end{bmatrix}\!$, which \textbf{is}\\
\vspace{-.6cm}\hspace{.9cm}\textbf{invertible}, will \textbf{break down}.\vspace{.4cm}\\
\item If $a_{kk}^{(k-1)}=0$, then breakdown will happen when applying  $\,G_k\,$ to $\,A^{(k-1)}\,$ :\vspace{.13cm}
\item[] $\hspace{1.3cm}$We say that $A^{(k-1)}=$
\raisebox{-.8cm}{\includegraphics[height=1.9cm]{images/zero-pivot.jpg}}
has a \textbf{zero-pivot}.
\end{itemize}
\vspace{.1cm}\tiny{Darve, E., \& Wootters, M. (2021). Numerical linear algebra with Julia. Society for Industrial and Applied Mathematics.}\normalsize
\begin{itemize}
\vspace{.1cm}
\item[]In particular, breakdown happens as we attempt to \textbf{divide by zero} to form
\begin{align*}
\hspace{-.4cm}
m_{i}^{(k)}:=a^{(k-1)}_{ik}/a^{(k-1)}_{kk}
\;\text{ for }\;i=k+1,\dots,n.
\end{align*}
\end{itemize}
\end{frame}

% Slide 16
\begin{frame}{Understanding the source of breakdown}
\begin{itemize}
\item We can think of the block $A^{(k-1)}[1\!:\!k,1\!:\!k]$ as
\begin{align*}
(G_{k-1}\dots G_{1})[1\!:\!k,1\!:\!n]A[1\!:\!n,1\!:\!k]=A^{(k-1)}[1\!:\!k,1\!:\!k].
\end{align*}
But since $(G_{k-1}\dots G_{1})[1\!:\!k,k+1\!:\!n]=0$, we have
\begin{align*}
(G_{k-1}\dots G_{1})[1\!:\!k,1\!:\!k]A[1\!:\!k,1\!:\!k]=A^{(k-1)}[1\!:\!k,1\!:\!k].
\end{align*}
Thus, we can focus our investigation on the leading principal blocks:\vspace{-.3cm}
%Thus, the singularity of $A^{(k-1)}[1\!:\!k,1\!:\!k]$ should relate to $A[1\!:\!k,1\!:\!k]$:\vspace{-.3cm}
\end{itemize}
\hspace{.2cm}\begin{minipage}{0.485\textwidth}
\raisebox{-3.75cm}{\includegraphics[height=2cm]{images/funny-block.jpg}}
\end{minipage}
\begin{minipage}{0.485\textwidth}
\includegraphics[height=3cm]{images/forward-elimination-zero-pivot.jpg}
\end{minipage}
$\vspace{.1cm}$\\
\tiny{Darve, E., \& Wootters, M. (2021). Numerical linear algebra with Julia. Society for Industrial and Applied Mathematics.}
\end{frame}

% Slide 17
\begin{frame}{Understanding the source of breakdown, cont'd}
$\vspace*{-.2cm}$\\
-
\begin{minipage}{0.3\textwidth}
\includegraphics[height=1.7cm]{images/zero-pivot-block.jpg}
\end{minipage}$\hspace{-1.5cm}$
\begin{minipage}{0.75\textwidth}
The leading block $A^{(k-1)}[1\!:\!k,1\!:\!k]$ is \textbf{singular} because it is \textbf{triangular} with a \textbf{zero on the diagonal}, i.e., $a_{kk}^{(k-1)}=0$.
\end{minipage}
$\vspace{.3cm}$\\
-
\begin{minipage}{0.3\textwidth}
\includegraphics[height=1.7cm]{images/gauss-transform-block.jpg}
\end{minipage}$\hspace{-1.5cm}$
\begin{minipage}{0.75\textwidth}
The leading block $(G_{k-1}\dots G_{1})[1\!:\!k,1\!:\!k]$ of the product of Gauss transformation matrices
is \textbf{non-singular} because it is \textbf{triangular} with a \textbf{ones on the diagonal}.
\end{minipage}
$\vspace{.3cm}$\\
-
\begin{minipage}{0.3\textwidth}
\includegraphics[height=1.7cm]{images/singular-block.jpg}
\end{minipage}$\hspace{-1.5cm}$
\begin{minipage}{0.75\textwidth}
Therefore, the leading block $A[1\!:\!k,1\!:\!k]$ must be \textbf{singular}.
\end{minipage}
$\vspace*{.1cm}$\\
\tiny{Darve, E., \& Wootters, M. (2021). Numerical linear algebra with Julia. Society for Industrial and Applied Mathematics.}\normalsize\vspace{-.07cm}


%\item Prior to a breakdown in the application of $G_k$ to $A^{(k-1)}$, the coefficients $m_i^{(l)}$ are well-defined for $l=1,\dots,k-1$ and $i=l+1,\dots,n$.
%Thus, there is a decomposition of $A[1\!:\!k,1\!:\!k]$ with the form\vspace{0cm}
%\end{itemize}
%\begin{center}
%\includegraphics[height=2.5cm]{images/LU-zero-det-block.jpg}
%\end{center}
%$\vspace{-.7cm}$\\
%\tiny{Darve, E., \& Wootters, M. (2021). Numerical linear algebra with Julia. Society for Industrial and Applied Mathematics.}\normalsize
%\begin{itemize}
%\begin{itemize}
%\item[]where $L[1\!:\!k,1\!:\!k]=(G_1^{-1}\cdots G^{-1}_{k})[1\!:\!k,1\!:\!k]$, but where the factor $U[1\!:\!k,1\!:\!k]=A^{(k-1)}[1\!:\!k,1\!:\!k]$ is singular, and thus, so is $A[1\!:\!k,1\!:\!k]$.\vspace{.1cm}
%\item $\!$You can also show that if $\!A[1\!:\!k,1\!:\!k]$ is singular then so is $\!A^{(k-1)}[1\!:\!k,1\!:\!k].\hspace{-1cm}$\vspace{-.07cm}
\begin{theorem}[Existence of an LU factorization without pivoting]
A matrix $A\in\mathbb{F}^{n\times n}$ admits an LU factorization without pivoting iff its $n-1$ leading principal sub-matrices are non-singular.
\end{theorem}
%\end{itemize}
\end{frame}

% Slide 18
\begin{frame}{Backward error of LU factorization without pivoting}
\begin{itemize}
\item Consider a matrix $A$ whose leading principal sub-matrices are non-singular, and let $\tilde{L}$ and $\tilde{U}$ be \textbf{approximations} of the factors $L$ and $U$ of $A$.
\item \textbf{Backward error analysis} considers that $\tilde{L}$ and $\tilde{U}$ are \textbf{exact factors of a perturbed matrix}, i.e., there exists $\delta A$ such that\vspace{-.1cm}
\begin{align*}
A+\delta A=\tilde{L}\tilde{U}.
\end{align*}
$\vspace{-.7cm}$\\
The analysis consists then of bounding this perturbation.\\
\item In Lecture 03, we introduced backward error analysis in a way that is agnostic to the algorithm.
For the LU factorization, this is not the case:
\begin{itemize}\normalsize
\item[-] $\tilde{L}$ and $\tilde{U}$ are specifically assumed to be computed by \textbf{forward elimination} with \textbf{floating-point arithmetic}.
\item[-] Then, the perturbation $\delta A$ is bounded \textbf{component-wise} by\vspace{-.1cm}
\begin{align*}
|\delta A|\leq \gamma_n\cdot|\tilde{L}||\tilde{U}|
\end{align*}
$\vspace{-.6cm}$\\
where $\gamma_n:=nu/(1-nu)$ and $nu<1$, in which $u$ is the unit roundoff.
\item[-] In general, the components of $\!|\tilde{L}||\tilde{U}|\!$ can take \textbf{arbitrary large values}, i.e.,$\hspace{-.2cm}$\vspace{.1cm}\\
\begin{center}
\fbox{\begin{minipage}{0.9\textwidth}\begin{center}
\textbf{forward elimination without pivoting is not backward stable.}
\end{center}\end{minipage}}
\end{center}
\end{itemize}
\end{itemize}
\end{frame}

\section{LU factorization with pivoting\\{\small Section 3.4 in Darve \& Wootters (2021)}}

% Slide 19
\begin{frame}{Row pivoting}
\begin{itemize}
\item For a given matrix $A$, one way to reduce the backward error of an approximate LU factorization is to contain the components of $|\tilde{L}|$.
\item Since $L[i,k]=m_i^{(k)}:=a^{(k-1)}_{ik}/a_{kk}^{(k-1)}$ for $i=k+1,\dots,n$, this can be done if we allow ourselves to permute the rows of $A^{(k-1)}[k\!+\!1,1\!:\!n]$ such that $a_{kk}^{(k-1)}\geq a^{(k-1)}_{ik}$ for $i=k+1,\dots,n$.
Then we would have $|L|\leq 1$.
\end{itemize}
\begin{center}
\includegraphics[height=5cm]{images/pivoting.jpg}
\end{center}
\tiny{Darve, E., \& Wootters, M. (2021). Numerical linear algebra with Julia. Society for Industrial and Applied Mathematics.}
\end{frame}

% Slide 20
\begin{frame}{Row pivoting, cont'd\textsubscript{1}}
\begin{itemize}
\item When row pivoting is introduced in forward elimination, it is expressed as $G_{n-1}P_{n-1}\cdots G_1P_1A$, where $P_1,\dots,P_k$ denote row swap permutations.
\item The similarity transformation $PG_kP^{-1}$ of a Gauss transformation matrix $G_k$ with pivot column $k$ using a permutation matrix $P$, is another Gauss transformation matrix $\widetilde{G}_k$ with pivot column $k$, e.g.,$\hspace{-1cm}$\vspace{-.45cm}
\end{itemize}
\begin{center}
\includegraphics[height=4cm]{images/row-swap-permutations.jpg}
\end{center}\vspace{-.2cm}
\tiny{Darve, E., \& Wootters, M. (2021). Numerical linear algebra with Julia. Society for Industrial and Applied Mathematics.}
\begin{itemize}\normalsize\vspace{-.15cm}
\item Then, as we let $\widetilde{G}_k:=P_{n-1}\cdots P_{k+1}G_kP_{k+1}^{-1}\cdots P_{n-1}^{-1}$, you can show that
\begin{align*}
G_{n-1}P_{n-1}\cdots G_1 P_1A=\widetilde{G}_{n-1}\cdots \widetilde{G}_1P_{n-1}\cdots P_1A=:U.
\end{align*}
\end{itemize}
\end{frame}

% Slide 21
\begin{frame}{Row pivoting, cont'd\textsubscript{2}}
\begin{itemize}
\item Similarly as without pivoting, this can be recast as
\begin{align*}
\widetilde{G}_{n-1}\cdots \widetilde{G}_1P_{n-1}\cdots P_1A=&\;U\\
P_{n-1}\cdots P_1A=&\;\widetilde{G}_1^{-1}\cdots \widetilde{G}_{n-1}^{-1}U\\
PA=&\;LU
\end{align*}
where $\displaystyle L=\widetilde{G}_1^{-1}\cdots \widetilde{G}_{n-1}^{-1}$ and $\displaystyle
P=P_{n-1}\cdots P_1$.
\item That is, there is a permutation $P$ such that an LU factorization of $PA$ exists, and can be obtained by forward elimination without pivoting.
\item Upon applying row pivoting during forward elimination, such a permutation $P$ is recovered along with the triangular factors $L$ and $U$ such that $PA=LU$.\vspace{.05cm}\\
Then, one can solve for $x$ such that $Ax=b$ by
\begin{align*}
&1.\;\;\text{Solving for }z\text{ such that }Lz=Pb\\
&2.\;\;\text{Solving for }x\text{ such that }Ux=z.
\end{align*}
\end{itemize}
\end{frame}

% Slide 22
\begin{frame}{Material we skip, for now}
\begin{itemize}
\item \textbf{Column pivoting} (p.~101 in Darve and Wootters~(2021))
\begin{itemize}\normalsize
\item[-] Column pivoting is introduced to allow for the computation of rank revealing factorizations:\vspace{-.1cm}
\end{itemize}
\end{itemize}
\begin{center}
\includegraphics[height=2cm]{images/rank-revealing.jpg}
\end{center}
$\vspace{-.8cm}$\\
\tiny{Darve, E., \& Wootters, M. (2021). Numerical linear algebra with Julia. Society for Industrial and Applied Mathematics.}\normalsize\\
$\vspace{-.3cm}$\\
\begin{itemize}
\item \textbf{Full pivoting} (p.~102 in Darve and Wootters~(2021))
\begin{itemize}\normalsize
\item[-] Performing both row and column swaps allows for the computation of rank revealing factorization while maintaining stability.
\end{itemize}
\item \textbf{Rook pivoting} (p.~103 in Darve and Wootters~(2021))
\begin{itemize}\normalsize
\item[-] Reduces the cost of full pivoting by simplifying the search for swaps.
\end{itemize}
\item \textbf{Pivots and singular values} (p.~104 in Darve and Wootters~(2021))
\begin{itemize}\normalsize
\item[-] Pivoting strategies can also be used to compute approximately optimal low-rank matrix approximations.
\end{itemize}
\item We may come back to these topics in our talk about \textbf{preconditioners}.
\end{itemize}
\end{frame}

\section{Cholesky factorization\\{\small Section 3.5 in Darve \& Wootters (2021)}}

% Slide 23
\begin{frame}{Cholesky factorization}
\begin{itemize}
\item LU factorization is intended for general square matrices. 
For Hermitian positive-definite matrices, it is possible to leverage the properties of such matrices to yield a better behaved factorization.
\begin{theorem}[Cholesky factorization]
\begin{itemize}
\item[-] If $A\in\mathbb{F}^{n\times n}$ is Hermitian positive-definite, then there exists a lower-triangular matrix $L\in\mathbb{F}^{n\times n}$ such that $A=LL^H$.
\item[-] If we limit our search to lower-triangular matrices with positive components on the diagonal, then $L$ is unique.
\end{itemize}
\end{theorem}
\item The existence of such factors $L$ is proven by inductive construction.\\
In particular, $A$ being Hermitian, if $L$ exists, we must have
\begin{align*}
A=
\begin{bmatrix}
a_{11}&a_1^H\\
a_1&A_1
\end{bmatrix}
=
\begin{bmatrix}
l_{11}&0\\
l_1&L_1
\end{bmatrix}
\begin{bmatrix}
l_{11}&l_1^H\\
0&L_1^H
\end{bmatrix}
\;\text{ where }\;
L=
\begin{bmatrix}
l_{11}&0\\
l_1&L_1
\end{bmatrix}
\end{align*}
where, due to positive definiteness, $a_{11}>0$, and the principal block $A_1$ is Hermitian positive-definite.
\end{itemize}
\end{frame}

% Slide 24
\begin{frame}{Cholesky factorization, cont'd\textsubscript{1}}
\begin{itemize}
\vspace{.1cm}
\item[] This is recast into 
$\displaystyle A=
\begin{bmatrix}
a_{11}&a_1^H\\
a_1&A_1
\end{bmatrix}
=
\begin{bmatrix}
l_{11}^2&l_{11}l_1^H\\
l_{11}l_1&L_1L_1^H+l_1l_1^H
\end{bmatrix}.$\vspace{.2cm}\\
By construction, we impose $l_{11}>0$, so that we have 
\begin{align*}
l_{11}=\sqrt{a_{11}}
\;\text{ and }\;l_1=a_1/l_{11}.
\end{align*}
We rely here on the assumption that the Cholesky factorization $\displaystyle L_1L_1^H=A_1-l_1l_1^H$ exists.
To show that, $A$ can be recast into $XBX^H$
\begin{align*}
A=
\begin{bmatrix}
a_{11}&a_1^H\\
a_1&A_1
\end{bmatrix}
=
\begin{bmatrix}
1&0\\
l_1/l_{11}&I_{n-1}
\end{bmatrix}
\begin{bmatrix}
l_{11}^2&0\\
0&A_1-l_1l_1^H
\end{bmatrix}
\begin{bmatrix}
1&l_1^H/ l_{11}\\
0&I_{n-1}
\end{bmatrix}
\end{align*}
where $\displaystyle X=\begin{bmatrix}1&0\\l_1/l_{11}&I_{n-1}\end{bmatrix}$ and $\displaystyle B=\begin{bmatrix}
l_{11}^2&0\\
0&A_1-l_1l_1^H
\end{bmatrix}$.\vspace{.15cm}\\
Since $A$ is Hermitian positive-definite, and $X$ is non-singular, then $B$ must be positive-definite.\\
Moreover, since the principal sub-matrices of a Hermitian positive-definite matrix are positive-definite, so is $A_1-l_1l_1^H$.
\end{itemize}
\end{frame}

% Slide 25
\begin{frame}{Cholesky factorization, cont'd\textsubscript{2}}
\begin{itemize}
\item To complete the construction of $L$, we assume that the $l_{ij}$ components of $L$ are known for $i=1,\dots,k$ and $j=1,\dots,i$, s.t. $l_{11},\dots,l_{k}>0$ and
{
\begin{align*}
\hspace{-.5cm}A=
\begin{bmatrix}
a_{11}   & \dots &\overline{a_{k1}}  &a_1^H\\
\vdots   & \ddots&\vdots             &\vdots\\
a_{k1 }  & \dots &a_{kk}             &a_{k}^H\\
a_1      & \dots &a_{k}              &A_k
\end{bmatrix}
=
\begin{bmatrix}
l_{11}   &      &           &\\
\vdots   &\ddots&           &\\
l_{k1}   &\dots &l_{kk}     &\\
l_1      &\dots &l_{k}      &L_k
\end{bmatrix}
\begin{bmatrix}
l_{11}   &\dots &\overline{l_{k1}}&l_1^H  \\
         &\ddots&\vdots           &       \\
         &      &l_{kk}           &l_{k}^H\\
         &      &                 &L_k^H
\end{bmatrix}
\end{align*}
}\normalsize
where $\displaystyle A_k-l_{k}l_{k}^H-\dots-l_1l_1^H$ is Hermitian positive-definite with Cholesky factorization $L_kL_k^H$.
\item The construction of $L$ is completed by showing that $L_{k+1}$ can be defined under similar conditions.
\item The uniqueness of $L$ is revealed with the final requirement $|L_{n}|^2=a_{nn}.$\vspace{.05cm}\\
Since both $a_{nn}$ and $L_{n}$ need be strictly positive, we simply have $L_{n}=a_{nn}$.\\
\hfill$\qed$
\end{itemize}
\end{frame}

% Slide 26
\begin{frame}{Computation of the Cholesky factorization}
\begin{itemize}
\item The procedure to compute a Cholesky factor follows the lines of our constructive proof.
\item[] It requires about half the number of operations than that of calculating an LU factorization by forward elimination.
\item \textbf{Backward error analysis} considers that $\tilde{L}$ is an \textbf{exact factor of a perturbed matrix}, i.e., there exists $\delta A$ such that\vspace{-.1cm}
\begin{align*}
A+\delta A=\tilde{L}\tilde{L}^H.
\end{align*}
$\vspace{-.55cm}$\\
The analysis consists then of bounding this perturbation.\vspace{.07cm}
\begin{itemize}\normalsize
\item[-] Such analyses assume $\tilde{L}$ is specifically computed using the procedure we described, with \textbf{floating-point arithmetic}.\vspace{.07cm}
\item[-] Then, the perturbation $\delta A$ is bounded \textbf{component-wise} by%\vspace{-.1cm}
\begin{align*}
|\delta A|\leq \frac{\gamma_{n+1}}{1-\gamma_{n+1}}\cdot dd^T
\;\text{ where }\;
d=[a_{11}^{1/2}\;\dots\;a_{nn}^{1/2}]^T
\end{align*}
%$\vspace{-.6cm}$\\
with $\gamma_n:=nu/(1-nu)$ and $nu<1$, in which $u$ is the unit roundoff.\vspace{.07cm}
\item[-] Therefore, computing the Cholesky factorization is a \textbf{backward stable} procedure, and thus, it \textbf{does not require pivoting}.
\end{itemize}
\end{itemize}
\end{frame}

\section{Homework problems}

% Slide 27
\begin{frame}{Homework problems}
Turn in \textbf{your own} solution to \textbf{Pb.$\,$12}:\vspace{.08cm}\\
\begin{minipage}[t]{0.1\textwidth}
\textbf{Pb.$\,$11}
\end{minipage}
\begin{minipage}[t]{0.89\textwidth}
Show that, if a leading principal sub-matrix of $A$, i.e., $A[1\!:\!k,1\!:\!k]$ with $k$ such that $1\leq k<n$, is singular, then Doolittle's forward elimination procedure, if applied to $A$ without pivoting, will break down.
Explain when precisely and how the breakdown will happen.
\end{minipage}\vspace{.08cm}\\
\begin{minipage}[t]{0.1\textwidth}
\textbf{Pb.$\,$12}
\end{minipage}
\begin{minipage}[t]{0.89\textwidth}
Answer the following questions, and provide proper explanations:\vspace{-.08cm}
\begin{enumerate}
\item[a.] Are the principal sub-matrices of a Hermitian positive-definite (HPD) matrix also HPD?\vspace{-.08cm}
\item[b.] Let $A=XBX^H$ be HPD and $X$ be non-singular. Is $B$ also HPD?\vspace{-.08cm}
\item[c.] Are the principal sub-matrices of a non-singular matrix also non-singular?
\end{enumerate}
\end{minipage}\vspace{-.12cm}\\
\begin{minipage}[t]{0.1\textwidth}
\textbf{Pb.$\,$13}
\end{minipage}
\begin{minipage}[t]{0.89\textwidth}
Consider the symmetric positive-definite matrix given by $\displaystyle A=\begin{bmatrix}3&1\\1&3\end{bmatrix}$,\vspace{-.15cm}\\
with eigenpairs $\left(4,[1\;1]^T\right)$ and $\left(2, [-1\;1]^T\right)$:
\begin{enumerate}\normalsize
\item[a.] Construct, with a pen and paper, the Cholesky factor $L$ with positive diagonal components such that $A=LL^T$.\vspace{-.08cm}
\item[b.] Form the square root $A^{1/2}$ of $A$.
\end{enumerate}
\end{minipage}
\end{frame}

\section{Practice session}

% Slide 28
\begin{frame}[fragile]{Practice session}
\begin{enumerate}
\item Write a function called \mintinline{julia}{RowMajorForwardSubstitution} that implements forward substitution as described in slide \#3.
\item Write a function called \mintinline{julia}{ColumnMajorForwardSubstitution} that implements forward substitution fetching components of the lower triangular matrix in a column-wise fashion.
\item Compare the runtime of \mintinline{julia}{RowMajorForwardSubstitution} and \mintinline{julia}{ColumnMajorForwardSubstitution} for different matrix sizes.
\item Write a function called \mintinline{julia}{get_LU} that returns the $L$ and $U$ factors of a matrix $A$ obtained by forward elimination without pivoting. Test your code by solving a linear system using the $L$ and $U$ factors.
\item Write a function called \mintinline{julia}{RowMajor_LU_InPlace!} that computes the $L$ and $U$ factors of $A$, in-place, by forward elimination without pivoting.
Verify your code.
\item Write a function called \mintinline{julia}{ColumnMajor_LU_InPlace!} that does in-place LU factorization without pivoting, with a column-wise data access pattern.
\item Compare the runtime of \mintinline{julia}{RowMajor_LU_InPlace!} and \mintinline{julia}{ColumnMajor_LU_InPlace!} for different matrix sizes.
\end{enumerate}
\end{frame}

\end{document}
