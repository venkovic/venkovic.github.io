\documentclass[t,usepdftitle=false]{beamer}

\input{../../../tex-beamer-custom/preamble.tex}

\title[NLA for CS and IE -- Lecture 15]{Numerical Linear Algebra\\for Computational Science and Information Engineering}
\subtitle{\vspace{.3cm}Lecture 15\\Restarted Krylov Subspace Methods}
\hypersetup{pdftitle={NLA-for-CS-and-IE\_Lecture15}}

\date[Summer 2025]{Summer 2025}

\author[nicolas.venkovic@tum.de]{Nicolas Venkovic\\{\small nicolas.venkovic@tum.de}}
\institute[]{Group of Computational Mathematics\\School of Computation, Information and Technology\\Technical University of Munich}

\titlegraphic{\vspace{0cm}\includegraphics[height=1.1cm]{../../../logos/TUM-logo.png}}

\begin{document}
	
\begin{frame}[noframenumbering, plain]
	\maketitle
\end{frame}
	
\myoutlineframe

\section{Introduction}

% Slide 01
\begin{frame}{Recap of Arnoldi procedures}
\begin{itemize}
\item For a general non-singular matrix $A\in\mathbb{F}^{n\times n}$ and a unit-length vector $v_1\in\mathbb{F}^n$, the Arnoldi procedure computes an orthonormal basis of the Krylov subspace given by
\begin{align*}
\mathcal{K}_{m}(A,v_1):=\mathrm{Span}\{v_1,Av_1,\dots,A^{m-1}v_1\},
\end{align*}
typically for some $m\ll n$.
That is,
\begin{align*}
\texttt{Arnoldi}(A, v_1, m) \mapsto(V_{m+1},\underline{H}_m)
\end{align*}
where the basis in  $V_{m}:=[v_1\,\dots\,v_m]$ is such that $\text{range}(V_m)=\mathcal{K}_{m}(A,v_1)$, $V_m^TV_m=I_m$, and the \textbf{upper Hessenberg} matrix $\underline{H}_m=V_{m+1}^HAV_m$ is a \textbf{by-product} of the procedure.
\item  The \textbf{Arnoldi relation}, given by
\begin{align*}
AV_m=&\;V_{m}H_m+h_{m+1,m}v_{m+1}e_m^{(m)}\;\text{ where }\;H_m=V_m^HAV_m\\
AV_m=&\;V_{m+1}\underline{H}_m,
\end{align*}
is \textbf{essential to} the \textbf{analysis} of Arnoldi procedures and the $\!$\textbf{development $\!$of} $\hspace{-1cm}$\\
\textbf{Krylov subspace-based methods}, typically, for $\!$\textbf{non-symmetric $\!$matrices}.$\hspace{-1cm}$
\end{itemize}
\end{frame}

% Slide 02
\begin{frame}{Recap of Arnoldi procedures, cont'd}
\begin{itemize}
\item Most variants of Arnoldi procedures can be recast into:
\vspace{.15cm}
\begin{center}\fbox{\begin{varwidth}{8cm}
$\mathbf{for}\;j=1,\dots,m\;\mathbf{do}$\\
\hspace*{.4cm}$v_{j+1}:=\Pi_{V_j}Av_{j}$; $v_{j+1}:=v_{j+1}/\|v_{j+1}\|_2$
\end{varwidth}}\end{center}
\vspace{.15cm}
where $\Pi_{{V}_j}$ is a projector onto $\text{range}(V_j)^\perp$, plus some book-keeping work to recover the upper-Hessenberg matrix.
\item[] The sole specification of the projector $\Pi_{{V}_j}$ yields a number of variants of Arnoldi procedures.
In particular,
\begin{itemize}\normalsize
\item[-] \textbf{Classical Gram-schmidt} (\texttt{CGS}) variant with $\Pi_{V_j}:=I_n-V_{j}V_{j}^H$.
\item[] \texttt{CGS} is rarely used for \texttt{GMRES} due to stability issues.\vspace{.1cm}
\item[-] \textbf{Modified Gram-Schmidt} (\texttt{MGS}) variant with\\
\begin{center}$\Pi_{V_j}:=(I_n-v_{j}v_{j}^H)\dots(I_n-v_{1}v_{1}^H)$.\end{center}
\item[] \texttt{MGS} is often used in practice due to a better stability than \texttt{CGS}.\vspace{.1cm}
\item[-] \textbf{Reorthogonalized CGS} (\texttt{CGS2}) with $\Pi_{V_j}:=(I_n-V_{j}V_{j}^H)(I_n-V_{j}V_{j}^H)$.
\item[] \texttt{CGS2} used for even lower loss of orthogonality, and low-communication.\vspace{.1cm}
\item[-] Other variants include \textbf{Householder}, \textbf{block versions}, ...
\end{itemize}
\end{itemize}
\end{frame}

% Slide 03
\begin{frame}{Recap of the Lanczos procedure}
\begin{itemize}
\item The Lanczos process is a specialized form of the Arnoldi process for \textbf{symmetric matrices}.
\item When $A$ is symmetric (i.e., $A^T=A$), the Hessenberg matrix $H_m=V_m^TAV_m$ is symmetric too.
Consequently, it is \textbf{tridiagonal}:
\begin{align*}
T_m =
\begin{bmatrix}
\alpha_1 & \beta_1 & 0 & \cdots & 0 \\
\beta_1 & \alpha_2 & \beta_2 & \cdots & 0 \\
0 & \beta_2 & \alpha_3 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & \alpha_m
\end{bmatrix}
\end{align*}
where $\alpha_j = v_j^TAv_j$ is a diagonal element and $\beta_j=v_j^TAv_{j+1}=v_{j+1}^TAv_j\hspace{-1cm}$\\
an off-diagonal elements.
\item This tridiagonal structure means that \textbf{most terms vanish} in the Arnoldi recurrence relation:
\begin{align*}
Av_j=\beta_{j-1}v_{j-1}+\alpha_jv_j+\beta_jv_{j+1}.
\end{align*}
\item[] This $\!$three-term $\!$recurrence $\!$relation $\!$is the foundation of the Lanczos process.$\hspace{-1cm}$
\end{itemize}
\end{frame}

% Slide 04
\begin{frame}{Recap of the Lanzos procedure, cont'd\textsubscript{1}}
\begin{itemize}
\item The Lanczos algorithm can be formulated as follows:\vspace{-.25cm}
\begin{algorithm}[H]
\small
\caption{Lanczos procedure}
\begin{algorithmic}[1]
\STATE{Choose a starting vector $v_1$ with $\|v_1\|_2 = 1$}
\STATE{Set $\beta_0:=0$ and $v_0:=0$}
\FOR{$j=1,2,\dots,m$}
\STATE{$w:=Av_j$}
\STATE{$\alpha_j:=v_j^Tw$}
\STATE{$w:=w-\alpha_jv_j-\beta_{j-1}v_{j-1}$}
\STATE{$\beta_j:=\|w\|_2$}
\STATE{$v_{j+1}:=w/\beta_j$}
\ENDFOR
\end{algorithmic}
\end{algorithm}
$\vspace{-.8cm}$\\
\item After $m$ steps, we have:
\begin{itemize}\normalsize
\item[-] An orthonormal basis $V_m=[v_1,v_2,\dots,v_m]$ of $\mathcal{K}_m(A,v_1)$,
\item[-] A tridiagonal matrix $T_m=V_m^TAV_m$ with diagonal elements $\alpha_j$ and off-diagonal elements $\beta_j$.
\end{itemize}
\item Variants of the Lanczos procedure are introduced by
\begin{itemize}\normalsize
\item[-] Reorthogonalization, e.g., full vs selective reorthogonalization,
\item[-] Block versions.
\end{itemize}
\end{itemize}
\end{frame}

% Slide 05
\begin{frame}{Recap of the Lanzos procedure, cont'd\textsubscript{2}}
\begin{itemize}
\item Similar to the Arnoldi relation, there is a \textbf{Lanczos relation}:
\begin{align*}
AV_m=V_mT_m+\beta_mv_{m+1}e_m^{(m)}{}^T
\end{align*}
where $T_m$ is the tridiagonal matrix.
\item[] We can also write:
\begin{align*}
AV_m=V_{m+1}\underline{T}_m
\end{align*}
where $\underline{T}_m$ is the $(m+1) \times m$ tridiagonal matrix:
\begin{align*}
\underline{T}_j =
\begin{bmatrix}
\alpha_1 & \beta_1 & 0 & \cdots & 0 \\
\beta_1 & \alpha_2 & \beta_2 & \cdots & 0 \\
0 & \beta_2 & \alpha_3 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & \alpha_m \\
0 & 0 & 0 & \cdots & \beta_m
\end{bmatrix}.
\end{align*}
\end{itemize}
\end{frame}

% Slide 06
\begin{frame}{Practical limitations of Krylov subspace-based methods}
\begin{itemize}
\item Ever since Arnoldi (1951), methods that rely on \textbf{Arnoldi procedures} to construct orthonormal bases for, say, some Krylov subspace $\mathcal{K}_m(A,v_1)$ of $A\in\mathbb{R}^{n\times n}$, have been \textbf{affected by} the following \textbf{limitations}:
\begin{itemize}\normalsize
\item[-] \textbf{Orthogonalization} has time complexity of $\mathcal{O}(m^2n)$,
\item[-] \textbf{Krylov bases} are usually \textbf{dense}, and \textbf{must be stored}, for both linear and eigen solves, with spatial complexity of $\mathcal{O}(mn)$,
\item[-] In case of eigensolves, \textbf{solving the reduced eigenvalue problem} has time complexity of $\mathcal{O}(m^3)$.
\end{itemize}
\item For Lanczos-based approaches, the \textbf{orthogonalization} is \textbf{not a problem}, \textbf{due to} reduction to \textbf{short-recurrence} relations.
Moreover, \textbf{linear solvers need not save the entire basis}.
Remaining limitations are
\begin{itemize}\normalsize
\item[-] \textbf{Reorthogonalization}, when necessary, can have up to $\mathcal{O}(m^2n)$ time complexity,
\item[-] For \textbf{eigensolves only}, the \textbf{basis}, usually \textbf{dense}, needs be \textbf{stored}, with spatial complexity of $\mathcal{O}(mn)$.
\end{itemize}
\end{itemize}\smallskip
\tiny{Arnoldi, W. E. (1951). The principle of minimized iterations in the solution of the matrix eigenvalue problem. Quarterly of applied mathematics, 9(1), 17-29}
\end{frame}

% Slide 07
\begin{frame}{Motivation for restarting strategies}
\begin{itemize}
\item When applied to \textbf{large}, \textbf{challenging problems} with \textbf{limited resources}, Krylov subspace-based  methods face limitations which need be remedied.
\item[] \textbf{Restart}ing a procedure \textbf{with} the \textbf{best approximation available} is the \textbf{most natural}, and \textbf{oldest strategy}.
However,
\begin{itemize}\normalsize
\item[-] \textbf{When restarting} a Krylov subspace-based method, \textbf{information} about the Krylov subspace \textbf{is lost}, and this reflects into the quality of subsequent iterates, \textbf{slowing down convergence}, sometimes \textbf{even leading to stagnation},
\item[-] For \textbf{linear solves}, the best choice of restart vector is unambiguous.
\item[] But, for \textbf{eigensolves}, as we often attempt to calculate several eigenpairs simultaneously, \textbf{the best choice for a single restart vector is not as straightforward}.
\end{itemize}
\item Insight remains to be gained on \textbf{how to minimize convergence hindering due to restart}, and clever \textbf{strategies} are \textbf{needed}.
\item[] We'll see that, over the years, a number of works were published on this topic, pushing forward better, \textbf{more robust restarting practices}.
\end{itemize}
\end{frame}

\section{Polynomial restart of Arnoldi process}
% Slide 08
\begin{frame}{Polynomial restart of Arnoldi}
\begin{itemize}
\item The first documented attempt to restart an Arnoldi procedure for the \textbf{simultaneous computation of several eigenpairs} is by Saad (1980).
\item[] Let $\{\lambda_\ell,y_\ell\}_{\ell=1}^{nev}$ be Ritz vectors in a Krylov subspace $\mathcal{K}_{m}(A,v_1)$.
\item[] Then, Saad (1980) suggests to restart the Arnoldi procedure with 
\begin{align*}
v_1^{(r)}\propto
\sum_{\ell=1}^{nev}\|Ay_\ell-\lambda_\ell y_\ell\|_2\;\Re\{y_\ell\}.
\end{align*}
Reasons invoked for this choice are:
\begin{itemize}\normalsize
\item[-] Using $\Re\{y_\ell\}$ circumvents the need for complex arithmetic.
\item[-] Using the eigen-residual $\|Ay_\ell-\lambda_\ell y_\ell\|_2$ to weigh the $\ell$-th Ritz vector in the restart vector $v_1^{(r)}$ favors the contribution of slower converging eigenpairs.
Thus, the slow convergence of those approximate eigenpairs is tentatively set off using a starting vector $v_1^{(r)}$ which is richer in the corresponding exact eigenvectors.
\end{itemize}
\smallskip
\end{itemize}
\tiny{Saad, Y. (1980). Variations on Arnoldi's method for computing eigenelements of large unsymmetric matrices. Linear algebra and its applications, 34, 269-295.}
\end{frame}

% Slide 09
\begin{frame}{Polynomial restart of Arnoldi, cont'd}
\begin{itemize}
\item Since all vectors in $\mathcal{K}_m(A,v_1)$ can be represented as the product of a matrix polynomial with the starting vector $v_1$, i.e.,
\begin{align*}
\mathcal{K}_m(A,v)=
\left\{p(A)v_1,\,p\in\mathcal{P}_{m-1}\right\},
\end{align*}
the restart vector $v_1^{(r)}$ proposed by Saad (1980) admits a representation
\begin{align*}
v_1^{(r)}=\psi^{(r)}(A)v_1
\end{align*}
where $\psi^{(r)}\in\mathcal{P}_{m-1}$ may be specified by setting its roots.
\item[-] The restart polynomial $\psi^{(r)}$ should be small on the unwanted portion of the spectrum, and amplify the components of the starting vector in the direction of
desired eigenvectors.
\item[-] Saad (1984) made use of Chebyshev polynomials for when the unwanted portion of the spectrum is contained in an ellipse.
\item[-] Heuveline and Sadkane (1996) introduced the use of Faber polynomials for cases where the unwanted part of the spectrum does not fit in an ellipse.
\end{itemize}
\smallskip
\tiny{Saad, Y. (1984). Chebyshev acceleration techniques for solving nonsymmetric eigenvalue problems. Mathematics of Computation, 42(166), 567-588.}\tinyskip\\
\tiny{Heuveline, V., \& Sadkane, M. (1996). Arnoldi-Faber method for large non hermitian eigenvalue problems. Inria report no 3007.}
\end{frame}

\section{Implicitly restarted Arnoldi (IRA) process}

% Slide 10
\begin{frame}{Implicitly restarted Arnoldi}
\begin{itemize}
\item Explicit restarting involves directly applying a restart polynomial or building a linear combination of Ritz vectors to create a restart vector, which is then used to initiate an entirely new Arnoldi procedure.
\item As an alternative to explicit restart, Sorensen (1992) interprets the Arnoldi method from the perspective of a \textbf{QR eigenvalue iteration}, including implicit shifting and deflation.
\item[] The resulting \textbf{implicitly restarted Arnoldi} (\textbf{IRA}) method
\begin{itemize}\normalsize
\item[-] allows for more stable restarts,
\item[-] uses Ritz values of unwanted eigenpairs, referred to as exact shifts, as roots of the restart polynomial,
\item[-] is proven to converge to wanted eigenpairs in case of Hermitian matrices.
\end{itemize}
\item[] IRA is at the heart of ARPACK (Lehoucq et al., 1998), the current state-of-the-art software for the computation of limited numbers of eigenpairs of large (typically sparse) matrices.
\end{itemize}\smallskip
\tiny{Sorensen, D. C. (1992). Implicit application of polynomial filters in a $k$-step Arnoldi method. SIAM journal on matrix analysis and applications, 13(1), 357-385.}\tinyskip\\
\tiny{Lehoucq, R. B., Sorensen, D. C., \& Yang, C. (1998). ARPACK users' guide: solution of large-scale eigenvalue problems with implicitly restarted Arnoldi methods. Society for Industrial and Applied Mathematics.}
\end{frame}

% Slide 11
\begin{frame}{Implicitly restarted Arnoldi, cont'd\textsubscript{1}}
\begin{itemize}
\item After $m=k+p$ Arnoldi iterations, we have
\begin{align*}
AV_m=V_mH_m+r_me_m^{(m)}{}^T
\;\text{ where }\;
r_m:=h_{m+1,m}v_{m+1}.
\end{align*}
Then, Sorensen (1992) applies $p$ shifted QR iterations to $H_m$ as follows:
\vspace{.05cm}
\begin{center}\fbox{\begin{varwidth}{8cm}
$H_m^{(0)}:=H_m$\\
$\mathbf{for}\;i=1,\dots,p\;\mathbf{do}$\\
\hspace*{.4cm}Find $Q_i,R_i$ s.t. $Q_iR_i=H_m^{(i-1)}-\mu_i I_m$\\
\hspace*{.4cm}$H_m^{(i)}:=Q_i^TH_m^{(i-1)}Q_i$
\end{varwidth}}\end{center}
We then have:
\begin{align*}
H_m^{(p)}=Q^{(p)}{}^TH_mQ^{(p)}
\;\text{ where }\;
Q^{(p)}:=Q_1\cdots Q_p
\end{align*}
so that $H^{(p)}_m$ is upper-Hessenberg.
\item[] The $Q_1,\dots,Q_p$ matrices are orthonormal and upper-Hessenberg, so that $Q^{(p)}$ is banded with a lower bandwidth of $p$.
\end{itemize}\smallskip
\tiny{Sorensen, D. C. (1992). Implicit application of polynomial filters in a $k$-step Arnoldi method. SIAM journal on matrix analysis and applications, 13(1), 357-385.}
\end{frame}

% Slide 12
\begin{frame}{Implicitly restarted Arnoldi, cont'd\textsubscript{2}}
\begin{itemize}
\item Right-multiplying the Arnoldi relation by $Q^{(p)}$, we get:
\begin{align*}
AV_mQ^{(p)}=&\,V_mH_mQ^{(p)}+r_me_m^{(m)}{}^TQ^{(p)}\\
AV_mQ^{(p)}=&\,V_mQ^{(p)}Q^{(p)}{}^TH_mQ^{(p)}+r_me_m^{(m)}{}^TQ^{(p)}\\
AV_m^{(p)}=&\,V_m^{(p)}H_m^{(p)}+r_me_m^{(m)}{}^TQ^{(p)}
\end{align*}
where $V_m^{(p)}:=V_mQ^{(p)}$.
\item[] Due to the banded structure of $Q^{(p)}$, $e_m^{(m)}{}^TQ^{(p)}$ has the following structure:
\begin{align*}
e_m^{(m)}{}^TQ^{(p)}
=
[\underbrace{0\,\cdots\,0}_{k-1}\;\underbrace{*\,\cdots\,*}_{p+1}].
\end{align*}
Moreover, since $H^{(p)}_m$ is upper-Hessenberg, we obtain
\begin{align*}
AV_m^{(p)}[:,1\!:\!k]=
V_m^{(p)}[:,1\!:\!k]H_m^{(p)}[1\!:\!k,1\!:\!k]
+
\left(
h^{(k)}_{k+1,k}v_{k+1}^{(p)}+q^{(p)}_{m,k}r_m
\right)
e_k^{(k)}{}^T
\end{align*}
\end{itemize}\smallskip
\tiny{Sorensen, D. C. (1992). Implicit application of polynomial filters in a $k$-step Arnoldi method. SIAM journal on matrix analysis and applications, 13(1), 357-385.}
\end{frame}

% Slide 13
\begin{frame}{Implicitly restarted Arnoldi, cont'd\textsubscript{3}}
\begin{itemize}
\item[] so that we have a new Arnoldi relation:
\begin{align*}
AV_m^{(p)}[:,1\!:\!k]=
V_m^{(p)}[:,1\!:\!k]H_m^{(p)}[1\!:\!k,1\!:\!k]
+
r_k^{(p)}
e_k^{(k)}{}^T
\end{align*}
where $r_k^{(p)}:=
h^{(p)}_{k+1,k}v_{k+1}^{(p)}+q^{(p)}_{m,k}r_m$.
\item[-]There remains to specify the shifts $\mu_1,\dots,\mu_p$.
For that, we compute all the eigenvalues of $H_m$ and set the shifts $\mu_1,\dots,\mu_p$ to the $p$ eigenvalues which are the furthest from the targeted part of the spectrum.
\item[] By induction, we can show (homework) that
\begin{align}\label{eq:new-arnoldi}
v_1^{(r)}:=V_m^{(p)}e_1^{(m)}\propto (A-\mu_pI_n)\cdots(A-\mu_1I_n)v_1.
\end{align}
Essentially, that is, the shifted QR iterations \textit{implicitly} apply polynomial filtering to the starting vector $v_1$.
\item[] Therefore, upon picking-up an Arnoldi procedure from Eq.~\eqref{eq:new-arnoldi}, we have \textit{implicitly} applied a \textit{restart polynomial} $\psi(A):=\Pi_{i=1}^p(A-\mu_iI_n)$ to $v_1$.
\end{itemize}\smallskip
\tiny{Sorensen, D. C. (1992). Implicit application of polynomial filters in a $k$-step Arnoldi method. SIAM journal on matrix analysis and applications, 13(1), 357-385.}
\end{frame}

% Slide 14
\begin{frame}{Implicitly restarted Arnoldi, cont'd\textsubscript{4}}
\begin{itemize}
\item[-] If the shifts $\mu_1,\dots,\mu_p$ are exact eigenvalues of $A$, then the induced restart polynomial deflates the components of $v_1$ in the direction of the corresponding eigenvectors.
This motivates setting the shifts to the Ritz values which are the most remote from the targeted part of the spectrum.
\item[-] In practice, the shifts are not exact eigenvalues of $A$.
But if a shift $\mu_i$ is close to an eigenvalue of $A$, then the corresponding monomial of the restart polynomial removes small components of $v_1$ in the direction of nearby eigenvectors, thus relatively amplifying $v_1$'s components in the direction of the targeted eigenvectors, making the restarted subspace a better candidate for their approximation.
\item[-] Morgan (1996) shows that the implicitly restarted subspace of degree $k+i$ is Krylov, and given by\vspace{-.15cm}
\begin{align*}
\text{span}\{y_1,\dots,y_k,Ay_j,\dots,A^{i}y_j\}
\;\text{ for }\;
j=1,\dots,k,
\end{align*}
$\vspace{-.6cm}$\\
where $y_1,\dots,y_k$ are the targeted Ritz vectors.
\end{itemize}\smallskip
\tiny{Sorensen, D. C. (1992). Implicit application of polynomial filters in a $k$-step Arnoldi method. SIAM journal on matrix analysis and applications, 13(1), 357-385.}\tinyskip\\
\tiny{Morgan, R. (1996). On restarting the Arnoldi method for large nonsymmetric eigenvalue problems. Mathematics of Computation, 65(215), 1213-1230.}
\end{frame}

% Slide 15
\begin{frame}{Implicitly restarted Arnoldi, cont'd\textsubscript{5}}
\begin{itemize}
\item[] The implicitly restarted Arnorldi (IRA) procedure is given by:\vspace{-.25cm}
\begin{algorithm}[H]
\small
\caption{IRA}
\begin{algorithmic}[1]
\STATE{Perform $m:=k+p$ steps of standard Arnoldi procedure}
\STATE{We have $V_m:=[v_1,\dots,v_m]$ and $H_m=V_m^TAV_m$ s.t. $AV_m=V_mH_m+r_me_m^{(m)}{}^T$}
\STATE{Compute $m$ eigenvalues $\lambda_1,\dots,\lambda_m$ of $H_m$}
\STATE{Set the shifts $\mu_1,\dots,\mu_p$ to the eigenvalues $\lambda_i$ which are the furthest from the targeted part of the spectrum}
\STATE{$Q:=I_m$}
\FOR{$i=1,2\dots,p$}
\STATE{Find $Q_i,R_i$ such that $Q_iR_i=H_m-\mu_iI_m$}
\STATE{$H_m:=Q_i^TH_mQ_i$}
\STATE{$Q:=QQ_i$}
\ENDFOR
\STATE{$V_{k+1}:=V_mQ[:,1\!:\!k+1]$}
\STATE{$r_k:=h_{k+1,k}v_{k+1}+q_{m,k}r_m$}
\STATE{$H_k:=H_m[1\!:\!k,1\!:\!k]$}
\STATE{Continue Arnoldi iteration from $AV_{k}=V_kH_k+r_ke_k^{(k)}{}^T$}
\end{algorithmic}
\end{algorithm}
\end{itemize}\vspace{-.25cm}
\tiny{Sorensen, D. C. (1992). Implicit application of polynomial filters in a $k$-step Arnoldi method. SIAM journal on matrix analysis and applications, 13(1), 357-385.}
\end{frame}

\section{Arnoldi process thick-restarted with Rayleigh-Ritz vectors}
% Slide 16
\begin{frame}{Origin and adaptation of thick-restarting}
\begin{itemize}
\item Thick-restart was introduced by Wu and Simon (2000) as a way to explicitly restart Lanczos procedures with several approximate eigenvectors while limiting convergence hindering.
\item The concept of thick-restarting was later adapted by Morgan (2000) and Morgan \& Zeng (2006) to improve convergence behaviors of restarted GMRES and Arnoldi processes, respectively.
\item[] In this Section, we focus on the thick-restart of Arnoldi processes with Rayleigh-Ritz vectors.
\end{itemize}\smallskip
\tiny{Wu, K., \& Simon, H. (2000). Thick-restart Lanczos method for large symmetric eigenvalue problems. SIAM Journal on
Matrix Analysis and Applications, 22(2), 602-616.}\tinyskip\\
\tiny{Morgan, R. B. (2002). GMRES with deflated restarting. SIAM Journal on Scientific Computing, 24(1), 20-37.}\tinyskip\\
\tiny{Morgan, R. B., \& Zeng, M. (2006). A harmonic restarted Arnoldi algorithm for calculating eigenvalues and determining
multiplicity. Linear algebra and its applications, 415(1), 96-113.}
\end{frame}

% Slide 17
\begin{frame}{Reminders of Rayleigh-Ritz vectors in an Arnoldi process}
\begin{itemize}
\item From Lecture~11, we recall that, after performing $m$ iterations of an Arnoldi process, we are equipped with:
\begin{itemize}\normalsize
\item[-] An orthonormal basis in the columns of $V_m:=[v_1,\dots,v_m]$ such that 
\begin{align*}
\text{range}(V_m)=\mathcal{K}_m(A,v_1).
\end{align*}
\item[-] An upper-Hessenberg matrix $H_m=V_m^HAV_m$ such that
\begin{align*}
AV_m=V_mH_m
+h_{m+1,m}v_{m+1}e_m^{(m)}{}^T.
\end{align*}
\item[-] $k\leq m$ Rayleigh-Ritz vectors $y_1,\dots,y_k\in\text{range}(V_m)$ such that
\begin{align*}
y_\ell:=V_m\hat{y}_\ell
\;\text{ where }\;
\hat{y}_\ell\neq 0
\;\text{ is s.t. }\,
H_m\hat{y}_\ell=\lambda_\ell\hat{y}_\ell,
\end{align*}
with eigen-residual
\begin{align*}
\tilde{r}_\ell
:=
Ay_\ell-\lambda_\ell\hat{y}_\ell
\propto
v_{m+1}
\end{align*}
for $\ell=1,\dots,k$.
\end{itemize}
\end{itemize}
\end{frame}

% Slide 18
\begin{frame}{Thick-restarting Arnoldi with Rayleigh-Ritz vectors}
\begin{itemize}
\item From here, we describe the method introduced in Morgan \& Zeng (2006) to restart the Arnoldi process with Rayleigh-Ritz vectors.
\item[-] Let the restart vector be given by
\begin{align*}
v_{k+1}^{(r)}:=v_{m+1}.
\end{align*}
Then we want to generate a new orthonormal basis 
\begin{align*}
\{
v_1^{(r)},\dots,v_k^{(r)},
v_{k+1}^{(r)},\dots,v_m^{(r)}
\}
\end{align*}
for the restarted subspace given by:
\begin{align*}
\text{span}\{y_1,\dots,y_k,v_{k+1}^{(r)},Av_{k+1}^{(r)},\dots,A^{m-k-1}v_{k+1}^{(r)}\}.
\end{align*}
This is done in two steps.
\end{itemize}\smallskip
\tiny{Morgan, R. B., \& Zeng, M. (2006). A harmonic restarted Arnoldi algorithm for calculating eigenvalues and determining
multiplicity. Linear algebra and its applications, 415(1), 96-113.}
\end{frame}

% Slide 19
\begin{frame}{Thick-restarting Arnoldi with Rayleigh-Ritz vectors, cont'd\textsubscript{1}}
\begin{itemize}
\item[]
\begin{itemize}\normalsize
\item[-] First, let the $k$ first basis vectors $v_1^{(r)},\dots,v_k^{(r)}$ be the columns of the orthogonal factor $Q$ from the thin $QR$ decomposition of the stack of Rayleigh-Ritz vectors $Y_k:=[y_1,\dots,y_k]$:
\begin{itemize}\normalsize
\item[-] Since $Y_k:=V_m\hat{Y}_k$ and $V_m$ is orthonormal, we have
\begin{align}\label{eq:qr-restart}
V_k^{(r)}:=[v_1^{(r)},\dots,v_k^{(r)}]=V_m\hat{Q}_k
\end{align}
where $\hat{Q}_k$ is the orthonormal factor of the thin QR decomposition of $\hat{Y}_k$:
\begin{align*}
\hat{Y}_k=\hat{Q}_kR_k.
\end{align*}
\item[-] So, essentially, we only need to compute the thin QR decomposition of the low-dimensional $m$-by-$k$ matrix $\hat{Y}_k$, and then evaluate the right-hand-side of Eq.~\eqref{eq:qr-restart}.
\end{itemize}
\end{itemize}
\end{itemize}\smallskip
\tiny{Morgan, R. B., \& Zeng, M. (2006). A harmonic restarted Arnoldi algorithm for calculating eigenvalues and determining
multiplicity. Linear algebra and its applications, 415(1), 96-113.}
\end{frame}

% Slide 20
\begin{frame}{Thick-restarting Arnoldi with Rayleigh-Ritz vectors, cont'd\textsubscript{2}}
\begin{itemize}
\item[]
\begin{itemize}\normalsize
\item[-] Second, construct $v_{k+1}^{(r)},\dots,v_m^{(r)}$ by orthonormalizing $Av_{j}^{(r)}$ against
\begin{align*}
v_1^{(r)},\dots,v_k^{(r)},v_{k+1}^{(r)},\dots,v_{j}^{(r)}
\end{align*}
for $j=k+1,\dots m-1$:
\begin{itemize}\normalsize
\item[-]First, $v_{k+1}^{(r)}=v_{m+1}$ is orthogonal to $v_1^{(r)},\dots,v_k^{(r)}$ by construction.\vspace{.1cm}
\item[-]Then, just as with a standard Arnoldi procedure, we need a projector $\Pi_{V_j^{(r)}}$ onto 
$\text{span}\{v_1^{(r)},\dots,v_j^{(r)}\}^\perp$ so that
\begin{align*}
v_{j+1}^{(r)}:=\Pi_{V_j^{(r)}}Av_j^{(r)}/\|\Pi_{V_j^{(r)}}Av_j^{(r)}\|_2
\end{align*}
for $j=k+1,\dots,m-1$.
\end{itemize}
\end{itemize}
\end{itemize}\smallskip
\tiny{Morgan, R. B., \& Zeng, M. (2006). A harmonic restarted Arnoldi algorithm for calculating eigenvalues and determining
multiplicity. Linear algebra and its applications, 415(1), 96-113.}
\end{frame}

% Slide 21
\begin{frame}{Thick-restarting Arnoldi with Rayleigh-Ritz vectors, cont'd\textsubscript{3}}
\begin{itemize}
\item Then, the restarted search space is such that\vspace{-.1cm}
\begin{align*}
\text{range}(V_m^{(r)})
=&\,
\text{span}\{y_1,\dots,y_k,v_{k+1}^{(r)},Av_{k+1}^{(r)},\dots,A^{m-k-1}v_{k+1}^{(r)}\}\\
=&\,
\text{span}\{y_1,\dots,y_k,v_{m+1},Av_{m+1},\dots,A^{m-k-1}v_{m+1}\}\\
=&\,
\text{span}\{y_1,\dots,y_k,\tilde{r}_\ell,A\tilde{r}_\ell,\dots,A^{m-k-1}\tilde{r}_\ell\}
\;\text{ for }\;
\ell=1,\dots,k\\
=&\,
\text{span}\{y_1,\dots,y_k,y_\ell,Ay_\ell,\dots,A^{m-k-1}y_\ell\}
\;\text{ for }\;
\ell=1,\dots,k
\end{align*}
so that the restarted search space $\text{range}(V_m^{(r)})$ contains\vspace{-.1cm}
\begin{align*}
\mathcal{K}_{m-k+1}(A,y_\ell)
\;\text{ for }\;
\ell=1,\dots,k.
\end{align*}
\item[] Moreover, it was proven by Eiermann et al. (2000), Morgan (2000), Morgan (2002) and Stewart (2002) that the restarted search space is also Krylov as a whole.
\end{itemize}\smallskip
\tiny{Morgan, R. B. (2000). Implicitly restarted GMRES and Arnoldi methods for nonsymmetric systems of equations. SIAM
Journal on Matrix Analysis and Applications, 21(4), 1112-1135.}\\
\tiny{Eiermann, M., Ernst, O. G., \& Schneider, O. (2000). Analysis of acceleration strategies for restarted minimal residual methods. Journal of Computational and Applied Mathematics, 123(1-2), 261-292.}\\
\tiny{Stewart, G. W. (2002). A Krylov–Schur algorithm for large eigenproblems. SIAM Journal on Matrix Analysis and
Applications, 23(3), 601-614.}\\
\tiny{Morgan, R. B. (2002). GMRES with deflated restarting. SIAM Journal on Scientific Computing, 24(1), 20-37.}\\
\tiny{Morgan, R. B., \& Zeng, M. (2006). A harmonic restarted Arnoldi algorithm for calculating eigenvalues and determining
multiplicity. Linear algebra and its applications, 415(1), 96-113.}
\end{frame}

% Slide 22
\begin{frame}{Thick-restarting Arnoldi with Rayleigh-Ritz vectors, cont'd\textsubscript{4}}
\begin{itemize}
\item Let us now look into how we can generate new Rayleigh-Ritz vectors with respect to the restarted search space.
\item[] That is, we want Ritz pairs $(\lambda,y)$ such that
\begin{align*}
y\in\text{range}(V_m^{(r)})
\;\text{ and }\;
Ay-\lambda y
\perp
\text{range}(V_m^{(r)})
\end{align*}
where $V_m^{(r)}$ is orthonormal so that we search for non-trivial pairs $(\lambda,\hat{y})$ such that
\begin{align*}
H_m^{(r)}\hat{y}=\lambda\hat{y}
\;\text{ where }\;
H_m^{(r)}:=V_m^{(r)}{}^TAV_m^{(r)},
\end{align*}
after what a Rayleigh-Ritz vector is given by $y:=V_m^{(r)}\hat{y}$.
\item[] The projected matrix has the following block structure:
\begin{align*}
H_m^{(r)}
=
\begin{bmatrix}
V_k^{(r)T}AV_k^{(r)}&V_k^{(r)T}AV_{k+1:m}^{(r)}\\
V_{k+1:m}^{(r)T}AV_k^{(r)}&V_{k+1:m}^{(r)T}AV_{k+1:m}^{(r)}
\end{bmatrix}
\end{align*}
There remains to look into how those blocks can be efficiently assembled.
\end{itemize}\smallskip
\tiny{Morgan, R. B., \& Zeng, M. (2006). A harmonic restarted Arnoldi algorithm for calculating eigenvalues and determining
multiplicity. Linear algebra and its applications, 415(1), 96-113.}
\end{frame}

% Slide 23
\begin{frame}{Thick-restarting Arnoldi with Rayleigh-Ritz vectors, cont'd\textsubscript{4}}
\begin{itemize}
\item Without loss of generality, we may assume that the orthonormalization is achieved by modified Gram-Schmidt.
That is:
\begin{align*}
\Pi_{V_j^{(r)}}:=\left(I_n-v_j^{(r)}v_j^{(r)}{}^T\right)\cdots\left(I_n-v_1^{(r)}v_1^{(r)}{}^T\right)
\end{align*}
so that, for $j=k+1,\dots,m-1$, we have:
\begin{align*}
1.&\;v_{j+1}^{(r)}:=Av_{j}^{(r)}\\
2.&\;\textbf{for}\;i=1,\dots,j\\
3.&\hspace{.75cm}h_{ij}^{(r)}:=v_{i}^{(r)T}v_{j+1}^{(r)}\\
4.&\hspace{.64cm}v_{j+1}^{(r)}:=v_{j+1}^{(r)}-h_{ij}^{(r)}v_{i}^{(r)}\\
5.&\;h_{j+1,j}^{(r)}:=\|v_{j+1}^{(r)}\|_2\\
6.&\;v_{j+1}^{(r)}:=v_{j+1}^{(r)}/h_{j+1,j}^{(r)}
\end{align*}
\end{itemize}\smallskip
\tiny{Morgan, R. B., \& Zeng, M. (2006). A harmonic restarted Arnoldi algorithm for calculating eigenvalues and determining
multiplicity. Linear algebra and its applications, 415(1), 96-113.}
\end{frame}

% Slide 24
\begin{frame}{Thick-restarting Arnoldi with Rayleigh-Ritz vectors, cont'd\textsubscript{5}}
\begin{itemize}
\item Then, for $j=k+1,\dots,m-1$, we have:
\begin{align*}
h_{ij}^{(r)}
=&\,
v_i^{(r)}{}^T\left(I_n-v_{i-1}^{(r)T}v_{i-1}^{(r)}\right)\cdots\left(I_n-v_{1}^{(r)T}v_{1}^{(r)}\right)Av_j^{(r)}\\
=&\,
v_i^{(r)}{}^HAv_j^{(r)}
\;\text{ for }\;
i=1,\dots,j.
\end{align*}
We also have:\vspace{-.6cm}
\begin{align*}
h_{j+1,j}^{(r)}v_{j+1}^{(r)}=&\,\Pi_{V_j^{(r)}}Av_j^{(r)}\\
h_{j+1,j}^{(r)}v_{j+1}^{(r)T}v_{j+1}^{(r)}=&\,v_{j+1}^{(r)T}\Pi_{V_j^{(r)}}Av_j^{(r)}\\
h_{j+1,j}^{(r)}=&\,v_{j+1}^{(r)T}Av_j^{(r)}.
\end{align*}
Finally, we have:
\begin{align*}
h_{j+1,j}^{(r)}v_{i}^{(r)T}v_{j+1}^{(r)}=&\,v_{i}^{(r)T}\Pi_{V_j^{(r)}}Av_j^{(r)}
\;\implies\;
v_{i}^{(r)T}Av_j^{(r)}=0
\;\text{ for }\;
i>j+1,
\end{align*}
so that all the non-zero components of $H_m^{(r)}[:,1\!:\!m]$ are by-products of the orthogonalization procedure.
\end{itemize}\smallskip
\tiny{Morgan, R. B., \& Zeng, M. (2006). A harmonic restarted Arnoldi algorithm for calculating eigenvalues and determining
multiplicity. Linear algebra and its applications, 415(1), 96-113.}
\end{frame}

% Slide 25
\begin{frame}{Thick-restarting Arnoldi with Rayleigh-Ritz vectors, cont'd\textsubscript{6}}
\begin{itemize}
\item As for the leading $k$-by-$k$ block of $H_m^{(r)}$, we have:
\begin{align*}
H_m^{(r)}[1\!:\!k,1\!:\!k]
=
V_k^{(r)T}AV_k^{(r)}
=
(V_m\hat{Q}_k)^TA(V_m\hat{Q}_k)
=
\hat{Q}_k^TH_m\hat{Q}_k,
\end{align*}
whose dimensions are independent of $n$, and can be efficiently evaluated.
\item Last, we need to look into the off-diagonal block $H_m^{(r)}[k+1\!:\!m,1\!:\!k]$.
\item[] We recall that $\hat{Y}_k=\hat{Q}_kR_k$, so that
\begin{align*}
V_k^{(r)}=V_m\hat{Q}_k=V_m\hat{Y}_kR_k^{-1}.
\end{align*}
Using this equation along with the standard Arnoldi relation, we get:
\begin{align*}
v_j^{(r)T}AV_k^{(r)}
=&\,v_j^{(r)T}AV_m\hat{Y}_kR_k^{-1}\\
=&\,v_j^{(r)T}(V_mH_m+h_{m+1,m}v_{m+1}e_m^{(m)T})\hat{Y}_kR_k^{-1}.
\end{align*}
\item[-] Then, for $j=k+1$, we obtain:\vspace{-.07cm}
\begin{align*}
v_j^{(r)T}AV_k^{(r)}
=&\,v_{m+1}^T(V_mH_m+h_{m+1,m}v_{m+1}e_m^{(m)T})\hat{Y}_kR_k^{-1}\\
=&\,h_{m+1,m}e_m^{(m)T}\hat{Y}_kR_k^{-1}
=h_{m+1,m}e_m^{(m)T}\hat{Q}_k.
\end{align*}
\end{itemize}
\tiny{Morgan, R. B., \& Zeng, M. (2006). A harmonic restarted Arnoldi algorithm for calculating eigenvalues and determining
multiplicity. Linear algebra and its applications, 415(1), 96-113.}
\end{frame}

% Slide 26
\begin{frame}{Thick-restarting Arnoldi with Rayleigh-Ritz vectors, cont'd\textsubscript{7}}
\begin{itemize}
\item[-] And, for $j=k+2,\dots,m$, we obtain:
\begin{align*}
\hspace{-.5cm}
v_j^{(r)T}AV_k^{(r)}
=v_{j}^{(r)T}(V_mH_m+h_{m+1,m}v_{m+1}e_m^{(m)T})\hat{Y}_kR_k^{-1}
=v_{j}^{(r)T}V_mH_m\hat{Y}_kR_k^{-1}
\end{align*}
in which $H_m\hat{Y}_k=\hat{Y}_k\Lambda_k$, where $\Lambda_k:=\text{diag}(\lambda_1,\dots,\lambda_k)$ contains the Rayleigh-Ritz values, so that
\begin{align}\label{eq:off-diag-block}
v_j^{(r)T}AV_k^{(r)}
=v_{j}^{(r)T}V_m\hat{Y}_k\Lambda_kR_k^{-1}
=v_{j}^{(r)T}Y_k\Lambda_kR_k^{-1}.
\end{align}
Remember that, by construction, we have
\begin{align*}
v_j^{(r)T}V_k^{(r)}=0\;\text{ for }\;j=k+1,\dots,m,
\end{align*}
so that
\begin{align*}
v_j^{(r)T}V_k^{(r)}
=v_j^{(r)T}V_m\hat{Q}_k
=v_j^{(r)T}V_m\hat{Y}_kR_k^{-1}
=v_j^{(r)T}Y_kR_k^{-1}=0.
\end{align*}
Then, Eq.~\eqref{eq:off-diag-block} becomes
\begin{align*}
v_j^{(r)T}AV_k^{(r)}=0
\;\text{ for }\;
j=k+2,\dots,m.
\end{align*}
\end{itemize}\smallskip
\tiny{Morgan, R. B., \& Zeng, M. (2006). A harmonic restarted Arnoldi algorithm for calculating eigenvalues and determining
multiplicity. Linear algebra and its applications, 415(1), 96-113.}
\end{frame}

% Slide 27
\begin{frame}{Thick-restarting Arnoldi with Rayleigh-Ritz vectors, cont'd\textsubscript{8}}
\begin{itemize}
\item[-] Finally, this means that the off-diagonal block $H_m^{(r)}[k+1\!:\!m,1\!:\!k]$ is given by:
\begin{align*}
H_m^{(r)}[k+1\!:\!m,1\!:\!k]
=
\begin{bmatrix}
h_{m+1,m}e_m^{(m)T}\hat{Q}_k\\
0_{{(m-k-1)}\times 1}
\end{bmatrix}
=
h_{m+1,m}e_1^{(m-k)}e_m^{(m)T}\hat{Q}_k.
\end{align*}
We now know how to efficiently assemble $H_m^{(r)}$ in order to evaluate new Rayleigh-Ritz pairs after a thick-restart.
In particular, we have
\begin{align*}
H_m^{(r)}
=
\begin{bmatrix}
\hat{Q}_k^TH_m\hat{Q}_k&V_k^{(r)T}AV_{k+1:m}^{(r)}\\
h_{m+1,m}e_1^{m-k}e_m^{(m)T}\hat{Q}_k&V_{k+1:m}^{(r)T}AV_{k+1:m}^{(r)}
\end{bmatrix}
\end{align*}
where $V_k^{(r)T}AV_{k+1:m}^{(r)}$ and $V_{k+1:m}^{(r)T}AV_{k+1:m}^{(r)}$ are by-products of the orthogonlization procedure.
\end{itemize}\smallskip
\tiny{Morgan, R. B., \& Zeng, M. (2006). A harmonic restarted Arnoldi algorithm for calculating eigenvalues and determining
multiplicity. Linear algebra and its applications, 415(1), 96-113.}
\end{frame}

% Slide 28
\begin{frame}{Thick-restarting Arnoldi with Rayleigh-Ritz vectors, cont'd\textsubscript{8}}
\begin{itemize}
\item One can also show that the following Arnoldi relation holds:
\begin{align*}
AV_m^{(r)}=V_m^{(r)}H_m^{(r)}+h^{(r)}_{m+1,m}v_{m+1}^{(r)}e_m^{(m)T}
\end{align*}
so that new Rayleigh-Ritz pairs $(\lambda,y)$ with respect to $\text{range}(V_m^{(r)})$, i.e., so that there exist non-zero $\hat{y}$ such that 
\begin{align*}
H_m^{(r)}\hat{y}=\lambda\hat{y}
\;\text{ with }\;
y:=V_m^{(r)}\hat{y},
\end{align*}
have residuals given by:
\begin{align*}
\tilde{r}:=Ay-\lambda y=h_{m+1,m}^{(r)}(e_m^{(m)T}\hat{y})v_{m+1}^{(r)}.
\end{align*}
One can then monitor the eigen-residual norm of Rayleigh-Ritz iterates after the thick-restart, without having to assemble the approximate eigenvectors, nor to carry additional matrix-vector products.
\end{itemize}\smallskip
\tiny{Morgan, R. B., \& Zeng, M. (2006). A harmonic restarted Arnoldi algorithm for calculating eigenvalues and determining
multiplicity. Linear algebra and its applications, 415(1), 96-113.}
\end{frame}

\section{Arnoldi process thick-restarted with harmonic Ritz vectors}

\section{GMRES with deflated restart}

\section{Loose GMRES}

\section{Thick-restart of Lanczos process}

\section{Homework problems}
% Slide **
\begin{frame}{Homework problems}\vspace{.1cm}
Turn in \textbf{your own} solution to \textbf{Pb.$\,$33}:\vspace{.15cm}\\
\begin{minipage}[t]{0.1\textwidth}
\textbf{Pb.$\,$33}
\end{minipage}
\begin{minipage}[t]{0.89\textwidth}
Consider the Arnoldi relation achieved after an implicit restart:
\begin{align*}
AV_m^{(p)}=V_m^{(p)}H_m^{(p)}+r_k^{(p)}e_m^{(m)}{}^T
\end{align*}
where $V_m^{(p)}:=V_mQ^{(p)}$ and
\begin{align*}
H_m^{(p)}
=Q_{p+1}R_{p+1}+\mu_{p+1}I_m
=Q_p^TH_m^{(p-1)}Q_p
=Q^{(p)}{}^TH_mQ^{(p)}
\end{align*}
with $Q^{(p)}:=Q_1\cdots Q_p$.
Show that 
\begin{align*}
v_1^{(r)}:=
V_m^{(p)}e_1^{(p)}
\propto
(A-\mu_pI_n)\cdots(A-\mu_1I_n)v_1.
\end{align*}
\end{minipage}
\vspace{.15cm}
\begin{minipage}[t]{0.1\textwidth}
\textbf{Pb.$\,$34}
\end{minipage}
\begin{minipage}[t]{0.89\textwidth}
*.
\end{minipage}


\end{frame}

























	
\end{document}
