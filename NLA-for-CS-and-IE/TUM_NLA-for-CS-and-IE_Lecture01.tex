\documentclass[t,usepdftitle=false]{beamer}

\input{../../../tex-beamer-custom/preamble.tex}
\usepackage{mdframed}

\title[NLA for CS and IE -- Lecture 01]{Numerical Linear Algebra\\for Computational Science and Information Engineering}
\subtitle{\vspace{.3cm}Lecture 01\\Essentials of Linear Algebra}
\hypersetup{pdftitle={NLA-for-CS-and-IE\_Lecture01}}

\date[Summer 2025]{Summer 2025}

\author[nicolas.venkovic@tum.de]{Nicolas Venkovic\\{\small nicolas.venkovic@tum.de}}
\institute[]{Group of Computational Mathematics\\School of Computation, Information and Technology\\Technical University of Munich}

\titlegraphic{\vspace{0cm}\includegraphics[height=1.1cm]{../../../logos/TUM-logo.png}}

\begin{document}
	
\begin{frame}[noframenumbering, plain]
	\maketitle
\end{frame}
	
\myoutlineframe
	
\section{Vector spaces\\{\small Section 2.1 in Darve \& Wootters (2021)}}

% Slide 1
\begin{frame}{Vectors}
\begin{itemize}
\item We are interested in vectors in vector spaces $\mathbb{F}^n$ with real ($\mathbb{F}:=\mathbb{R}$) and complex ($\mathbb{F}:=\mathbb{C}$) scalar coefficients.\vspace{.1cm}\\
A vector $\mathbf{x}\in\mathbb{F}^n$ is an $n$-tuple given by
\begin{align*}
\mathbf{x}=
\begin{bmatrix}
x_1\\x_2\\\vdots\\x_n
\end{bmatrix}
\;\text{with scalar coefficients }
x_1,x_2,\dots,x_n\in\mathbb{F}.
\end{align*}
\item The vector space $\mathbb{F}^n$ is said to support addition, and scalar multiplication.
That is, for every pair $\mathbf{x},\mathbf{y}\in\mathbb{F}^n$ and $\alpha\in\mathbb{F}$, we have
\begin{align*}
\mathbf{x}+\alpha\mathbf{y}=
\begin{bmatrix}
x_1+\alpha y_1\\x_2+\alpha y_2\\\vdots\\x_n+\alpha y_n
\end{bmatrix}\in\mathbb{F}^n.
\end{align*}
\end{itemize}
\end{frame}

% Slide 2
\begin{frame}{Vector spaces}
\begin{itemize}
\item Vector spaces are fundamental structures of (numerical) linear algebra.\vspace{-.12cm}
\begin{definition}[Vector space]
A vector space $\mathcal{V}$ over a scalar field $\mathbb{F}$ is a non-empty set which supports addition and multiplication with the following axioms:
\begin{enumerate}
\item[1.] Associative addition:\hfill$\mathbf{x}+(\mathbf{y}+\mathbf{z})=(\mathbf{x}+\mathbf{y})+\mathbf{z}\;\forall\;\mathbf{x},\mathbf{y},\mathbf{z}\in \mathcal{V}$
\item[2.] Commutative addition:\hfill$\mathbf{x}+\mathbf{y}=\mathbf{y}+\mathbf{x}\;\forall\;\mathbf{x},\mathbf{y}\in \mathcal{V}$
\item[3.] Additive identity:\hfill$\exists\;\boldsymbol{0}\in \mathcal{V}$ s.t. $\mathbf{x}+\boldsymbol{0}=\mathbf{x}\;\forall\;\mathbf{x}\in \mathcal{V}$
\item[4.] Additive inverse:\hfill$\forall\;\mathbf{x}\in \mathcal{V}$, $\exists\;-\mathbf{x}\in \mathcal{V}$ s.t. $\mathbf{x}+(-\mathbf{x})=\boldsymbol{0}$
\item[5.] Field-multiplication compatibility:\hfill$\alpha(\beta \mathbf{x})=(\alpha\beta)\mathbf{x}\;\forall\;\mathbf{x}\in \mathcal{V},\alpha,\beta\in\mathbb{F}$
\item[6.] Field-multiplicative identity:\hfill$\exists\;1\in\mathbb{F}$ s.t. $1\mathbf{x}=\mathbf{x}\;\forall\;\mathbf{x}\in \mathcal{V}$
\item[7.] Distributive field multiplication w.r.t. $\!$vector addition:\\
\hfill$\alpha(\mathbf{x}+\mathbf{y})=\alpha \mathbf{x}+\alpha \mathbf{y}\;\forall\;\mathbf{x},\mathbf{y}\in \mathcal{V},\alpha\in\mathbb{F}$
\item[8.] Distributive field multiplication w.r.t. $\!$field addition:\\
\hfill$(\alpha+\beta)\mathbf{x}=\alpha \mathbf{x}+\beta \mathbf{x}\;\forall\;\mathbf{x}\in \mathcal{V},\alpha,\beta\in \mathbb{F}$
\end{enumerate}
\end{definition}
\end{itemize}
\end{frame}

% Slide 3
\begin{frame}{Vector spaces, cont'd}
\begin{itemize}
\item In practice, most axioms are trivially satisfied, and verifying whether $\mathcal{V}$ is a vector space boils down to checking for \textbf{closure under addition} and \textbf{scalar multiplication}, i.e., whether\vspace*{-.15cm}
\begin{align*}
\mathbf{x}+\mathbf{y}\in \mathcal{V}\;\text{ and }\;\alpha \mathbf{x}\in \mathcal{V}\;\forall\;\mathbf{x},\mathbf{y}\in \mathcal{V},\alpha\in\mathbb{F}.
\end{align*}
\vspace*{-.65cm}\\
\begin{exampleblock}{Example and counter-examples}
\begin{itemize}
\item[-] $\mathbb{R}^n$ \textbf{is} a vector space over $\mathbb{R}$.$\vspace{-.15cm}$
\item[-]  $\mathcal{V}:=\{\mathbf{x}\in\mathbb{R}^n\;\text{s.t.}\;x_i>0,\;i=1,\dots,n\}$ \textbf{is not} a vector space. $\vspace{-.15cm}$
\item[-] The set of floating-point numbers \textbf{does not} form a field, and cannot serve as the scalar field for a vector space.
%\mathcal{V}:=\{[\text{fl}(x_1),\dots,\text{fl}(x_n)]^T\;\text{s.t.}\;x_i\in\mathbb{R},\;i=1,\dots,n\}$ \textbf{is not} a vector space.
\end{itemize}
\end{exampleblock}
%~\vspace*{-.65cm}\\
%{\scriptsize{$\text{fl}(\alpha)$ stands for the floating-point representation of $\alpha\in\mathbb{R}.\vspace{-.1cm}$}}
~\vspace*{-.6cm}\\
\begin{alertblock}{Disclaimer}
\begin{itemize}
\item[-] The elements of a vector space $\mathcal{V}$ may not always be finite-dimensional vectors; they can also be \textbf{functions}, \textbf{even when} $\mathcal{V}$ \textbf{is finite-dimensional}.$\vspace{-.15cm}$
\item[-] In this course, we generally assume $\mathcal{V}\subseteq\mathbb{F}^n$, where the field $\mathbb{F}$ is either $\mathbb{R}$ or $\mathbb{C}.\vspace{-.15cm}$
\item[-] Note that much of what we cover here also applies to function spaces, which can be useful when developing methods to solve PDEs, ODEs, when processing spatio-temporal data, ...
\end{itemize}
\end{alertblock}
\end{itemize}
\end{frame}

% Slide 4
\begin{frame}{Linear combinations}
\begin{itemize}
\item A linear combination of vectors $\mathbf{x}_1,\dots,\mathbf{x}_k\in\mathbb{F}^n$ is prescribed by some scalars $\alpha_1,\dots,\alpha_k\in\mathbb{F}$ and given by
\begin{align*}
\alpha_1\mathbf{x}_1+\dots+\alpha_k\mathbf{x}_k\in\mathbb{F}^n.
\end{align*}
\item The span of $\mathbf{x}_1,\dots,\mathbf{x}_k\in\mathbb{F}^n$ is a subspace of $\mathbb{F}^n$ which consists of all the linear combinations of $\mathbf{x}_1,\dots,\mathbf{x}_n$, i.e.,
\begin{align*}
\text{span}\{\mathbf{x}_1,\dots,\mathbf{x}_k\}:=
\{
\alpha_1\mathbf{x}_1+\dots+\alpha_k\mathbf{x}_k
\;\text{s.t.}\;
\alpha_1,\dots,\alpha_k\in\mathbb{F}
\}
\end{align*}
\item The information that can be captured by linear combination depends on the \textbf{linear independence}, or lack thereof, of the spanning vectors.
\begin{definition}[Linear independence]
$\mathbf{x}_1,\dots,\mathbf{x}_k\in\mathbb{F}^n$ are linearly independent if no $\mathbf{x}_i$ can be written as a linear combination of the other vectors.
Or, equivalently, if\vspace{-.3cm}
\begin{align*}
\sum_{i=1}^k\alpha_i\mathbf{x}_i=0\implies\alpha_1,\dots,\alpha_k=0.
\end{align*}
\end{definition}
\end{itemize}
\end{frame}

% Slide 5
\begin{frame}{Bases, dimension and subspaces of vector spaces}
\begin{itemize}
\item The elements of a vector space can all be represented using a basis.
\begin{definition}[Basis \& dimension of vector space]
\begin{itemize}
\item[-] The vectors $\mathbf{v}_1,\dots,\mathbf{v}_k\in\mathcal{V}$ form a basis of the vector space $\mathcal{V}$, if they are linearly independent, and $\text{span}\{\mathbf{v}_1,\dots,\mathbf{v}_k\}=\mathcal{V}$.\vspace{-.15cm}
\item[-] While $\mathcal{V}$ admits infinitely many different bases, all of these consist of $k$ linearly independent vectors. We call $k$ the dimension of $\mathcal{V}$, and we write $\dim(\mathcal{V})=k$.
\end{itemize}
\end{definition}
For a basis $\mathbf{v_1},\dots,\mathbf{v}_k\in\mathcal{V}$ of $\mathcal{V}$, we define $\mathbf{V}:=[\mathbf{v_1},\dots,\mathbf{v}_k]$, so that for each $\mathbf{x}\in\mathcal{V}$, there is a unique $\boldsymbol{\alpha}\in\mathbb{F}^k$ such that $\mathbf{x}=\mathbf{V}\boldsymbol{\alpha}=\sum_{i=1}^k\alpha_i\mathbf{v}_i$.\vspace{.1cm}
\item In practice, linear subspaces, i.e., lower-dimensional vector spaces within vector spaces, are often used in place of high-dimensional vector spaces.
\begin{definition}[Linear subspace]
A linear subspace $\mathcal{S}\subset \mathcal{V}$ of a vector space $\mathcal{V}$ is a non-empty subset which is \textbf{closed under addition} and \textbf{scalar multiplication}, i.e.,\vspace*{-.2cm}
\begin{align*}
\mathbf{x}+\mathbf{y}\in \mathcal{S}\;\text{ and }\;\alpha \mathbf{x}\in \mathcal{S}\;\forall\;\mathbf{x},\mathbf{y}\in \mathcal{S},\alpha\in\mathbb{F}.
\end{align*}
\end{definition}
\end{itemize}
\end{frame}

\section{Inner products and norms\\{\small Section 2.2 in Darve \& Wootters (2021)}}

% Slide 6
\begin{frame}{Vector inner products}
\begin{itemize}
\item The abstract definition of length of a vector, angles and orthogonality between vectors, which are important notions in numerical linear algebra, is made possible with the use of inner products.
\begin{definition}[Inner product]
An inner product $(\cdot,\cdot)$ on a vector space $\mathcal{V}\!$ over $\mathbb{F}$ is a mapping $\mathcal{V}\!\times\!\mathcal{V}\!\rightarrow\mathbb{F}$ s.t.\\
\begin{enumerate}
\item[1.] $(\cdot,\cdot)$ is linear w.r.t. $\!$to its first argument:\\
\hfill$(\alpha\mathbf{x}_1+\beta\mathbf{x}_2,\mathbf{y})
=\alpha(\mathbf{x}_1,\mathbf{y})+\beta(\mathbf{x}_2,\mathbf{y})
\;\forall\;
\mathbf{x}_1,\mathbf{x}_2,\mathbf{y}\in\mathcal{V},\alpha,\beta\in\mathbb{F}$
\item[2.] $(\cdot,\cdot)$ is Hermitian:
\hfill$(\mathbf{y},\mathbf{x})=\overline{(\mathbf{x},\mathbf{y})}
\;\forall\;
\mathbf{x},\mathbf{y}\in \mathcal{V}$
\item[3.] $(\cdot,\cdot)$ is positive-definite:
\hfill$(\mathbf{x},\mathbf{x})\geq 0\;\forall\;\mathbf{x}\in \mathcal{V}$ and $(\mathbf{x},\mathbf{x})=0\iff \mathbf{x}=\boldsymbol{0}$
\end{enumerate}
\end{definition}
~\vspace*{-.65cm}\\
{\scriptsize{$\overline{\alpha}:=\Re\{\alpha\}-i\,\Im\{\alpha\}$ is the complex conjugate of $\alpha\in\mathbb{F}$.}}
\item In particular, \textbf{dot products}, which are often used on $\mathcal{V}\subseteq\mathbb{F}^n$, are given by:
\end{itemize}
\begin{itemize}
\centering
\item[-] $(\mathbf{x},\mathbf{y})=\mathbf{x}\cdot\mathbf{y}=\mathbf{x}^T\mathbf{y}=\sum_{i=1}^nx_iy_i\;\forall\;\mathbf{x},\mathbf{y}\in \mathcal{V}\subseteq\mathbb{R}^n$,\smallskip
\item[-] $(\mathbf{x},\mathbf{y})=\mathbf{x}\cdot\mathbf{y}=\mathbf{x}^H\mathbf{y}=\sum_{i=1}^n\overline{x_i}y_i\;\forall\;\mathbf{x},\mathbf{y}\in \mathcal{V}\subseteq\mathbb{C}^n$.
\end{itemize}
~\vspace*{-.45cm}\\
{\hspace{.42cm}\scriptsize{$\mathbf{x}^H:=\overline{\mathbf{x}}^T$ denotes the conjugate transpose of $\mathbf{x}\in\mathbb{F}^n$.}}
\end{frame}

% Slide 7
\begin{frame}{Properties of inner products}
\begin{itemize}
\vspace{-.075cm}
\item \textbf{By} the \textbf{Hermitian property}, every inner product $(\cdot,\cdot)$ on a vector space $\mathcal{V}$ is such that, for all $\mathbf{x}\in\mathcal{V}$, $(\mathbf{x},\mathbf{x})$ is \textbf{real}, even if $\mathcal{V}$ is defined over $\mathbb{C}$.\vspace{-.1cm}
\begin{theorem}[Cauchy-Schwarz inequality]
Inner products $(\cdot,\cdot):\mathcal{V}\times\mathcal{V}\rightarrow\mathbb{F}$ are s.t. $|(\mathbf{x},\mathbf{y})|^2\leq(\mathbf{x},\mathbf{x})(\mathbf{y},\mathbf{y})\;\forall\,\mathbf{x},\mathbf{y}\in\mathcal{V}.\hspace{-1cm}$\vspace{-.05cm}
\end{theorem}
\vspace{-.15cm}
\begin{proof}
Let $f(\mathbf{x},\mathbf{y},\alpha):=(\mathbf{x}-\alpha\mathbf{y},\mathbf{x}-\alpha\mathbf{y})$ for $\mathbf{x},\mathbf{y}\in\mathcal{V}\setminus\{\mathbf{0}\},\alpha\in\mathbb{F}.\hspace{-1cm}$\vspace{.075cm}\\
By linearity of 1st argument and Hermitian property, we have\vspace{-.275cm}
\begin{align*}
f(\mathbf{x},\mathbf{y},\alpha)=&\,(\mathbf{x},\mathbf{x}-\alpha\mathbf{y})-\alpha(\mathbf{y},\mathbf{x}-\alpha\mathbf{y})
=\overline{(\mathbf{x}-\alpha\mathbf{y},\mathbf{x})}-\alpha\overline{(\mathbf{x}-\alpha\mathbf{y},\mathbf{y})}\\
=&\,
(\mathbf{x},\mathbf{x})-
\overline{\alpha}\cdot(\mathbf{x},\mathbf{y})
-\alpha\cdot(\mathbf{y},\mathbf{x})
+|\alpha|^2\cdot(\mathbf{y},\mathbf{y})
\end{align*}
\vspace{-.775cm}\\
By positive-definiteness, we have $f(\mathbf{x},\mathbf{y},\alpha)\geq0$ so that\vspace{-.275cm}
\begin{align*}
(\mathbf{x},\mathbf{x})+|\alpha|^2\cdot(\mathbf{y},\mathbf{y})\geq
\overline{\alpha}\cdot(\mathbf{x},\mathbf{y})
+\alpha\cdot(\mathbf{y},\mathbf{x})
\end{align*}
\vspace{-.775cm}\\
Then, if we let $\alpha:=(\mathbf{x},\mathbf{y})/(\mathbf{y},\mathbf{y})$, we get\vspace{-.275cm}
\begin{align*}
\hspace{-.1cm}
(\mathbf{x},\mathbf{x})+\frac{|(\mathbf{x},\mathbf{y})|^2}{(\mathbf{y},\mathbf{y})}\geq
\frac{|(\mathbf{x},\mathbf{y})|^2}{(\mathbf{y},\mathbf{y})}
+\frac{|(\mathbf{x},\mathbf{y})|^2}{(\mathbf{y},\mathbf{y})}
\;\forall\; \mathbf{x},\mathbf{y}\in\mathcal{V}\setminus\{\mathbf{0}\},
\text{ then}
\times\text{by }(\mathbf{y},\mathbf{y}).
\end{align*}
\vspace{-.95cm}\\
\end{proof}
\end{itemize}
\end{frame}

% Slide 8
\begin{frame}{Vector norms}
\begin{itemize}
\item Vector norms are abstract measures of length in vector spaces.
\begin{definition}[Vector norm]
A vector norm $\|\cdot\|$ on a vector space $\mathcal{V}$ over $\mathbb{F}$ is any real-valued function s.t.\\
\begin{enumerate}
\item[1.] $\|\cdot\|$ is positive-definite:\hfill$\|\mathbf{x}\|\geq 0\;\forall\;\mathbf{x}\in \mathcal{V}$ and  $\|\mathbf{x}\|= 0\iff \mathbf{x}=\boldsymbol{0}$
\item[2.] $\|\cdot\|$ is homogeneous:\hfill$\|\alpha\mathbf{x}\|=|\alpha|\,\|\mathbf{x}\|\;\forall\;\mathbf{x}\in \mathcal{V},\alpha\in\mathbb{F}$
\item[3.] $\|\cdot\|$ satisfies the triangular inequality:\hfill$\|\mathbf{x}+\mathbf{y}\|\leq\|\mathbf{x}\|+\|\mathbf{y}\|\;\forall\;\mathbf{x},\mathbf{y}\in \mathcal{V}$
\end{enumerate}
\end{definition}
\item Popular vector norms on vector spaces $\mathcal{V}\subseteq \mathbb{F}^n$ are 
\begin{itemize}
\item[-]  $\;\,$1-norm: $\;\,\|\mathbf{x}\|_1:=\sum_{i=1}^n|x_i|\;\forall\;\mathbf{x}\in \mathcal{V}$.\smallskip
\item[-]  $\;\,$2-norm: $\;\,\|\mathbf{x}\|_2:=\left(\sum_{i=1}^n|x_i|^2\right)^{1/2}\;\forall\;\mathbf{x}\in \mathcal{V}$.\smallskip\\
$\;\,$- The 2-norm is induced by the dot product, i.e., $\|\mathbf{x}\|_2=(\mathbf{x}\cdot \mathbf{x})^{1/2}$.\smallskip\\
$\;\,$- Every inner product $(\cdot,\cdot)$ on $\mathcal{V}$ induces a norm $\|\mathbf{x}\|:=(\mathbf{x},\mathbf{x})^{1/2}\;\forall\;\mathbf{x}\in \mathcal{V}$.\smallskip
\item[-]  $\;\,p$-norm: $\;\,\|\mathbf{x}\|_p:=\left(\sum_{i=1}^n|x_i|^p\right)^{1/p}\;\forall\;\mathbf{x}\in\mathcal{V}$.\smallskip
\item[-]  $\infty$-norm: $\|\mathbf{x}\|_\infty:=\max_{1\leq i\leq n}|x_i|\;\forall\;\mathbf{x}\in \mathcal{V}$.
\end{itemize}
\end{itemize}
\end{frame}

% Slide 9
\begin{frame}{Equivalence of vector norms}
\begin{itemize}
\item A sequence of vectors $\{\mathbf{x}_k\}_{k\in\mathbb{N}}\subset \mathcal{V}$ \textbf{converges to a vector} $\mathbf{x}\in \mathcal{V}$ \textbf{under the norm} $\|\cdot\|$ defined on $\mathcal{V}$ if, for any real value $\epsilon>0$, there exists $K$ s.t. $\!\|\mathbf{x}_k-\mathbf{x}\|<\epsilon$ for all $k\geq K$.
\item Two vector norms $\|\cdot\|$ and $\|\cdot\|^\prime$ are \textbf{equivalent} if convergence under one norm implies convergence under the other.\\
The equivalence of vector norms is usually revealed by making use of the following theorem:
\begin{theorem}[Equivalent vector norms]
Two vector norms $\|\cdot\|$ and $\|\cdot\|^\prime$ on a vector space $\mathcal{V}$ are equivalent iff there exist real constants $C_1,C_2>0$ s.t. $\!C_1\|\mathbf{x}\|\leq\|\mathbf{x}\|^\prime\leq C_2\|\mathbf{x}\|\;\forall\;\mathbf{x}\in \mathcal{V}$.
\end{theorem}
\item In finite-dimensional vector spaces $\mathcal{V}\subseteq \mathbb{F}^n$, \textbf{all norms are equivalent}.\\
In particular, we have:\tinyskip\\
\hspace{3.2cm}\begin{minipage}{0.5\textwidth}
\begin{itemize}
\item[-] $\|\mathbf{x}\|_2\leq\|\mathbf{x}\|_1\leq \sqrt{n}\,\|\mathbf{x}\|_2\;\forall\;\mathbf{x}\in \mathcal{V}$,
\item[-] $\|\mathbf{x}\|_\infty\leq\|\mathbf{x}\|_2\leq \sqrt{n}\,\|\mathbf{x}\|_\infty\;\forall\;\mathbf{x}\in \mathcal{V}$,
\item[-] $\|\mathbf{x}\|_\infty\leq\|\mathbf{x}\|_1\leq n\,\|\mathbf{x}\|_\infty\;\forall\;\mathbf{x}\in \mathcal{V}$.
\end{itemize}
\end{minipage}
\end{itemize}
\end{frame}

% Slide 10
\begin{frame}{Orthogonality and orthonormality}
\begin{itemize}
\item For any inner product $(\cdot,\cdot)$ defined on a vector space $\mathcal{V}$ over $\mathbb{R}$, the notion of angle between two vectors $\mathbf{x},\mathbf{y}\in\mathcal{V}$ is introduced through the relation
\begin{align*}
(\mathbf{x},\mathbf{y})=\|\mathbf{x}\|\|\mathbf{y}\|\cos(\angle(\mathbf{x},\mathbf{y}))\;\forall\;\mathbf{x},\mathbf{y}\in\mathcal{V}
\end{align*}
where $\|\cdot\|$ is the induced norm $\|\mathbf{x}\|=(\mathbf{x},\mathbf{x})^{1/2}\;\forall\;\mathbf{x}\in\mathcal{V}$.\smallskip\\
Consequently, non-zero vectors $\mathbf{x},\mathbf{y}\in\mathcal{V}$ form a right angle, which is $cos(\angle(\mathbf{x},\mathbf{y}))=0$, iff $(\mathbf{x},\mathbf{y})=0$. 
More generally, 
\begin{definition}[Orthogonality \& orthonormality]
\begin{itemize}
\item[-] A set of vectors $\mathbf{x}_1,\dots,\mathbf{x}_k\in\mathcal{V}$ in a vector space $\mathcal{V}$ equipped with an inner product $(\cdot,\cdot)$ over a scalar field $\mathbb{F}$ is orthogonal if\vspace{0cm}
\begin{align*}
 i\neq j\implies(\mathbf{x}_i,\mathbf{x}_j)=0\;\text{ for }\;i,j=1,\dots,k.
\end{align*}
\item[-] If $(\mathbf{x}_i,\mathbf{x}_j)=\delta_{ij}$ for $i,j=1,\dots,k$, then the vectors $\mathbf{x}_1,\dots,\mathbf{x}_k$ are orthonormal.
\end{itemize}
\end{definition} 
\end{itemize}
\end{frame}

% Slide 11
\begin{frame}{Orthogonality and orthonormality, cont'd}
\begin{itemize}
\item The notion of orthogonal subspaces is useful for the definition and analysis of numerical methods in linear algebra.\vspace{-.1cm}
\begin{definition}[Orthogonal subspace \& orthogonal complement]
\begin{itemize}
\item[-] Let $\mathcal{S}$ and $\mathcal{T}$ be linear subspaces of the vector space $\mathcal{V}$ equipped with an inner product $(\cdot,\cdot)$.
Then, we say that $\mathcal{S}$ is orthogonal to $\mathcal{T}$, i.e., $\mathcal{S}\perp \mathcal{T}$, iff $(\mathbf{x},\mathbf{y})=0\;\forall\;\mathbf{x},\mathbf{y}\in \mathcal{S}\times \mathcal{T}$.\vspace{-.1cm}
\item[-] The orthogonal complement of $\mathcal{S}$, denoted by $\mathcal{S}^\perp$, consists of all the vectors in $\mathcal{V}$ which are orthogonal to $\mathcal{S}$, i.e.,\vspace{-.1cm}
\begin{align*}
\mathcal{S}^\perp:=\{\mathbf{x}\in \mathcal{V}\;\text{s.t.}\;(\mathbf{x},\mathbf{y})=0\;\forall\;\mathbf{y}\in \mathcal{S}\}.
\end{align*}
\end{itemize}
\end{definition}
\vspace{-.1cm}
\begin{block}{Theorem (Orthogonal decomposition of vector spaces)}
If $\mathcal{S}$ is a linear subspace of a vector space $\mathcal{V}\subseteq\mathbb{F}^n$, then every $\mathbf{x}\in\mathcal{V}$ admits a \textbf{unique decomposition} $\mathbf{x}=\mathbf{y}+\mathbf{z}$ where $\mathbf{y}\in\mathcal{S}$ and $\mathbf{z}\in\mathcal{S}^\perp$:
\vspace{-.1cm}
\begin{align*}
\mathcal{V}=\mathcal{S}\oplus \mathcal{S}^\perp
\;\text{ and }\;\dim(\mathcal{V})=\dim(\mathcal{S})+\dim(\mathcal{S}^\perp).
\end{align*}
$\vspace{-.7cm}$\\
We say that $\mathcal{V}$ is decomposed by the \textbf{direct sum} of $\mathcal{V}$ and $\mathcal{V}^\perp$.
\end{block}
\end{itemize}
\end{frame}

\section{Linear transformations and matrices\\{\small Section 2.3 in Darve \& Wootters (2021)}}

% Slide 12
\begin{frame}{$\hspace{-.2cm}$From linear transformations between vector spaces to matrices$\hspace{-1cm}$}
\begin{itemize}
\item Linear transformations (a.k.a. linear maps) are essential operations which are used over and over again in (numerical) linear algebra.
\begin{definition}[Linear transformation]
A linear transformation $T$ from a vector space $\mathcal{V}$ to another vector space $\mathcal{W}$, both defined over a field $\mathbb{F}$, i.e., $T:\mathbf{x}\in \mathcal{V}\mapsto T(\mathbf{x})\in \mathcal{W}$, is such that\vspace{-.2cm}
\begin{align*}
T(\mathbf{x}+\alpha\mathbf{y})=T(\mathbf{x})+\alpha T(\mathbf{y})
\;\forall\;\mathbf{x},\mathbf{y}\in \mathcal{V},\alpha\in \mathbb{F}.
\end{align*}
\end{definition}
\item Irrespective of whether a linear map operates between function or discrete vector spaces, practical problem-solving often benefits from expressing the action of such maps in a discrete, matrix form.
\begin{block}{Proposition (Matrix representation)}
The action of a linear transformation $T:\mathcal{V}\!\rightarrow \!\mathcal{W}$ between \textbf{finite-dimensional}$\hspace{-1cm}$\\
vector spaces defined over a same field $\mathbb{F}$, can be recast into a matrix-vector product with a matrix $\mathbf{A}\in\mathbb{F}^{m\times n}$ where $n:=\dim(\mathcal{V})$ and $m:=\dim(\mathcal{W})$.\vspace{.1cm}\\
Remark: A \textbf{function space} can be a finite dimensional vector space.
\end{block}
\end{itemize}
\end{frame}

% Slide 13
\begin{frame}{Review of matrix arithmetic}
\footnotesize
The components of $\mathbf{A},\mathbf{B}\in\mathbb{F}^{m\times n}$ and $\mathbf{C}\in\mathbb{F}^{n\times p}$ are denoted by $a_{ij}$, $b_{ij}$ and $c_{ij}$.\vspace{.1cm}\\
The vector $\mathbf{x}\in\mathbb{F}^n$ has components $x_i$ and $\alpha\in\mathbb{F}$.\vspace{.1cm}\\
\hspace{0.3cm}  % This line adds a small space to the right
\begin{minipage}{\dimexpr\textwidth-1cm}
\begin{columns}
\column{0.5\textwidth}
\textbf{Basic Operations}:\vspace{-.05cm}
\begin{itemize}
\item[-] Addition: $(\mathbf{A} + \mathbf{B})_{ij} = a_{ij} + b_{ij}$\vspace{-.05cm}
\item[-] Scalar mult.: $(\alpha\mathbf{A})_{ij} = \alpha a_{ij}$\vspace{-.05cm}
\item[-] Matrix mult.: $(\mathbf{A}\mathbf{C})_{ij} = \sum_k a_{ik}c_{kj}$\vspace{-.05cm}
\item[-] Matrix-vector: $(\mathbf{A}\mathbf{x})_i = \sum_j a_{ij}x_j$
\end{itemize}
\textbf{Transposition and Conjugation}:\vspace{-.05cm}
\begin{itemize}
\item[-] $(\mathbf{A}^T)_{ij} = a_{ji}$, $(\mathbf{A}^H)_{ij} = \overline{a_{ji}}$\vspace{-.05cm}
\item[-] $(\mathbf{A} + \mathbf{B})^H = \mathbf{A}^H + \mathbf{B}^H$\vspace{-.05cm}
\item[-] $(\mathbf{A}\mathbf{C})^H = \mathbf{C}^H \mathbf{A}^H$\vspace{-.05cm}
\item[-] $(\mathbf{A}^H)^H = \mathbf{A}$
\end{itemize}
\textbf{Inverse} ($m=n$, $\mathbf{A},\mathbf{B}\in\mathrm{GL}(n,\mathbb{F})$):\vspace{-.05cm}
\begin{itemize}
\item[-] $\mathbf{A}\mathbf{A}^{-1} = \mathbf{A}^{-1}\mathbf{A} = \mathbf{I}$\vspace{-.05cm}
\item[-] $(\mathbf{A}\mathbf{B})^{-1} = \mathbf{B}^{-1}\mathbf{A}^{-1}$\vspace{-.05cm}
\item[-] $(\mathbf{A}^H)^{-1} = (\mathbf{A}^{-1})^H$
\end{itemize}
\column{0.5\textwidth}
\textbf{Trace}:\vspace{-.05cm}
\begin{itemize}
\item[-] $\text{tr}(\mathbf{A}) = \sum_i a_{ii}$\vspace{-.05cm}
\item[-] $\text{tr}(\mathbf{A} + \mathbf{B}) = \text{tr}(\mathbf{A}) + \text{tr}(\mathbf{B})$\vspace{-.05cm}
\item[-] $\text{tr}(\mathbf{A}\mathbf{C}) = \text{tr}(\mathbf{C}\mathbf{A})$\vspace{-.05cm}
\item[-] $\text{tr}(\mathbf{A}^H) = \overline{\text{tr}(\mathbf{A})}$
\end{itemize}
\textbf{Determinant} ($m=n$):\vspace{-.05cm}
\begin{itemize}
\item[-] $\det(\mathbf{A}\mathbf{B}) = \det(\mathbf{A})\det(\mathbf{B})$\vspace{-.05cm}
\item[-] $\det(\mathbf{A}^H) = \overline{\det(\mathbf{A})}$\vspace{-.05cm}
\item[-] $\det(\mathbf{A}^{-1}) = (\det(\mathbf{A}))^{-1}\;\;(\mathbf{A}\in\mathrm{GL}(n,\mathbb{F}))\hspace{-1cm}$\vspace{-.05cm}
\item[-] $\det(\alpha\mathbf{A}) = \alpha^n\det(\mathbf{A})$
\end{itemize}
\textbf{Other Identities}:\vspace{-.05cm}
\begin{itemize}
\item[-] $\text{tr}(\mathbf{A}^H\mathbf{A}) = \|\mathbf{A}\|_F^2 = \sum_{i,j} |a_{ij}|^2$
\end{itemize}
\textbf{Other Identities} ($m=n$):\vspace{-.05cm}
\begin{itemize}
\item[-] $\text{tr}(\mathbf{A}) = \sum_i \lambda_i$\;\;($\lambda_i$ are eigenvalues)\vspace{-.05cm}
\item[-] $\det(\mathbf{A}) = \prod_i \lambda_i$
\end{itemize}
\end{columns}
\end{minipage}
\end{frame}

\begin{comment}
% Slide ..
\begin{frame}{$\hspace{-.25cm}$From linear transformations between vector spaces to matrices,$\hspace{-1cm}$\\\hspace{-.25cm}cont'd}
\begin{itemize}
\vspace{-.1cm}
\item The matrix representation of the linear map $T:\mathcal{V}\rightarrow \mathcal{W}$ is made explicit as follows for $\dim(\mathcal{V})=n$ and $\dim(\mathcal{W})=m$.\vspace{-.15cm}
\begin{proof}
\begin{itemize}
\item[-] Given that $\mathcal{V}\!$ is an $n$-dimensional vector space, there exists a basis $\mathbf{v}_1,\dots,\mathbf{v}_n\!\in\! \mathcal{V}\hspace{-1cm}$\\ such that for every $\mathbf{x}\in \mathcal{V}$, there is a unique $\boldsymbol{\alpha}\in\mathbb{F}^n$ satisfying $\mathbf{x}=\sum_{j=1}^n\alpha_j\mathbf{v}_j$.\vspace{-.1cm}
\item[-] Given that $T:\mathcal{V}\!\rightarrow \mathcal{W}$ is a linear map, $T(\mathbf{x})\!=\!T(\sum_{j=1}^n\alpha_j\mathbf{v}_j)=\!\sum_{j=1}^n\alpha_jT(\mathbf{v}_j).\hspace{-1cm}$\vspace{-.1cm}
\item[-] Since $\mathcal{W}$ is an $m$-dimensional vector space, there exists a basis $\mathbf{w}_1,\dots,\mathbf{w}_m\in \mathcal{W}$ such that for every $\mathbf{y}\in \mathcal{W}$, there is a unique $\boldsymbol{\beta}\in\mathbb{F}^m$ satisfying $\mathbf{y}=\sum_{i=1}^m\beta_i\mathbf{w}_i$.\smallskip\\
In particular, we denote by $\boldsymbol{\beta}^{(j)}\in\mathbb{F}^m$ the $m$-vector of coefficients used to express$\hspace{-1cm}$\\
the linear transformation $T(\mathbf{v}_j)$ of the basis vector $\mathbf{v}_j\in \mathcal{V}$ with respect to the basis of $\mathcal{W}$, i.e, $T(\mathbf{v}_j)=\mathbf{W}\boldsymbol{\beta}^{(j)}=\sum_{i=1}^m\beta_i^{(j)}\mathbf{w}_i$ where $\mathbf{W}:=[\mathbf{w}_1,\dots,\mathbf{w}_m].\hspace{-1cm}$\vspace{-.1cm}
\item[-] Then, we get $T(\mathbf{x})=\mathbf{W}\mathbf{A}\boldsymbol{\alpha}=\!\sum_{i=1}^m\sum_{j=1}^n\alpha_j\beta_i^{(j)}\mathbf{w}_i$,
where $\mathbf{A}:=[\boldsymbol{\beta}^{(1)}\!\!,\dots,\boldsymbol{\beta}^{(n)}]\hspace{-1cm}$\\
so that we have
$T(\mathbf{x})=\mathbf{W}\boldsymbol{\gamma}=\sum_{i=1}^m\gamma_i\mathbf{w}_i$, where $\boldsymbol{\gamma}=\mathbf{A}\boldsymbol{\alpha}\in\mathbb{F}^m$.\vspace{-.25cm}
\end{itemize}
\end{proof}
\end{itemize}
\end{frame}

% Slide ..
\begin{frame}{From compositions of linear transformations between vector spaces to matrix-matrix multiplications}
\begin{itemize}
\item Some algorithms are defined in the form of sequences of linear transformations between vector spaces.
\item Let us consider, two linear maps between vector spaces defined over a same field $\mathbb{F}$, given by $T_1:\mathcal{U}\rightarrow \mathcal{V}$ and $T_2:\mathcal{V}\rightarrow \mathcal{W}$.\\
We are interested in the composition map given by
\begin{align*}
T_2\circ T_1:
&\,\mathcal{U}\rightarrow \mathcal{W}\\
&\;\mathbf{x}\mapsto T_2(T_1(\mathbf{x})).
\end{align*}
\item Let the vector spaces $\mathcal{U},\mathcal{V}$ and $\mathcal{W}$ be finite-dimensional with some bases, respectively given by $\{\mathbf{u}\}_{i=1}^\ell\subset \mathcal{U}$, $\{\mathbf{v}\}_{i=1}^m\subset \mathcal{V}$ and $\{\mathbf{w}\}_{i=1}^n\subset \mathcal{W}$. Then,
\begin{align*}
\hspace{-.5cm}\forall\;\mathbf{x}\in \mathcal{U}:\exists \,\boldsymbol{\alpha}\in\mathbb{F}^\ell\;\text{s.t.}\;
(T_2\circ T_1)(\mathbf{x})=\mathbf{W}\mathbf{B}\mathbf{A}\boldsymbol{\alpha}\;\text{ where }\;\mathbf{W}:=[\mathbf{w}_1,\dots,\mathbf{w}_n]
\end{align*}
in which $\mathbf{A}\in\mathbb{F}^{m\times\ell}$ and $\mathbf{B}\in\mathbb{F}^{n\times m}$ are defined such that
\begin{align*}
\forall\;\mathbf{x}\in \mathcal{U}:&\;\exists \,\boldsymbol{\alpha}\in\mathbb{F}^\ell\;\;\text{s.t.}\;T_1(\mathbf{x})=\mathbf{V}\mathbf{A}\boldsymbol{\alpha}\;\text{ where }\;\mathbf{V}:=[\mathbf{v}_1,\dots,\mathbf{v}_m]\\
\forall\;\mathbf{y}\in V:&\;\exists \,\boldsymbol{\beta}\in\mathbb{F}^m\;\text{s.t.}\;T_2(\mathbf{x})=\mathbf{W}\mathbf{B}\boldsymbol{\beta}.
\end{align*}
\end{itemize}
\end{frame}
\end{comment}

% Slide 14
\begin{frame}{Fundamental subspaces associated with a matrix}
\begin{itemize}
\item For every matrix $\mathbf{A}\in\mathbb{F}^{m\times n}\!$, four linear subspaces of $\mathbb{F}^n\!$ and $\mathbb{F}^m\!$ are defined,$\hspace{-1cm}$\\
 whose characterization provides insight into the structure of linear systems, revealing key properties like solvability and the geometry of solutions.
\vspace{-.075cm}
\begin{definition}[Range of $\mathbf{A}$]
The range (or column space) of a matrix $\mathbf{A}\in\mathbb{F}^{m\times n}$ is a linear subspace of $\mathbb{F}^m$ given by\vspace{-.65cm}
\begin{align*}
\mathrm{range}(\mathbf{A}):=\{\mathbf{A}\mathbf{x}\;\text{ s.t.}\;\mathbf{x}\in\mathbb{F}^n\}.
\end{align*}
\end{definition}
\vspace{-.075cm}
\begin{definition}[Null space of $\mathbf{A}$]
The null space (or kernel) of a matrix $\mathbf{A}\in\mathbb{F}^{m\times n}$ is a linear subspace of $\mathbb{F}^n$ given by\vspace{-.65cm}
\begin{align*}
\mathrm{null}(\mathbf{A}):=\{\mathbf{x}\in\mathbb{F}^n\,\text{ s.t.}\;\mathbf{A}\mathbf{x}=\boldsymbol{0}\}.
\end{align*}
\end{definition}
\vspace{-.075cm}
\begin{definition}[Row space of $\mathbf{A}$]
The row space of a matrix $\mathbf{A}\in\mathbb{F}^{m\times n}$ is a linear subspace of $\mathbb{F}^n$ given by\vspace{-.25cm}
\begin{align*}
\mathrm{range}(\mathbf{A}^H):=\{\mathbf{A}^H\mathbf{y}\,\text{ s.t.}\;\mathbf{y}\in\mathbb{F}^m\}.
\end{align*}
\end{definition}
\end{itemize}
\end{frame}

% Slide 15
\begin{frame}{Fundamental subspaces associated with a matrix, cont'd}
\begin{itemize}
\item[] 
\begin{definition}[Left null space of $\mathbf{A}$]
The left null space of a matrix $\mathbf{A}\in\mathbb{F}^{m\times n}$ is a linear subspace of $\mathbb{F}^m$ given by\vspace{-.25cm}
\begin{align*}
\mathrm{null}(\mathbf{A}^H):=\{\mathbf{y}\in\mathbb{F}^m\,\text{ s.t.}\;\mathbf{A}^H\mathbf{y}=\boldsymbol{0}\}.
\end{align*}
\end{definition}
\vspace{-.1cm}
\begin{block}{Definition / Theorem (Rank of $\mathbf{A}$)}
\begin{itemize}
\item[-] The \textbf{column rank} of a matrix $\mathbf{A}\in\mathbb{F}^{m\times n}$ is the dimension of the column space of $\mathbf{A}$, i.e., $\dim(\mathrm{range}(\mathbf{A}))$.\vspace{-.15cm}
\item[-] The \textbf{row rank} of a matrix $\mathbf{A}\in\mathbb{F}^{m\times n}$ is the dimension of the row space of $\mathbf{A}$, i.e., $\dim(\mathrm{range}(\mathbf{A}^H))$.\vspace{-.15cm}
\item[-] The column rank and row ranks of $\mathbf{A}\in\mathbb{F}^{m\times n}$ are always equal.
This common value is the \textbf{rank} of $\mathbf{A}$, and is denoted by $\mathrm{rank}(\mathbf{A})$, with $\mathrm{rank}(\mathbf{A})\leq\min(m,n)$.
\end{itemize}
\end{block}
\vspace{-.1cm}
\begin{definition}[Full rank]
\begin{itemize}
\item[-] A matrix $\mathbf{A}\in\mathbb{F}^{m\times n}$ is \textbf{full-column-rank} if $\dim(\mathrm{range}(\mathbf{A}))=n$.\vspace{-.15cm}
\item[-] A matrix $\mathbf{A}\in\mathbb{F}^{m\times n}$ is \textbf{full-row-rank} if $\dim(\mathrm{range}(\mathbf{A}^H))=m$.\vspace{-.15cm}
\item[-] A matrix $\mathbf{A}\in\mathbb{F}^{m\times n}$ is \textbf{full-rank} if $\mathrm{rank}(\mathbf{A})=\min(m,n)$.
\end{itemize}
\end{definition}
\end{itemize}
\end{frame}

% Slide 16
\begin{frame}{Equivalence of column and row ranks}
\begin{itemize}
\item The equivalence between the row rank and the column rank of a matrix $\mathbf{A}\in\mathbb{F}^{m\times n}$ can be shown as follows.
\begin{proof}
\begin{itemize}
\item[-] Let $\mathbf{A}$ have row rank $r$, and $\mathbf{r}_1\dots,\mathbf{r}_r\in\mathbb{F}^n$ be a basis of $\mathrm{range}(\mathbf{A}^H)$.\vspace{-.05cm}
\item[-] Then, the vectors $\mathbf{A}\mathbf{r}_1,\dots,\mathbf{A}\mathbf{r}_r$ are linearly independent:\vspace{-.14cm}
\begin{itemize}
\item[-] Consider $\alpha_1,\dots,\alpha_r\in\mathbb{F}$ such that $\alpha_1\mathbf{A}\mathbf{r}_1+\dots+\alpha_r\mathbf{A}\mathbf{r}_r=\mathbf{0}$.\vspace{.1cm}
\item[-] Then, $\mathbf{x}:=\alpha_1\mathbf{r}_1+\dots+\alpha_r\mathbf{r}_r\in\mathrm{range}(\mathbf{A}^H)$ is such that $\mathbf{A}\mathbf{x}=\mathbf{0}$.\vspace{.1cm}
\item[-] For all $\mathbf{y}\in\mathbb{F}^{m}$, we then have $\mathbf{y}^H\mathbf{A}\mathbf{x}=\mathbf{x}^H\mathbf{A}^H\mathbf{y}=\mathbf{0}$, which implies $\mathbf{x}\!\perp\!\mathrm{range}(\mathbf{A}^H).\hspace{-1cm}$\vspace{.1cm}
\item[-] $\mathbf{x}\in\mathrm{range}(\mathbf{A}^H)$ and $\mathbf{x}\perp\mathrm{range}(\mathbf{A}^H)$ imply $\mathbf{x}=\mathbf{0}$. $\qedsymbol{}$\vspace{-.1cm}
\end{itemize}
\item[-] Then, since $\mathbf{A}\mathbf{r}_i\in\mathrm{range}(\mathbf{A})$ for $i=1,\dots,r$, we have $c:=\dim(\mathrm{range}(\mathbf{A}))\geq r$.\vspace{-.05cm}
\item[-] Let $\mathbf{c}_1\dots,\mathbf{c}_c\in\mathbb{F}^m$ be a basis of $\mathrm{range}(\mathbf{A})$.\vspace{-.05cm}
\item[-] Similarly, we can show that the vectors $\mathbf{A}^H\mathbf{c}_1,\dots,\mathbf{A}^H\mathbf{c}_c$ are linearly independent.\vspace{-.05cm}
\item[-] Since $\mathbf{A}^H\mathbf{c}_i\in\mathrm{range}(\mathbf{A}^H)$ for $i=1,\dots,c$, we have $r=\dim(\mathrm{range}(\mathbf{A}^H))\geq c$.\vspace{-.05cm}
\item[-] From $c\geq r$ and $r\geq c$, we have $r=c$.\vspace{-.5cm}
\end{itemize}
\end{proof}
\end{itemize}
\end{frame}

% Slide 17
\begin{frame}{Characterization of linear systems}
\begin{itemize}
\item A key goal of numerical linear algebra is to develop and analyze methods to find an unknown vector $\mathbf{x}\in\mathbb{F}^n$ such that $\mathbf{A}\mathbf{x}=\mathbf{b}$ where $\mathbf{A}\in\mathbb{F}^{m\times n}$ is a matrix and $\mathbf{b}\in\mathbb{F}^m$ is a given right-hand side.
\item Each such matrix equation corresponds to a system of $m$ linear equations with $n$ scalar unknowns $x_1,\dots,x_n\in\mathbb{F}$, of the form 
\begin{align*}
\begin{cases}
\;a_{11}x_1+\dots+\;a_{1n}x_n=b_1\\
\hspace{.55cm}\vdots\hspace{1cm}\vdots\hspace{1.2cm}\vdots\hspace{1cm}\vdots\\
a_{m1}x_1+\dots+a_{mn}x_n=b_m
\end{cases}
\end{align*}
\item A linear system is either:
\begin{itemize}
\item[-] \textbf{over-determined}: the number $m$ of equations is larger than the number $n$ of unknowns, i.e., $m>n$. 
$\mathbf{A}$ is a "tall"/"skinny" matrix.
\item[-] \textbf{under-determined}: the number $m$ of equations is smaller than the number $n$ of unknowns, i.e., $m<n$.
$\mathbf{A}$ is a "short"/"fat" matrix.
\item[-] \textbf{square}: the number of equations $m$ is equal to the number $n$ of unknowns, i.e., $m=n$.
$\mathbf{A}$ is a square matrix.
\end{itemize}
\end{itemize}
\end{frame}

% Slide 18
\begin{frame}{Characterization of linear systems, cont'd}
\begin{itemize}
\item A linear system is characterized by either of 3 situations:
\begin{itemize}
\item[-] The system admits \textbf{no solution}.
\item[-] The system has a \textbf{unique solution}.
\item[-] The system admits \textbf{infinitely many solutions}.
\end{itemize}
\item A proper characterization can be made using two notions:
\begin{enumerate}
\item[1.] \textbf{Consistency}: A linear system is consistent if $\mathbf{b}\in\mathrm{range}(\mathbf{A})$, which means there exists at least one vector $\mathbf{x}\in\mathbb{F}^n$ such that $\mathbf{A}\mathbf{x}=\mathbf{b}$.\tinyskip\\
An equivalent statement of consistency is $\mathrm{rank}([\mathbf{A},\mathbf{b}])=\mathrm{rank}(\mathbf{A})$, which implies that $\mathbf{b}$ can be formed by linear combination of the columns of $\mathbf{A}$.\tinyskip
\item[2.]  \textbf{Full column rank}: A \textbf{consistent} linear system has a \textbf{unique solution} iff the column rank of $\mathbf{A}$ equals the number of unknowns, i.e., $\dim(\mathrm{range}(\mathbf{A}))=n$.\tinyskip\\
On the other hand, if $\dim(\mathrm{range}(\mathbf{A}))<n$, and the linear system is consistent, then there exist infinitely many vectors $\mathbf{x}\in\mathbb{F}^m$ such that $\mathbf{A}\mathbf{x}=\mathbf{b}$.
\end{enumerate}
\item In conclusion:
\begin{itemize}
\item[-] A linear system has \textbf{solution(s)} \underline{iff} it is \textbf{consistent}.
\item[-] A linear system has a \textbf{unique solution} \underline{iff} it is \textbf{consistent} and it has \textbf{full column rank}.
\item[-] An \textbf{under-determined} linear system \textbf{cannot have} a \textbf{unique solution}.%, because it cannot have full column rank.
\end{itemize}
\end{itemize}
\end{frame}

% Slide 19
\begin{frame}{Invertible and singular matrices}
\begin{itemize}
\item The \textbf{identity matrix}, which we denote by $\mathbf{I}_n\in\mathbb{R}^{n\times n}$, is the matrix with ones on the diagonal, and zeros everywhere else.
\item A \textbf{square} matrix $\mathbf{A}\in\mathbb{F}^{n\times n}$ is \textbf{invertible} if there exists a matrix $\mathbf{B}\in\mathbb{F}^{n\times n}$ such that\vspace{-.2cm}
\begin{align*}
\mathbf{B}\mathbf{A}=\mathbf{A}\mathbf{B}=\mathbf{I}_n.
\end{align*}
\vspace{-.7cm}\\
If such a matrix exists, it is unique, denoted by $\mathbf{A}^{-1}$, and referred to as the inverse of $\mathbf{A}$.
\item A matrix $\mathbf{A}\in\mathbb{F}^{n\times n}$ has an inverse $\mathbf{A}^{-1}\in\mathbb{F}^{n\times n}$ iff
\begin{itemize}
\item[-] $\mathbf{A}\mathbf{x}=\mathbf{b}$ has a unique solution $\mathbf{x}\in\mathbb{F}^n$ for every $\mathbf{b}\in\mathbb{F}^n$.\vspace{.07cm}
\item[-] $\mathbf{A}$ is full-rank, i.e., $\mathrm{rank}(\mathbf{A})=n$.\vspace{.07cm}
\item[-] The columns of $\mathbf{A}$ form a basis of $\mathbb{F}^n$, i.e., $\mathrm{range}(\mathbf{A})=\mathbb{F}^n$.\vspace{.07cm}
\item[-] The null space of $\mathbf{A}$ is trivial, i.e., $\mathrm{null}(\mathbf{A})=\{\mathbf{0}\}$.\vspace{.07cm}
\item[-] $0$ is neither an eigenvalue nor a singular value of $\mathbf{A}$.\vspace{.07cm}
\item[-] $\det(\mathbf{A})\neq 0$.
\end{itemize}
\item The set of invertible matrices is the general linear group of degree $n$ over $\mathbb{F}\!,\hspace{-1cm}$ denoted by $\mathrm{GL}(n,\mathbb{F})$.
\end{itemize}
\end{frame}

% Slide 20
\begin{frame}{Moore-Penrose inverse}
\begin{itemize}
\item The \textbf{pseudo-inverse} (or \textbf{Moore-Penrose inverse}) of a matrix generalizes the concept of matrix inverse to rectangular and singular matrices.
\begin{definition}[Moore-Penrose inverse]
For a given matrix $\mathbf{A}\in\mathbb{F}^{m\times n}$, the pseudo-inverse $\mathbf{A}^\dagger\in\mathbb{F}^{n\times m}$ is \textbf{unique} and such that
\begin{itemize}
\item $\mathbf{A}^\dagger$ is consistent with $\mathbf{A}$:\hspace{1cm}$\mathbf{A}\mathbf{A}^\dagger\mathbf{A}=\mathbf{A}$
\item $\mathbf{A}^\dagger$ is consistent with $\mathbf{A}^\dagger$:\hspace{.85cm}$\mathbf{A}^\dagger\mathbf{A}\mathbf{A}^\dagger=\mathbf{A}^\dagger$
\item $\mathbf{A}^\dagger\mathbf{A}$ is Hermitian:\hspace{1.63cm}$(\mathbf{A}\mathbf{A}^\dagger)^H=\mathbf{A}\mathbf{A}^\dagger$
\item $\mathbf{A}\mathbf{A}^\dagger$ is Hermitian:\hspace{1.63cm}$(\mathbf{A}^\dagger\mathbf{A})^H=\mathbf{A}^\dagger\mathbf{A}$
\end{itemize}
\end{definition}
\item If a is invertible, then $\mathbf{A}^\dagger=\mathbf{A}^{-1}$.
\item Pseudo-inverses are used to provide \textbf{alternative solutions} to \textbf{linear systems which admit admit no standard solution}, or \textbf{infinitely many solutions}.
\end{itemize}
\end{frame}

% Slide 21
\begin{frame}{Different methods for different types of linear systems}
Methods for solving linear systems depend on the system's characteristics:
\begin{itemize}
\item Systems with \textbf{square matrices}:\vspace{.03cm}
\begin{itemize}
\item[-] \textbf{Full-rank} (\textbf{invertible}) matrices:\vspace{.03cm}
\begin{itemize}
\item[-] Direct methods: Gaussian elimination, LU decomposition.\vspace{.03cm}
\item[-] Iterative methods: Fixed-point methods, Krylov subspace methods.\vspace{.03cm}
\end{itemize}
\item[-] \textbf{Singular} matrices but \textbf{consistent} systems:\vspace{.03cm}
\begin{itemize}
\item[-] Low-rank approximation (SVD, pseudoinverse).\vspace{.03cm}
\item[-] Specialized iterative methods that exploit the structure of fundamental subspaces.\vspace{.03cm}
\end{itemize}
\end{itemize}
\item \textbf{Over-determined} systems (\textbf{tall-and-skinny matrices}):\vspace{.03cm}
\begin{itemize}
\item[-] \textbf{Full column-rank} matrices:\vspace{.03cm}
\begin{itemize}
\item[-] \textbf{Consistent} systems: QR decomposition or transformation to normal equations.\vspace{.03cm}
\item[-] \textbf{Inconsistent} systems: Least squares problems.\vspace{.03cm}
\end{itemize}
\item[-] \textbf{Rank-deficient} matrices: Least squares problems with regularization.\vspace{.03cm}
\end{itemize}
\item \textbf{Under-determined} systems (\textbf{short-and-fat matrices}):\vspace{.03cm}
\begin{itemize}
\item[-] \textbf{Consistent} systems: Low-rank approximation (SVD, pseudoinverse).\vspace{.03cm}
\item[-] \textbf{Inconsistent} systems: Least squares problems with regularization.\vspace{.03cm}
\end{itemize}
\item Least squares problems:\vspace{.03cm}
\begin{itemize}
\item[-] $\!$direct methods (through QR factorization), or$\!$ iterative methods$\!$ (LSQR, LSMR).$\hspace{-1cm}$
\end{itemize}
\end{itemize}
\end{frame}

% Slide 22
\begin{frame}{Types of matrices}
\begin{itemize}
\item Normal matrices, i.e., $\mathbf{A}\in\mathbb{F}^{n\times n}$ s.t.$\!$ $\mathbf{A}\mathbf{A}^H = \mathbf{A}^H\mathbf{A}$:
\begin{itemize}
\item[-] Diagonal matrices, i.e., $\mathbf{D}=\mathrm{diag}(d_{1},\dots,d_{n})$.\vspace{.05cm}
\item[-] Hermitian matrices, i.e., $\mathbf{A}\in\mathbb{F}^{n\times n}$ s.t.$\!$   $\mathbf{A}^H = \mathbf{A}$.\vspace{.05cm}
\item[-] Symmetric matrices, i.e., $\mathbf{A}\in\mathbb{R}^{n\times n}$ s.t.$\!$   $\mathbf{A}^T = \mathbf{A}$.\vspace{.05cm}
\item[-] Unitary matrices, i.e., $\mathbf{U}\in\mathbb{F}^{n\times n}$ s.t.$\!$  $\mathbf{U} \mathbf{U}^H=\mathbf{U} ^H\mathbf{U} = \mathbf{I}_n$, i.e., $\mathbf{U}^{-1}=\mathbf{U}^H$.\vspace{.05cm}
\item[-] Skew-Hermitian matrices, i.e.,  $\mathbf{A}\in\mathbb{F}^{n\times n}$ s.t.$\!$  $\mathbf{A}^H = -\mathbf{A}$.\vspace{.05cm}
\item[-] Skew-symmetric matrices, i.e.,  $\mathbf{A}\in\mathbb{R}^{n\times n}$ s.t.$\!$  $\mathbf{A}^T = -\mathbf{A}$.
\end{itemize}
\item Orthogonal matrices: $\mathbf{Q}\in\mathbb{R}^{m\times n}$, $\mathbf{Q}^T\mathbf{Q}=\mathbf{I}_n$.\vspace{.1cm}
\item Tridiagonal matrices: \raisebox{-.2cm}{\includegraphics[height=.6cm]{images/T.png}}
\item Triangular matrices:\vspace{-.1cm}
\begin{itemize}
\item[-] Lower-triangular matrices: \raisebox{-.2cm}{\includegraphics[height=.6cm]{images/L.jpg}}
\hspace{1cm}{\color{blue}-} Upper-triangular matrices: \raisebox{-.2cm}{\includegraphics[height=.6cm]{images/U.jpg}}
\end{itemize}
\item Hessenberg matrices:
\begin{itemize}
\item[-] Lower Hessenberg matrices: \raisebox{.4cm}{\includegraphics[height=.6cm,angle=180]{images/Upper-Hessenberg.jpg}}
\hspace{.72cm}{\color{blue}-} Upper Hessenberg matrices: \raisebox{-.2cm}{\includegraphics[height=.6cm]{images/Upper-Hessenberg.jpg}}
\end{itemize}
\vspace{.075cm}
\item Block diagonal matrices, i.e., $\mathbf{A}\in\mathbb{F}^{n\times n}$ s.t.$\!$ $\mathbf{A}=\mathrm{diag}(\mathbf{A}_1,\dots,\mathbf{A}_{n_b})$.
\item ...
\end{itemize}
\end{frame}

% Slide 23
\begin{frame}{Projections in $\mathbb{F}^n$}
\begin{itemize}
\vspace{-.03cm}
\item Projections, and their abstract representation, play an important role in formulating and understanding methods in NLA.\vspace{-.125cm}
\begin{definition}[Projection in $\mathbb{F}^n$ \& projector]
\begin{itemize}
\item[-]A projection in $\mathbb{F}^n$ is an \textbf{idempotent linear map} $\mathbf{x}\in\mathbb{F}^n\mapsto\mathbf{P}\mathbf{x}\in\mathbb{F}^n$, i.e., such that $\mathbf{P}^2\mathbf{x}=\mathbf{P}\mathbf{x}\;\forall\;\mathbf{x}\in\mathbb{F}^{n}$.
The matrix $\mathbf{P}\in\mathbb{F}^{n\times n}$ is called a \textbf{projector}.\vspace{-.1cm}
\item[-]The range of a non-trivial projector is a proper subset of $\mathbb{F}^n$, i.e., $\mathrm{rank}(\mathbf{P})<n$. 
\end{itemize}
\end{definition}
\vspace{-.175cm}
\begin{block}{Proposition (Complementary projector)}
If $\mathbf{P}\in\mathbb{F}^{n\times n}$ is a projector, then $\mathbf{I}_n-\mathbf{P}$ is a projector onto $\mathrm{null}(\mathbf{P})$.
\end{block}
\vspace{-.175cm}
\begin{proof}
\begin{itemize}
\item[-] $\mathbf{I}_n-\mathbf{P}$ is idempotent, i.e., $(\mathbf{I}_n\!-\mathbf{P})^2=\mathbf{I}_n\!-\mathbf{P}-\mathbf{P}+\mathbf{P}^2=\mathbf{I}_n\!-2\mathbf{P}+\mathbf{P}=\mathbf{I}_n\!-\mathbf{P}$.\vspace{-.1cm}
\item[-] $\mathrm{range}(\mathbf{I}_n-\mathbf{P})=\mathrm{null}(\mathbf{P})$:
\begin{itemize}
\item[-] $\mathbf{x}=\mathbf{P}\mathbf{x}+(\mathbf{I}_n-\mathbf{P})\mathbf{x}\;\forall\;\mathbf{x}\in\mathbb{F}^n$, so that $\mathbf{P}\mathbf{x}=\boldsymbol{0}\implies \mathbf{x}=(\mathbf{I}_n-\mathbf{P})\mathbf{x}$\\
\hspace{6.65cm}$\implies\mathrm{null}(\mathbf{P})\subseteq\mathrm{range}(\mathbf{I}_n-\mathbf{P})$.\vspace{.1cm}
\item[-] $\mathbf{P}(\mathbf{I}_n-\mathbf{P})\mathbf{x}=(\mathbf{P}-\mathbf{P}^2)\mathbf{x}
=(\mathbf{P}-\mathbf{P})\mathbf{x}=\boldsymbol{0}\;\forall\;\mathbf{x}\in\mathbb{F}^n\implies\mathrm{range}(\mathbf{I}_n-\mathbf{P})\subseteq \mathrm{null}(\mathbf{P})$.\vspace{-.25cm}
\end{itemize}
\end{itemize}
\end{proof}
\end{itemize}
\end{frame}

% Slide 24
\begin{frame}{Projections in $\mathbb{F}^n$, cont'd}
\begin{itemize}\item[]
\begin{theorem}[Decomposition of $\mathbb{F}^n$ via projection]
Given a projector $\mathbf{P}\in\mathbb{F}^{n\times n}$, every $\mathbf{x}\in\mathbb{F}^n$ is uniquely decomposed into a sum $\mathbf{x}=\mathbf{P}\mathbf{x}+(\mathbf{I}_n-\mathbf{P})\mathbf{x}$ with $\mathbf{P}\mathbf{x}\in\mathrm{range}(\mathbf{P})$
and
$(\mathbf{I}_n-\mathbf{P})\mathbf{x}\in\mathrm{null}(\mathbf{P})$:\vspace{-.25cm}
\begin{align*}
\mathbb{F}^n=&\,\mathrm{range}(\mathbf{P})\oplus\mathrm{null}(\mathbf{P})\\
n=&\,\mathrm{rank}(\mathbf{P})+\mathrm{rank}(\mathbf{I}_n-\mathbf{P}).
\end{align*}
\end{theorem}
\begin{proof}
\begin{itemize}
\item[-] $\mathrm{range}(\mathbf{P})\cap\mathrm{null}(\mathbf{P})=\{\mathbf{0}\}$.\vspace{.08cm}\\
Let $\mathbf{y}\in\mathrm{range}(\mathbf{P})\cap\mathrm{null}(\mathbf{P})$, then\vspace{-.1cm}
\begin{itemize}
\item[-] Since $\mathbf{P}$ is idempotent and $\mathbf{y}\in\mathrm{range}(\mathbf{P})$, we have $\mathbf{P}\mathbf{y}=\mathbf{y}$.\vspace{.08cm}
\item[-] From $\mathbf{y}\in\mathrm{null}(\mathbf{P})$, we have $\mathbf{P}\mathbf{y}=\mathbf{0}$.\vspace{.08cm}\\
$\implies\mathbf{y}=\mathbf{0}$.$\;\;\qedsymbol{}$
\end{itemize}
\vspace{-.18cm}
\item[-] Since $\mathbf{x}=\mathbf{P}\mathbf{x}+(\mathbf{I}_n-\mathbf{P})\mathbf{x}\;\forall\;\mathbf{x}\in\mathbb{F}^n$, in which $\mathbf{x}\mapsto\mathbf{P}\mathbf{x}$ is onto $\mathrm{range}(\mathbf{P})$, while
$\mathbf{x}\mapsto(\mathbf{I}_n-\mathbf{P})\mathbf{x}$ is onto $\mathrm{null}(\mathbf{P})$,
and $\mathrm{range}(\mathbf{P})\cap\mathrm{null}(\mathbf{P})=\{\mathbf{0}\}$,
we have that the decomposition of $\mathbf{x}$ via projection is unique.\vspace{-.25cm}
\end{itemize}
\end{proof}
\end{itemize}
\end{frame}

% Slide 25
\begin{frame}{Abstract and geometric formulation of projections in $\mathbb{F}^n$}
\begin{itemize}
\item For every pair $(\mathcal{M},\mathcal{S})$ of linear subspaces of $\mathbb{F}^n$ such that $\mathbb{F}^n=\mathcal{M}\oplus\mathcal{S}$, there exists a unique projector $\mathbf{P}\in\mathbb{F}^{n\times n}$ such that\vspace{-.05cm}
\begin{align*}
\mathcal{M}=\mathrm{range}(\mathbf{P})\;\;\text{and}\;\;\mathcal{S}=\mathrm{null}(\mathbf{P}).
\end{align*}
\item In general, $\mathbf{y}\in\mathcal{S}=\mathrm{null}(\mathbf{P})\;\;\not\!\!\!\!\!\iff\mathbf{y}\perp\mathcal{M}=\mathrm{range}(\mathbf{P})$.\vspace{.05cm}
\item For every such pair $(\mathcal{M},\mathcal{S})$, the linear map $\mathbf{x}\mapsto \mathbf{u}:=\mathbf{P}\mathbf{x}$ is recast into:\vspace{-.1cm}
\begin{align*}
\text{Find}\hspace{1cm}\mathbf{u}\in&\,\mathcal{M}\\
\text{s.t.}\hspace{.2cm}\mathbf{x}-\mathbf{u}\in&\,\mathcal{S}.
\end{align*}
$\vspace{-.55cm}$\\
Since $\mathbf{y}\in\mathcal{S}\iff\mathbf{y}\perp\mathcal{L}$ where $\mathcal{L}:=\mathcal{S}^\perp$, the projection is recast into\vspace{-.05cm}
\begin{empheq}[box=\fbox]{align}
\text{Find}\hspace{1cm}\mathbf{u}\in&\,\mathcal{M}
\;\;\;\;\text{where}\;\;\;\mathcal{M}=\mathrm{range}(\mathbf{P})\label{eq:proj-onto}\\
\text{s.t.}\hspace{.18cm}\mathbf{x}-\mathbf{u}\perp&\,\mathcal{L}\hspace{2.06cm}\mathcal{L}=\mathrm{null}(\mathbf{P})^\perp\label{eq:proj-ortho}
\end{empheq}
where $\mathcal{L}$ is the \textbf{orthogonality subspace} (or \textbf{space of constraints}).\vspace{.05cm}\\
We say that Eqs.~\eqref{eq:proj-onto}-\eqref{eq:proj-ortho} define a projector $\!\mathbf{P}$ \textbf{onto} $\!\mathcal{M}$ \textbf{perpendicular to} $\mathcal{L}\hspace{-1cm}$\\
(or \textbf{along} $\mathcal{L}$).
$\!$An advantage of this reformulation is that $\dim(\mathcal{L})\!=\!\dim(\mathcal{M})\!.\hspace{-1cm}$
%\item Non-orthogonal projections, Gram-Schmidt, Petrov-Galerkin, optimal representations of (orthogonal) projections.
\end{itemize}
\end{frame}

% Slide 26
\begin{frame}{Orthogonal and oblique projections in $\mathbb{F}^n$}
\begin{itemize}
\item Every projection induced by the decomposition of $\mathbb{F}^n$ into complementary subspaces $\mathcal{M},\mathcal{L}^\perp\subset\mathbb{F}^n$ is either orthogonal, or oblique.
\begin{definition}[Orthogonal projections \& oblique projections]
\begin{itemize}
\item[-] The projection in $\mathbb{F}^n$ induced by the linear subspaces $\mathcal{M},\mathcal{L}\subset \mathbb{F}^n$ such that $\mathbb{F}^n=\mathcal{M}\oplus\mathcal{L}^\perp$, is \textbf{orthogonal} if the space of constraints $\mathcal{L}$, is the approximation space $\mathcal{M}$, i.e., $\mathcal{L}=\mathcal{M}$.
Then, for every $\mathbf{x}\in\mathbb{F}^n$, the projection $\mathbf{x}\mapsto\mathbf{u}:=\mathbf{P}\mathbf{x}$ is the solution of \vspace{-.3cm}
\begin{align*}
\text{Find}\hspace{1cm}\mathbf{u}\in&\,\mathcal{M}\\
\text{s.t.}\hspace{.18cm}\mathbf{x}-\mathbf{u}\perp&\,\mathcal{M}.
\end{align*}
\vspace{-.8cm}
\item[-] A projection in $\mathbb{F}^n$ that is not orthogonal, is \textbf{oblique}.
Then, for every $\mathbf{x}\in\mathbb{F}^n$, the projection $\mathbf{x}\mapsto\mathbf{u}:=\mathbf{P}\mathbf{x}$ is the solution of%\vspace{-.1cm}
\begin{align*}
\text{Find}\hspace{1cm}\mathbf{u}\in&\,\mathcal{M}\\
\text{s.t.}\hspace{.18cm}\mathbf{x}-\mathbf{u}\perp&\,\mathcal{L}
\end{align*}
$\vspace{-.6cm}$\\
where the space of constraints $\mathcal{L}$ is different from the approximation space $\mathcal{M}$, i.e., $\mathcal{L}\neq \mathcal{M}$.
\end{itemize}
\end{definition}
\end{itemize}
\end{frame}

% Slide 27
\begin{frame}{Optimality of orthogonal projections in $\mathbb{F}^n$}
\begin{itemize}
\item Orthogonal projections have important approximation properties.
\vspace{-.058cm}
\begin{theorem}[Optimality of orthogonal projections in $\mathbb{F}^n$]
Let $\mathbf{x}\mapsto\mathbf{P}\mathbf{x}$ be the orthogonal projection induced by the linear subspace $\mathcal{M}\subset\mathbb{F}^n$.
Then, for every $\mathbf{x} \in\mathbb{F}^n$, $\mathbf{P}\mathbf{x}$ is the best approximation of $\mathbf{x}$ in $\mathcal{M}$:\vspace{-.12cm}
\begin{align*}
\|\mathbf{x}-\mathbf{P}\mathbf{x}\|=\underset{\mathbf{u}\in\mathcal{M}}{\min}\|\mathbf{x}-\mathbf{u}\|
\;\forall\;\mathbf{x}\in\mathbb{F}^n.
\end{align*}
\end{theorem}
\vspace{-.1cm}
\begin{proof}
\begin{itemize}
\item[-] Every orthogonal projection $\mathbf{x}\mapsto\mathbf{P}\mathbf{x}$ is such that $\mathrm{range}(\mathbf{P})^\perp=\mathrm{null}(\mathbf{P})$.\vspace{-.12cm}
\item[-] Then, for all $(\mathbf{x},\mathbf{u})\in\mathbb{F}^n\times\mathcal{M}$, we have:\vspace{-.12cm}
\begin{align*}
\|\mathbf{x}-\mathbf{u}\|
=\|(\mathbf{I}_n-\mathbf{P})\mathbf{x}+\mathbf{P}\mathbf{x}-\mathbf{u}\|
=\|(\mathbf{I}_n-\mathbf{P})\mathbf{x}\|+\|\mathbf{P}\mathbf{x}-\mathbf{u}\|
\end{align*}
\vspace*{-.55cm}\\
where we use the fact that $\mathrm{range}(\mathbf{I}_n-\mathbf{P})\perp\mathcal{M}$ and $\mathbf{u}\in\mathcal{M}=\mathrm{range}(\mathbf{P})$,
and we assume $\|\cdot\|$ is induced by the inner product from the definition of orthogonality.\vspace{-.12cm}
\item[-] Then, we have $\|\mathbf{x}-\mathbf{u}\|\geq\|\mathbf{x}-\mathbf{P}\mathbf{x}\|\;\forall\;\mathbf{u}\in\mathcal{M}$.\vspace{-.12cm}
\item[-] Moreover, $\|\mathbf{x}-\mathbf{u}\|$ is minimized when $\mathbf{u}=\mathbf{P}\mathbf{x}$.\vspace{-.4cm}
\end{itemize}
\end{proof}
\end{itemize}
\end{frame}

% Slide 28
\begin{frame}{Matrix form of orthogonal projections in $\mathbb{F}^n$}
\begin{itemize}
\item Let the linear subspace $\mathcal{M}\subset\mathbb{F}^n$ of an orthogonal projection $\mathbf{x}\mapsto\mathbf{P}\mathbf{x}$ have dimension $m$, i.e., $\dim(\mathcal{M})=m<n$.
\item Then, there exists a basis $\mathbf{v}_1,\dots,\mathbf{v}_m\in\mathcal{M}$ such that, for every $\mathbf{u}\in\mathcal{M}$, there is a unique $\hat{\mathbf{u}}\in\mathbb{F}^m$ such that $\mathbf{u}=\mathbf{V}\hat{\mathbf{u}}$, where $\mathbf{V}:=[\mathbf{v}_1,\dots,\mathbf{v}_m]$.
\item Then, from the geometric definition of an orthogonal projection onto $\mathcal{M}$ given by $\mathbf{x}\mapsto\mathbf{u}:=\mathbf{P}\mathbf{x}$, for every $\mathbf{x}\in\mathbb{F}^n$, there exists $\hat{\mathbf{u}}\in\mathbb{F}^m$ such that\vspace{-.1cm}
\begin{align*}
\hspace{-.5cm}
\mathbf{u}\in\mathcal{M},\;\mathbf{x}-\mathbf{u}\perp\mathcal{M}
\;\iff\;
\mathbf{u}=\mathbf{V}\hat{\mathbf{u}},\;
(\mathbf{v}_i,\mathbf{x}-\mathbf{V}\hat{\mathbf{u}})=0\;\text{ for }\;i=1,\dots,m.
\end{align*}
\item Let the inner product $(\cdot,\cdot)$ be a dot product, in which case we have\vspace{-.1cm}
\begin{align*}
\mathbf{V}^H(\mathbf{x}-\mathbf{V}\hat{\mathbf{u}})=&\,\mathbf{0}\\
\mathbf{V}^H\mathbf{x}=&\,\mathbf{V}^H\mathbf{V}\hat{\mathbf{u}}
\end{align*}
so that $\hat{\mathbf{u}}=(\mathbf{V}^H\mathbf{V})^{-1}\mathbf{V}^H\mathbf{x}$, and $\mathbf{u}=\mathbf{P}\mathbf{x}$, where
\begin{align*}
\boxed{\mathbf{P}=\mathbf{V}(\mathbf{V}^H\mathbf{V})^{-1}\mathbf{V}^H}.
\end{align*}
\item Note that the basis $\mathbf{v}_1,\dots,\mathbf{v}_m$ does not need to be orthogonal for $\mathbf{x}\mapsto\mathbf{P}\mathbf{x}$ to be an orthogonal projection. 
But if it is, then $\mathbf{P}=\mathbf{V}\mathbf{V}^H$.
\end{itemize}
\end{frame}

% Slide 29
\begin{frame}{Matrix form of oblique projections in $\mathbb{F}^n$}
\begin{itemize}
\item Let the linear subspaces $\mathcal{M},\mathcal{L}\subset\mathbb{F}^n$ of an oblique projection $\mathbf{x}\mapsto\mathbf{P}\mathbf{x}$ have dimension $m$, i.e., $\dim(\mathcal{M})=\dim(\mathcal{L})=m<n$ and $\mathcal{M}\oplus\mathcal{L}^\perp=\mathbb{F}^n$.
\item Then, there exist two bases $\mathbf{v}_1,\dots,\mathbf{v}_m\in\mathcal{M}$ 
and  $\mathbf{w}_1,\dots,\mathbf{w}_m\in\mathcal{L}$.
The first one is such that, for every $\mathbf{u}\in\mathcal{M}$, there is a unique $\hat{\mathbf{u}}\in\mathbb{F}^m$ such that $\mathbf{u}=\mathbf{V}\hat{\mathbf{u}}$, where $\mathbf{V}:=[\mathbf{v}_1,\dots,\mathbf{v}_m]$.
\item From the definition of an oblique projection onto $\mathcal{M}$ perpendicular to $\mathcal{L}$ given by $\mathbf{x}\mapsto\mathbf{u}:=\mathbf{P}\mathbf{x}$, for every $\mathbf{x}\in\mathbb{F}^n$, there exists $\hat{\mathbf{u}}\in\mathbb{F}^m$ such that\vspace{-.1cm}
\begin{align*}
\hspace{-.5cm}
\mathbf{u}\in\mathcal{M},\;\mathbf{x}-\mathbf{u}\perp\mathcal{L}
\;\iff\;
\mathbf{u}=\mathbf{V}\hat{\mathbf{u}},\;
(\mathbf{w}_i,\mathbf{x}-\mathbf{V}\hat{\mathbf{u}})=&\,0\;\text{ for }\;i=1,\dots,m\\
\mathbf{W}^H(\mathbf{x}-\mathbf{V}\hat{\mathbf{u}})=&\,\mathbf{0}\\
\mathbf{W}^H\mathbf{x}=&\;\mathbf{W}^H\mathbf{V}\hat{\mathbf{u}}.
\end{align*}
where the inner product is a dot product, so that $\hat{\mathbf{u}}=(\mathbf{W}^H\mathbf{V})^{-1}\mathbf{W}^H\mathbf{x}$, and $\mathbf{u}=\mathbf{P}\mathbf{x}$, where\vspace{-.12cm}
\begin{align*}
\boxed{\mathbf{P}=\mathbf{V}(\mathbf{W}^H\mathbf{V})^{-1}\mathbf{W}^H}
\end{align*}
where $\mathbf{W}^H\mathbf{V}$ is not singular because, by definition, we have $\mathcal{M}\!\cap\!\mathcal{L}^\perp\!=\!\{\mathbf{0}\}.\hspace{-1cm}$
\end{itemize}
\end{frame}

\begin{comment}
% Slide ..
\begin{frame}{Change of basis}
\begin{itemize}
\item A particularly useful basis of $\mathbb{F}^n$ is the \textbf{standard basis} $\mathbf{e}_1,\dots,\mathbf{e}_n$, in which $\mathbf{e}_i$ has all-zero components, except the $i$-th component, which is one.
\item Every vector $\mathbf{x}\in\mathbf{F}^n$ can be expressed as $\mathbf{x}=\sum_{i=1}^nx_i\mathbf{e}_i$.
\item For every alternative basis $\mathbf{v}_1,\dots,\mathbf{v}_n$ of $\mathbb{F}^n$, there exists a unique set of coefficients $\alpha_1,\dots,\alpha_n\in\mathbb{F}$ such that $\mathbf{x}=\sum_{i=1}^n\alpha_i\mathbf{v}_i$.\vspace{.1cm}\\
Equivalently, there is a matrix form $\mathbf{x}=\mathbf{V}\boldsymbol{\alpha}$ of this representation, in which $\mathbf{V}:=[\mathbf{v}_1,\dots,\mathbf{v}_n]$ and $\boldsymbol{\alpha}:=[\alpha_1,\dots,\alpha_n]^T$.
\item Since $\mathbf{V}$ is full-rank, we have $\boldsymbol{\alpha}=\mathbf{V}^{-1}\mathbf{x}$.
\end{itemize}
\end{frame}
\end{comment}

% Slide 30
\begin{frame}{Matrix norms}
\begin{itemize}
\item Matrix norms are abstract measures of the strength of a transformation. 
%They are used in designing numerical methods, assessing convergence bounds of iterative algorithms, and other aspects of NLA.
\begin{definition}[Matrix norm]
A matrix norm is a function $\mathbb{F}^{m\times n}\rightarrow \mathbb{R}$ such that\vspace{-.1cm}
\begin{enumerate}
\item[1.] $\|\cdot\|$ is positive-definite:\hfill$\|\mathbf{A}\|\geq 0$ and $\|\mathbf{A}\|=0\iff \mathbf{A}=\boldsymbol{0}$\vspace{-.075cm}
\item[2.] $\|\cdot\|$ is homogeneous:\hfill$\|\alpha\mathbf{A}\|=|\alpha|\|\mathbf{A}\|\;\forall\;\mathbf{A}\in\mathbb{F}^{m\times n},\alpha\in \mathbb{F}$\vspace{-.075cm}
\item[3.] $\|\cdot\|$ satisfies the triangular inequality:\hfill$\|\mathbf{A}+\mathbf{B}\|\leq \|\mathbf{A}\|+\|\mathbf{B}\|\;\forall\;\mathbf{A},\mathbf{B}\in\mathbb{F}^{m\times n}$
\end{enumerate}
\end{definition}
\item Some matrix norms are naturally induced by vector norms.%\vspace{-.1cm}
\begin{definition}[Induced norms]
Let $\|\cdot\|_\beta:\mathbb{F}^m\!\rightarrow\! \mathbb{R}$ and $\|\cdot\|_\alpha:\mathbb{F}^n\!\rightarrow \!\mathbb{R}$ be vector norms.
The \textbf{induced norm}$\hspace{-1cm}$\\  (or subordinate norm, or operator norm) $\|\cdot\|:\mathbb{F}^{m\times n}\rightarrow\mathbb{R}$ is the matrix norm defined as\vspace{-.2cm}
\begin{align*}
\|\mathbf{A}\|=\underset{\mathbf{x}\in\mathbb{F}^n\setminus\{\boldsymbol{0}\}}{\sup}\frac{\|\mathbf{A}\mathbf{x}\|_\beta}{\|\mathbf{x}\|_\alpha}\;\forall\;\mathbf{A}\in\mathbb{F}^{m\times n}.
\end{align*}
\end{definition}
\end{itemize}
\end{frame}

% Slide 31
\begin{frame}{Properties of matrix norms}
\begin{itemize} 
\item A number of properties of matrix norms can come in handy, in particular
\begin{definition}[Consistency of matrix norms]
\begin{itemize}
\item[-]The matrix norms $\|\cdot\|_\alpha:\mathbb{F}^{m\times n}\rightarrow \mathbb{R}$,
$\|\cdot\|_\beta:\mathbb{F}^{n\times p}\rightarrow \mathbb{R}$ and
$\|\cdot\|_\gamma:\mathbb{F}^{m\times p}\rightarrow \mathbb{R}$ are mutually \textbf{consistent} if
\begin{align*}
\|\mathbf{A}\mathbf{B}\|_\gamma\leq\|\mathbf{A}\|_\alpha\|\mathbf{B}\|_\beta\;\forall\;\mathbf{A}\in\mathbb{F}^{m\times n},\mathbf{B}\in\mathbb{F}^{n\times p}.
\end{align*}
\item[-] In case $m=n=p$ and $\gamma=\beta=\alpha$, we say that the matrix norm $\|\cdot\|_\alpha$ is \textbf{sub-multiplicative}.
\end{itemize}
\end{definition}
%\begin{definition}[Sub-multiplicative matrix norm]
%A matrix norm $\|\cdot\|:\mathbb{F}^{m\times n}\rightarrow \mathbb{R}$ is sub-multiplicative if
%\begin{align*}
%\|\mathbf{A}\mathbf{B}\|\leq\|\mathbf{A}\|\|\mathbf{B}\|\;\forall\;\mathbf{A}\in\mathbb{F}^{m\times n},\mathbf{B}\in\mathbb{F}^{n\times p}.
%\end{align*}
%\end{definition}
\item All \textbf{induced norms are consistent}, i.e., mutually consistent with themselves.
\item The \textbf{Frobenius norm}, which is \textbf{not induced by any vector norm}, and is defined as\vspace{-.3cm}
\begin{align*}
\|\mathbf{A}\|_F:=\left(\sum_{i=1}^m\sum_{j=1}^n a_{ij}^2\right)^{1/2}
\end{align*}
\vspace{-.4cm}\\
\textbf{is consistent}.
\end{itemize}
\end{frame}

% Slide 32
\begin{frame}{Properties of matrix norms, cont'd}
\begin{itemize} 
\item A number of properties of matrix norms can come in handy, in particular
\begin{definition}[Consistency of matrix and vector norms]
\begin{itemize}
\item[-] A matrix norm $\|\cdot\|:\mathbb{F}^{m\times n}\rightarrow \mathbb{R}$ is \textbf{consistent with} the vector norms $\|\cdot\|_\beta:\mathbb{F}^{m}\rightarrow \mathbb{R}$ and $\|\cdot\|_\alpha:\mathbb{F}^{n}\rightarrow \mathbb{R}$ if
\begin{align*}
\|\mathbf{A}\mathbf{x}\|_\beta\leq\|\mathbf{A}\|\|\mathbf{x}\|_\alpha\;\forall\;\mathbf{A}\in\mathbb{F}^{m\times n},\mathbf{x}\in\mathbb{F}^n.
\end{align*}
\item[-] In case $m=n$ and $\beta=\alpha$, we say the matrix norm $\|\cdot\|$ is \textbf{compatible with} $\|\cdot\|_\alpha$.
\item[-] All \textbf{induced norms} are \textbf{consistent} with their underlying vector norms \textbf{by definition}.
\end{itemize}
\end{definition}
\item The \textbf{Frobenius norm} is \textbf{consistent with vector 2-norms}.
\item For every matrix norm $\|\cdot\|:\mathbb{F}^{m\times n}\rightarrow\mathbb{R}$ induced by the vector norms $\|\cdot\|_\beta:\mathbb{F}^{m}\rightarrow \mathbb{R}$ and $\|\cdot\|_\alpha:\mathbb{F}^{m}\rightarrow \mathbb{R}$, and every matrix $\mathbf{A}\in\mathbb{F}^{m\times n}$, there exists $\mathbf{x}\in\mathbb{F}^n$ s.t. $\|\mathbf{Ax}\|_\beta=\|\mathbf{A}\|\|\mathbf{x}\|_\alpha.\hspace{-2cm}$
\end{itemize}
\end{frame}

% Slide 33
\begin{frame}{Equivalence of matrix norms}
\begin{itemize} 
\item A sequence of matrices $\{\!\mathbf{A}_k\!\}_{k\in\mathbb{N}}\!\subset\!\mathbb{F}^{m\times n}\!$ \textbf{converges to a matrix} $\mathbf{A}\!\in\!\mathbb{F}^{m\times n}\hspace{-1cm}$ \textbf{under the norm} $\|\cdot\|$ defined on $\mathbb{F}^{m\times n}$ if, for any real $\epsilon>0$, there exists $K$ s.t. $\|\mathbf{A}_k-\mathbf{A}\|<\epsilon$ for all $k>K$.
\item Two matrix norms $\|\cdot\|$ and $\|\cdot\|^{\prime}$ are \textbf{equivalent} if convergence under one norm implies convergence under the other.
This can be checked by
%The equivalence of matrix norms is usually revealed by making use of the following theorem:
\begin{theorem}[Equivalent matrix norms]
Two matrix norms  $\|\cdot\|$ and $\|\cdot\|^{\prime}$ on $\mathbb{F}^{m\times n}$ are equivalent iff there exist real constants $C_1,C_2>0$ s.t. $C_1\|\mathbf{A}\|\leq\|\mathbf{A}\|^{\prime}\leq C_2\|\mathbf{A}\|\;\forall\;\mathbf{A}\in\mathbb{F}^{m\times n}$.
\end{theorem}
\item \textbf{All matrix norms} defined \textbf{on} $\mathbb{F}^{m\times n}$ \textbf{are equivalent}.
In particular,\vspace{.2cm}\\
\hspace{2cm}\begin{minipage}{0.7\textwidth}
\begin{itemize}
%\centering
\item[-] $\frac{1}{\sqrt{n}}\|\mathbf{A}\|_\infty\leq\|\mathbf{A}\|_2\leq\sqrt{m}\|\mathbf{A}\|_\infty\;\forall\;\mathbf{A}\in\mathbb{F}^{m\times n}$,\vspace{-.1cm}
\item[-] $\frac{1}{\sqrt{m}}\|\mathbf{A}\|_1\leq\|\mathbf{A}\|_2\leq\sqrt{n}\|\mathbf{A}\|_1\;\forall\;\mathbf{A}\in\mathbb{F}^{m\times n}$,\vspace{-.1cm}
\item[-] $\frac{1}{\sqrt{\min(m,n)}}\|\mathbf{A}\|_F\leq\|\mathbf{A}\|_2\leq\|\mathbf{A}\|_F\;\forall\;\mathbf{A}\in\mathbb{F}^{m\times n}$,\vspace{-.1cm}
\item[-] $\underset{i,j}{\max}\,|a_{ij}|\leq\|\mathbf{A}\|_2\leq\sqrt{mn}\;\underset{i,j}{\max}\,|a_{ij}|\;\forall\;\mathbf{A}\in\mathbb{F}^{m\times n}$.
\end{itemize}
\end{minipage}
\end{itemize}
\end{frame}

\section{Eigenvalues and eigenvectors\\{\small Section 2.4 in Darve \& Wootters (2021)}}

% Slide 34
\begin{frame}{Eigenvalue and eigenvectors}
\begin{itemize}
\item \textbf{Eigenvalues and eigenvectors} reveal how a square matrix transforms space by identifying invariant directions (along eigenvectors) and their scaling factors (eigenvalues).
\begin{definition}[Eigenvalues, eigenvectors \& spectrum]
\begin{itemize}
\item[-] A scalar $\lambda\in\mathbb{C}$ is an eigenvalue of $\mathbf{A}\in\mathbb{F}^{n\times n}$ if there is a non-zero vector $\mathbf{u}\in\mathbb{C}^n$ such that $\mathbf{A}\mathbf{u}=\lambda\mathbf{u}$.\vspace{-.12cm}
\item[-] $\mathbf{u}$ is called an \textbf{eigenvector} of $\mathbf{A}$.\vspace{-.12cm}
\item[-] The set of all the eigenvalues of $\mathbf{A}$, denoted by $\mathrm{Sp}(\mathbf{A})$, is the \textbf{spectrum} of $\mathbf{A}$.
\end{itemize}
\end{definition}
\item Equivalently, $\lambda\in\mathbb{C}$ is an eigenvalue of $\mathbf{A}\in\mathbb{F}^{n\times n}$ if it is a root of the characteristic polynomial:
\begin{align*}
p_{\mathbf{A}}:\lambda\in\mathbb{C}\mapsto\det(\mathbf{A}-\lambda\mathbf{I}_n)\in\mathbb{F}.
\end{align*}
\vspace{-.5cm}
\begin{block}{Definition (Left eigenvectors)}
\begin{itemize}
\item[-] If $\lambda$ is an eigenvalue of $\mathbf{A}$, then $\overline{\lambda}$ is an eigenvalue of $\mathbf{A}^H$.\vspace{-.12cm}
\item[-] A vector $\mathbf{v}$ such that $\mathbf{A}^H\mathbf{v}=\overline{\lambda}\mathbf{v}$ is called a \textbf{left eigenvector} of $\mathbf{A}$.
\end{itemize}
\end{block}
\end{itemize}
\end{frame}

% Slide 35
\begin{frame}{Multiplicity of eigenvalues}
\begin{itemize}
\item Eigenvalues can have two types of multiplicities: \textbf{algebraic} and \textbf{geometric}.
\item \textbf{Algebraic multiplicity}: 
The number of times an eigenvalue appears as a root of the characteristic polynomial.
\item \textbf{Geometric multiplicity}: 
The dimension of the eigenspace corresponding to the eigenvalue, i.e., the number of linearly independent eigenvectors associated with that eigenvalue.
\begin{align*}
1\leq\text{ geometric multiplicity }\leq\text{ algebraic multiplicty}
\end{align*}
\item An eigenvalue is \textbf{semisimple} if its \textbf{geometric mult}. $\!=$ its \textbf{algebraic mult}..$\hspace{-1cm}$
\item An eigenvalue with \textbf{geometric mult}. $<$ \textbf{algebraic mult}. has fewer independent eigenvectors than the size of the eigenspace implied by the algebraic multiplicity.
\item If a matrix has at least one eigenvalue with \textbf{geometric mult}. $<$ \textbf{algebraic mult}., then it is \textbf{defective}, i.e., it does not have a full set of linearly independent eigenvectors.
\item $\hspace{3.9cm}$\textbf{defective} $\;\;\;\not\!\!\!\!\!\iff$ \textbf{singular}
\end{itemize}
\end{frame}

% Slide 36
\begin{frame}{Characterization of normal matrices}
\begin{itemize}
\item A square matrix $\mathbf{A}\in\mathbb{F}^{n \times n}$ is called \textbf{normal} if it satisfies:\vspace{-.05cm}
\begin{align*}
\mathbf{A} \mathbf{A}^H = \mathbf{A}^H \mathbf{A}.
\end{align*}
\vspace{-.55cm}\\
\item Common examples of normal matrices are
\begin{itemize}
\item[-] Diagonal matrices, i.e., $\mathbf{A}=\mathrm{diag}(a_{11},\dots,a_{nn})$.\vspace{.05cm}
\item[-] Hermitian matrices, i.e., $\mathbf{A}\in\mathbb{F}^{n\times n}$ such that  $\mathbf{A}^H = \mathbf{A}$.\vspace{.05cm}
\item[-] Symmetric matrices, i.e., $\mathbf{A}\in\mathbb{R}^{n\times n}$ such that  $\mathbf{A}^T = \mathbf{A}$.\vspace{.05cm}
\item[-] Unitary matrices, i.e., $\mathbf{A}\in\mathbb{F}^{n\times n}$ such that $\mathbf{A} \mathbf{A}^H=\mathbf{A} ^H\mathbf{A} = \mathbf{I}_n$.\vspace{.05cm}
\item[-] Skew-Hermitian matrices, i.e.,  $\mathbf{A}\in\mathbb{F}^{n\times n}$ such that $\mathbf{A}^H = -\mathbf{A}$.\vspace{.05cm}
\item[-] Skew-symmetric matrices, i.e.,  $\mathbf{A}\in\mathbb{R}^{n\times n}$ such that $\mathbf{A}^T = -\mathbf{A}$.
\end{itemize}
\begin{theorem}
A normal triangular matrix must be diagonal.
\end{theorem}
\vspace{-.075cm}
\begin{theorem}[Spectral characterization of normal matrices]
A matrix $\mathbf{A}\in\mathbb{F}^{n \times n}$ is normal if and only if each of its eigenvectors is also an eigenvector of $\mathbf{A}^H$.
That is, $\mathbf{A}$ is normal if and only if for every eigen-pair $(\lambda,\mathbf{u})\in\mathbb{C}\times\mathbb{C}^n$ such that $\mathbf{A}\mathbf{u}=\lambda\mathbf{u}$, we have $\mathbf{A}^H\mathbf{u}=\overline{\lambda}\mathbf{u}$.
\end{theorem}
\end{itemize}
\end{frame}

% Slide 37
\begin{frame}{Characterization of Hermitian matrices}
\begin{itemize}
\item A square matrix $\mathbf{A}\in\mathbb{F}^{n \times n}$ is called \textbf{Hermitian} if it satisfies $\mathbf{A}^H=\mathbf{A}$.
\begin{theorem}
\begin{itemize}
\item[-] The eigenvalues of a Hermitian matrix are real.\vspace{-.1cm}
\item[-] A normal matrix whose eigenvalues are real is Hermitian.
\end{itemize}
\end{theorem}
\item The ordered eigenvalues $\lambda_1\geq\dots\geq\lambda_n$ of a Hermitian matrix $\mathbf{A}\in\mathbb{F}^{n \times n}$ are characterized by optimality properties of the \textbf{Rayleigh quotient}\vspace{-.02cm}
\begin{align*}
\mu_{\mathbf{A}}:\mathbf{x}\in\mathbb{C}^{n}\!\setminus\!\{\mathbf{0}\}\mapsto\frac{\mathbf{x}^H\mathbf{A}\mathbf{x}}{\mathbf{x}^H\mathbf{x}}\in\mathbb{R}.
\end{align*}
\vspace{-.35cm}
\begin{theorem}[Courant-Fisher min-max principle]
The ordered eigenvalues of a Hermitian matrix $\mathbf{A}\in\mathbb{F}^{n \times n}$ are such that\vspace{-.35cm}
\begin{align*}
\lambda_k
=&\;
\underset{\mathcal{S}\,\subseteq\,\mathbb{C}^n,\,\dim(\mathcal{S})=n-k+1}{\min}
\hspace{.3cm}
\underset{\mathbf{x}\in\mathcal{S}\setminus\{\mathbf{0}\}}{\max}
\hspace{.3cm}
\mu_{\mathbf{A}}(\mathbf{x})\\
=&\;
\underset{\mathcal{S}\,\subseteq\,\mathbb{C}^n,\,\dim(\mathcal{S})=k}{\min}
\hspace{.3cm}
\underset{\mathbf{x}\in\mathcal{S}\setminus\{\mathbf{0}\}}{\max}
\hspace{.3cm}
\mu_{\mathbf{A}}(\mathbf{x}).
\end{align*}
\end{theorem}
\end{itemize}
\end{frame}

% Slide 38
\begin{frame}{Characterization of Hermitian matrices, cont'd}
\begin{itemize}
\item A corrolary of the (Courant-Fisher) min-max principle is that the largest and smallest eigenvalues of a Hermitian matrix $\mathbf{A}\in\mathbb{F}^{n\times n}$ are such that
\begin{align*}
\lambda_1=\underset{\mathbf{x}\,\in\,\mathbb{C}^n\setminus{\{\mathbf{0}\}}}{\max}\mu_{\mathbf{A}}(\mathbf{x})
\;\;\text{ and }\;\;
\lambda_n=\underset{\mathbf{x}\,\in\,\mathbb{C}^n\setminus{\{\mathbf{0}\}}}{\min}\mu_{\mathbf{A}}(\mathbf{x}).
\end{align*}
\item Another way to characterize the optimality of the ordered eigenvalues $\lambda_1\geq\dots\geq\lambda_n$ of a Hermitian matrix is
\begin{theorem}[Courant characterization]
The largest eigenvalue $\lambda_1$ of a Hermitian matrix $\mathbf{A}$ 
with a corresponding eigenvector $\mathbf{u}_1$ is such that\vspace{-.2cm}
\begin{align*}
\lambda_1=\mu_{\mathbf{A}}(\mathbf{u}_1)=
\underset{\mathbf{x}\in\mathbb{C}^n\setminus\{\mathbf{0}\}}{\max}
\hspace{.2cm}
\mu_{\mathbf{A}}(\mathbf{x})
\end{align*}
and the other eigenvalues $\lambda_2\geq\dots\geq\lambda_n$, with corresponding eigenvectors, are such that\vspace{-.2cm}
\begin{align*}
\lambda_k=\mu_{\mathbf{A}}(\mathbf{u}_k)=
\underset{\mathbf{x}\,\in\,\mathrm{range}([\mathbf{u}_1,\dots,\mathbf{u}_{k-1}])^\perp\setminus\{\mathbf{0}\}}{\max}
\hspace{.2cm}
\mu_{\mathbf{A}}(\mathbf{x}).
\end{align*}
\end{theorem}
\end{itemize}
\end{frame}

% Slide 39
\begin{frame}{$\hspace{-.15cm}$Characterization of Hermitian (semi)positive-definite matrices$\hspace{-1cm}$}
\begin{itemize}
\item A matrix $\mathbf{A}\in\mathbb{F}^{n\times n}$ is \textbf{Hermitian semipositive-definite} if $\mathbf{A}^H=\mathbf{A}$ and\vspace{-.1cm}
\begin{align}\label{eq:pd}
\mathbf{x}^H\mathbf{A}\mathbf{x}\geq 0\;\forall\;\mathbf{x}\in\mathbb{F}^n\setminus\{\mathbf{0}\}.
\end{align}
$\vspace{-.65cm}$\\
The \textbf{eigenvalues} of such matrices are \textbf{non-negative}, i.e., $\lambda_1\!\geq\!\dots\!\geq\!\lambda_n\!\geq\!0.\hspace{-1cm}$
\item If Eq.~\eqref{eq:pd} is \textbf{strictly satisfied}, the matrix is \textbf{Hermitian positive-definite}, or \textbf{symmetric positive-definite}, i.e., \textbf{SPD}, if $\mathbb{F}=\mathbb{R}$.\vspace{.05cm}\\
The \textbf{eigenvalues} of such matrices are \textbf{positive}, i.e., $\lambda_1\!\geq\!\dots\!\geq\!\lambda_n\!>\!0.\hspace{-1cm}$
%\item For any matrix $\mathbf{A}\in\mathbb{F}^{m\times n}$, the matrix of inner and outer products, namely $\mathbf{A}^T\mathbf{A}$ and $\mathbf{A}\mathbf{A}^T$, are always Hermitian semipositive-definite, e.g., SVD.
\item Hermitian positive definite matrices are \textbf{invertible}.\vspace{.05cm}\\
They admit \textbf{Cholesky decompositions} of the form $\mathbf{A}=\mathbf{L}\mathbf{L}^H$ where $\mathbf{L}$ is lower-triangular.
\item Hermitian (semi)positive-definite matrices are common in practice:$\hspace{-1cm}$
\begin{itemize}
\item[-] \textbf{Low-rank approximation}: $\mathbf{A}^H\mathbf{A}$ and $\mathbf{A}\mathbf{A}^H$.\vspace{.07cm}
\item[-] \textbf{Optimization}: Hessian matrices.\vspace{.07cm}
\item[-] \textbf{Machine learning}: Covariance matrices, kernel matrices.\vspace{.07cm}
\item[-] \textbf{Computational physics}: Discretizatized PDEs.\vspace{.07cm}
\item[-] \textbf{Statistics}: Fisher information matrices.
\end{itemize}
\end{itemize}
\end{frame}

\section{Matrix canonical forms\\{\small Section 2.4 in Darve \& Wootters (2021)}}

% Slide 40
\begin{frame}{Similarity transformations}
\begin{itemize}
\item Similarity is an equivalence relation between linear maps$\!$ induced$\!$ by$\!$ a$\!$ change$\hspace{-1cm}$\\
of basis, that preserves key properties of the underlying linear operator.$\vspace{-.1cm}$
\begin{definition}[Similar matrices]
Two square matrices $\mathbf{A}, \mathbf{B} \in \mathbb{F}^{n \times n}$ are \textbf{similar} if there exists an invertible matrix $\mathbf{X}\in\mathbb{F}^{n\times n}$ such that $\mathbf{A} = \mathbf{X}\mathbf{B}\mathbf{X}^{-1}$.
\end{definition}
\item The mapping $\mathbf{A}\mapsto\mathbf{B}$ is a \textbf{similarity transformation}, which recasts the matrix representation of a linear map in a different basis.
\item Similar matrices have the same \textbf{rank}, \textbf{characteristic polynomial} and underlying properties (i.e., \textbf{determinant}, \textbf{eigenvalues} and their \textbf{algebraic multiplicities}, \textbf{trace}, ...), \textbf{geometric multiplicities}, ...
\item An eigenvector $\mathbf{v}$ of $\mathbf{B}$ is transformed into an eigenvector $\mathbf{u}:=\mathbf{X}\mathbf{v}$ of the similar $\mathbf{A}=\mathbf{X}\mathbf{B}\mathbf{X}^{-1}$.
\item Similarity is crucial for finding canonical forms, such as the \textbf{diagonal form}, the \textbf{Jordan form} and the \textbf{Schur form}, which provide simplified representations useful in solving and analyzing numerical linear algebraic problems.
\end{itemize}
\end{frame}

% Slide 41
\begin{frame}{Diagonal form and eigen-decomposition}
\begin{itemize}
\item The simplest similar form into which a matrix may be reduced is the diagonal form, i.e., $\mathbf{A}=\mathbf{X}\mathbf{D}\mathbf{X}^{-1}$, for some diagonal matrix $\mathbf{D}$.
%Such matrices are called \textbf{diagonalizable}.
\begin{theorem}
A square matrix $\mathbf{A}\in\mathbb{F}^{n\times n}$ is diagonalizable iff it has $n$ linearly independent eigenvectors or, equivalently, iff it is not defective.
\end{theorem}
\item In particular, every diagonalizable matrix $\mathbf{A}$ can be recast into an \textbf{eigen-decomposition} of the form $\mathbf{A}=\mathbf{U}\boldsymbol{\Lambda}\mathbf{U}^{-1}$, where $\boldsymbol{\Lambda}$ is a diagonal matrix of eigenvalues, and the columns of $\mathbf{U}$ are normalized eigenvectors.\vspace{.1cm}\\
If $\mathbf{A}$ is \textbf{normal}, then $\mathbf{U}^{-1}\!=\!\mathbf{U}^H\!\!$.
\textbf{All normal matrices are diagonalizable}.$\hspace{-1cm}$
\item An \textbf{invertible} matrix is \textbf{not necessarily diagonalizable}, e.g.:\vspace{.1cm}\\
$\hspace{3cm}\begin{bmatrix}1&1\\0&1\end{bmatrix}$ is invertible, but defective.\vspace{.12cm}
\item A \textbf{diagonalizable} matrix is \textbf{not necessarily invertible}, e.g.:\vspace{.1cm}\\
$\hspace{3.15cm}\begin{bmatrix}1&0\\0&0\end{bmatrix}$ is diagonal, but singular.\vspace{.12cm}
\end{itemize}
\end{frame}

% Slide 42
\begin{frame}{Jordan form}
\vspace{-.1cm}
\begin{itemize}
\item For defective matrices, an alternative representation is the \textbf{Jordan form}.\vspace{-.12cm}
\begin{definition}[Jordan block]
A Jordan block is either a scalar $\lambda$, or a matrix of the form:\vspace{-.25cm}
{\small
\begin{align*}
\mathbf{J} = 
\begin{bmatrix}
\lambda & 1 & 0 & \dots & 0 \\ 
0 & \lambda & 1 & \dots & 0 \\ 
\vdots & \vdots & \ddots & \ddots & 0 
\\ 0 & 0 & \dots & \lambda & 1 \\ 
0 & 0 & \dots & 0 & \lambda 
\end{bmatrix}.
\end{align*}}
\end{definition}
\vspace{-.12cm}
\begin{theorem}[Reduction to Jordan form]
\begin{itemize}
\item[-] For every matrix $\mathbf{A}\in\mathbb{F}^{n\times n}$, there exist $\mathbf{X},\mathbf{J}\in\mathbb{C}^{n\times n}$ such that $\mathbf{A}=\mathbf{X}\mathbf{J}\mathbf{X}^{-1}\!,\hspace{-1cm}$\\ 
where $\mathbf{J}$ is a block-diagonal matrix of Jordan blocks, i.e., $\mathbf{J}=\mathrm{diag}(\mathbf{J}_1,\dots,\mathbf{J}_k)$.\vspace{-.2cm}
\item[-] For each distinct eigenvalue $\lambda$ of $\mathbf{A}$, there is a number of associated Jordan blocks in $\mathbf{J}$ given by the geometric multiplicity of $\lambda$, whereas the sizes of these blocks add up to the algebraic multiplicity of $\lambda$.
\end{itemize}
\end{theorem}
\vspace{-.12cm}
\item The Jordan form is a generalization of the eigen-decomposition, mostly used for the analysis of defective matrices.
\end{itemize}
\end{frame}

% Slide 43
\begin{frame}{Schur form/decomposition}
\begin{itemize}
\item A Jordan form exists for every square matrix, but its computation can be challenging, and the associated basis ill-conditioned.
\item An alternative to the Jordan form, which also exists for every square matrix, is the \textbf{Schur form} (or \textbf{decomposition}).
\begin{theorem}[Schur decomposition]
For every matrix $\mathbf{A}\in\mathbb{F}^{n\times n}$, there exist $\mathbf{Q},\mathbf{R}\in\mathbb{C}^{n\times n}$ such that
\begin{align*}
\mathbf{A}=\mathbf{Q}\mathbf{R}\mathbf{Q}^H
\end{align*}
where $\mathbf{R}$ is upper triangular, and $\mathbf{Q}$ is unitary, i.e., $\mathbf{Q}^{-1}=\mathbf{Q}^H$.
\end{theorem}
\item Since $\mathbf{R}$ is a triangular matrix, its eigenvalues are listed on the diagonal.
And since $\mathbf{R}$ and $\mathbf{A}$ are similar, the values listed on the diagonal of $\mathbf{R}$ are also the eigenvalues of $\mathbf{A}$.
\end{itemize}
\end{frame}

% Slide 44
\begin{frame}{}
\vspace{.7cm}
\begin{figure}
\includegraphics[height=6cm]{images/Canonical-forms2.jpg}
\vfill
\tiny{Darve, E., \& Wootters, M. (2021). Numerical linear algebra with Julia. Society for Industrial and Applied Mathematics.}
\end{figure}
\end{frame}

% Slide 45
\begin{frame}{Matrix functions}
\begin{itemize}
\item A \textbf{matrix function} $f(\mathbf{A})$ extends scalar functions like $e^x$, $sin(x)$, or $\log(x)$ to matrices $\mathbf{A}\in\mathbb{F}^{n\times n}$.
%\item One way to define matrix functions is via Taylor series.
\item The previously introduced canonical forms can be used to define the application of matrix functions.
In particular, for\vspace{.1cm}
\begin{itemize}
\item[-] \textbf{Diagonalizable matrices}: If $\mathbf{A}=\mathbf{U}\mathbf{D}\mathbf{U}^{-1}$ where $\mathbf{D}$ is diagonal, then\vspace{-.1cm}
\begin{align*}
f(\mathbf{A})=\mathbf{U}f(\mathbf{D})\mathbf{U}^{-1}
\;\;\text{ where }\;\;
f(\mathbf{D})=\mathrm{diag}(f(d_{11}),\dots,f(d_{nn})).
\end{align*}
\item[-] \textbf{Jordan forms}: If $\mathbf{A}=\mathbf{X}\mathbf{J}\mathbf{X}^{-1}$ where $\mathbf{J}$ is a Jordan matrix, then\vspace{-.1cm}
\begin{align*}
f(\mathbf{A})=\mathbf{X}f(\mathbf{J})\mathbf{X}^{-1}
\end{align*}
\vspace{-.6cm}\\
where $f(\mathbf{J})$ is block diagonal, with each block corresponding to a Jordan block. 
Computing functions of Jordan blocks requires handling the non-diagonal terms via derivatives of the scalar function.\vspace{.1cm}
\item[-] \textbf{Schur form}: If $\mathbf{A}=\mathbf{Q}\mathbf{R}\mathbf{Q}^H$ where $\mathbf{R}$ is upper triangular, then:\vspace{-.1cm}
\begin{align*}
f(\mathbf{A}) = \mathbf{Q} f(\mathbf{R}) \mathbf{Q}^H
\end{align*}
\vspace{-.6cm}\\
For upper triangular matrices, matrix functions can be computed recursively based on the entries of $\mathbf{R}$.
\end{itemize}
\end{itemize}
\end{frame}

\section{Singular value decomposition\\{\small Section 2.5 in Darve \& Wootters (2021)}}

% Slide 46
\begin{frame}{Singular value decomposition}
\begin{itemize}
\item \textbf{Eigenvalues} and related \textbf{canonical forms} are \textbf{for square matrices only}.\vspace{.06cm}\\
But \textbf{singular value decompositions} exist \textbf{for all matrices}.
\begin{theorem}[Singular value decomposition (SVD)]
\begin{itemize}
\item[-] For every matrix $\mathbf{A}\in\mathbb{F}^{m\times n}$, there exist decompositions of the form\vspace{-.2cm}
\begin{align*}
\mathbf{A}=\mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^H
\end{align*}
\vspace{-.7cm}\\
where $\mathbf{U}\in\mathbb{F}^{m\times m}$ and $\mathbf{V}\in\mathbb{F}^{n\times n}$ are unitary matrices, and the only non-zero entries of $\boldsymbol{\Sigma}\in\mathbb{R}^{m\times n}$ are on the diagonal.\vspace{-.12cm}
\item[-] The diagonal entries $\sigma_1\geq\dots\geq\sigma_p\geq 0$ of $\boldsymbol{\Sigma}$ are called \textbf{singular values}.\vspace{-.12cm}
\item[-] The columns $\mathbf{u}_1,\dots,\mathbf{u}_m\in\mathbb{F}^m$ of $\,\mathbf{U}$ are called \textbf{left singular vectors}.\vspace{-.12cm}
\item[-] The columns $\mathbf{v}_1,\dots,\mathbf{v}_n\in\mathbb{F}^v$ of $\,\mathbf{V}$ are called \textbf{right singular vectors}.
\end{itemize}
\end{theorem}
\item The \textbf{singular values} $\sigma_1\geq\dots\geq\sigma_p\geq 0$ are \textbf{unique}, 
but the matrices $\mathbf{U}$ and $\mathbf{V}$ of left and right singular vectors are not unique.
\item If $\mathbf{A}\in\mathbb{F}^{m\times n}$ has rank $r<p$ where $p:=\min(m,n)$, then $\sigma_{k}=0$ for $k=r+1,\dots,p$. 
Thus, the number of \textbf{non-zero singular values} equals the \textbf{rank} of $\mathbf{A}$. 
\end{itemize}
\end{frame}

% Slide 47
\begin{frame}{Relation between singular and eigenvalue problems}
\begin{itemize}
\item If $\mathbf{A}\in\mathbb{F}^{n\times n}$ is \textbf{Hermitian} with (real) eigenvalues ordered such that $|\lambda_1|\geq\dots\geq|\lambda_n|$, then the ordered singular values $\sigma_1\geq\dots\geq\sigma_n$ of $\mathbf{A}$ are given by $\sigma_i=|\lambda_i|$ for $i=1,\dots,n$.
\item If  $\mathbf{A}\in\mathbb{F}^{m\times n}$ is not Hermitian, or even square, then the singular values and singular vectors of $\mathbf{A}$ can be characterized as eigen-pairs of Gram matrices. 
\begin{theorem}[Relation between the SVD and Gram matrices]
For every $\mathbf{A}\in\mathbb{F}^{m\times n}$ with $p:=\min(m,n)$ singular values $\sigma_1\geq\dots\geq\sigma_p\geq 0$ 
and a proper SVD $\mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^H$ with $\mathbf{U}=[\mathbf{u}_1,\dots,\mathbf{u}_n]$ and $\mathbf{V}=[\mathbf{v}_1,\dots,\mathbf{v}_m]$,
\vspace{-.1cm}
\begin{itemize}
\item[-] $(\sigma_1,\mathbf{u}_1),\dots,(\sigma_{p},\mathbf{u}_{p})\in\mathbb{R}\times\mathbb{F}^n$ are eigen-pairs of $\mathbf{A}^H\mathbf{A}\in\mathbb{F}^{n\times n}$,\vspace{-.1cm}
\item[-] $(\sigma_1,\mathbf{v}_1),\dots,(\sigma_{p},\mathbf{v}_{p})\in\mathbb{R}\times\mathbb{F}^m$ are eigen-pairs of $\mathbf{A}\mathbf{A}^H\in\mathbb{F}^{m\times m}$.
\end{itemize}
\end{theorem}
\vspace{.15cm}
\item 
\begin{minipage}[t]{.29\textwidth}
As a corollary, we have:
\end{minipage}
\begin{minipage}[t]{.5\textwidth}
\vspace{-.9cm}
\begin{align*}
\hspace{-.8cm}\|\mathbf{A}\|_2=&\,\sigma_1(\mathbf{A})=\sqrt{\lambda_{max}(\mathbf{A}^{\!H}\!\mathbf{A})}=\sqrt{\lambda_{max}(\mathbf{A}\mathbf{A}^{\!H})}\\
\hspace{-.8cm}\text{and }\,\|\mathbf{A}\|_F=&\,\sqrt{\sum_{i=1}^{p}\sigma_i^2}.
\end{align*}
\end{minipage}
\end{itemize}
\end{frame}

% Slide 48
\begin{frame}{Compact singular value decomposition}
\begin{itemize}
\item For rectangular matrices, we denote two types of structures of SVD:\vspace{.1cm}\\
\begin{minipage}[t]{.44\textwidth}
$\hspace{-.3cm}$Tall-and-skinny matrices ($m>n$)\vspace{-.55cm}\\
\begin{figure}
$\hspace*{-.6cm}$\includegraphics[height=2.2cm]{images/SVD-full-tall-and-skinny.jpg}
\end{figure}
\end{minipage}
\begin{minipage}[t]{.44\textwidth}
$\hspace{.4cm}$Short-and-fat matrices ($m<n$)\vspace{-.9cm}\\
\begin{figure}
$\hspace*{-.3cm}$\includegraphics[height=2.2cm]{images/SVD-full-short-and-fat.jpg}
\end{figure}
\end{minipage}
\item Due to the structure of $\boldsymbol{\Sigma}$, a \textbf{compact SVD} $\mathbf{A}=\hat{\mathbf{U}}\hat{\boldsymbol{\Sigma}}\hat{\mathbf{V}}^H$ is formed by discarding the zero block of $\boldsymbol{\Sigma}$, and the corresponding blocks of singular vectors in $\mathbf{U}$ or $\mathbf{V}$, leading to $\hat{\mathbf{U}}\in\mathbb{F}^{m\times p}$, $\hat{\boldsymbol{\Sigma}}\in\mathbb{R}^{p\times p}$ and $\hat{\mathbf{V}}\in\mathbb{F}^{n\times p}$ :\vspace{.15cm}\\
\begin{minipage}[t]{.44\textwidth}
$\hspace{-.3cm}$Tall-and-skinny matrices ($m>n$)\vspace{-.45cm}\\
\begin{figure}
$\hspace*{-.6cm}$\includegraphics[height=2.2cm]{images/SVD-compact-tall-and-skinny.jpg}
\end{figure}
\end{minipage}
\begin{minipage}[t]{.44\textwidth}
$\hspace{.4cm}$Short-and-fat matrices ($m<n$)\vspace{-.87cm}\\
\begin{figure}
$\hspace*{-.3cm}$\includegraphics[height=1.5cm]{images/SVD-compact-short-and-fat.jpg}
\end{figure}
\end{minipage}
\vfill
\tiny{Darve, E., \& Wootters, M. (2021). Numerical linear algebra with Julia. Society for Industrial and Applied Mathematics.}
\end{itemize}
\end{frame}

% Slide 49
\begin{frame}{Low-rank approximation}
\begin{itemize}
\item The$\!$ factors of$\!$ th SVD$\!$ capture essential$\!$ information about the action of $\mathbf{A}:\hspace{-1cm}$
\begin{theorem}[Four fundamental subspaces and the SVD]
Every matrix $\mathbf{A}\in\mathbb{F}^{m\times n}$ of rank $r$ with left singular vectors $\mathbf{u}_1,\dots,\mathbf{u}_n\in\mathbb{F}^n$, and right singular vectors $\mathbf{v}_1,\dots,\mathbf{v}_m\in\mathbb{F}^m$ is such that\vspace{-.25cm}
\begin{align*}
\mathrm{range}(\mathbf{A})=&\,\mathrm{span}\{\mathbf{u}_1,\dots,\mathbf{u}_r\},
\hspace{1.27cm}
\mathrm{null}(\mathbf{A})=\mathrm{span}\{\mathbf{v}_{r+1},\dots,\mathbf{v}_n\},\\
\mathrm{range}(\mathbf{A}^H)=&\,\mathrm{span}\{\mathbf{v}_1,\dots,\mathbf{v}_r\},
\hspace{1cm}
\mathrm{null}(\mathbf{A}^H)=\mathrm{span}\{\mathbf{u}_{r+1},\dots,\mathbf{u}_m\}.
\end{align*}
\end{theorem}
%\item Something else.
\vspace{-.1cm}
\begin{theorem}[Eckart-Young theorem]
Consider $\mathbf{A}\in\mathbb{F}^{m\times n}$, its $p=\min(m,n)$ singular values $\sigma_1\geq\dots\geq\sigma_1\geq 0$ and corresponding left singular vectors $\mathbf{u}_1,\dots,\mathbf{u}_{\min(p,m)}$ and right singular vectors  $\mathbf{v}_1,\dots,\mathbf{v}_{\min(p,n)}$.
Then, for $r<p$, the matrix $\mathbf{B}:=\sum_{i=1}^r\sigma_i\mathbf{u}_i\mathbf{v}_i^H$ is such that\vspace{-.6cm}
\begin{align*}
\|\mathbf{A}-\mathbf{B}\|_2=\sigma_{r+1}
\;\text{ and }\;
\|\mathbf{A}-\mathbf{B}\|_F=\sum_{i=r+1}^p\sigma_i^2.
\end{align*}
\vspace{-.4cm}\\
Moreover, $\mathbf{B}$ minimizes $\|\mathbf{A}-\mathbf{B}\|_2$ and $\|\mathbf{A}-\mathbf{B}\|_F$ among matrices of rank $r$.
\end{theorem}
\end{itemize}
\end{frame}

\section{Homework problems}
% Slide 50
\begin{frame}{Homework problems}
Turn in \textbf{your own} solution to \textbf{Pb.$\,$5}:\vspace{.08cm}\\
\begin{minipage}[t]{0.1\textwidth}
\textbf{Pb.$\,$1}
\end{minipage}
\begin{minipage}[t]{0.89\textwidth}
Let $\mathbf{A}\in\mathbb{R}^{n\times n}$ be symmetric positive-definite. 
Show that\vspace{-.3cm}
\begin{align*}
(\cdot,\cdot)_{\mathbf{A}}:
(\mathbf{x},\mathbf{y})\in\mathbb{R}^n\times\mathbb{R}^n
\mapsto
(\mathbf{x},\mathbf{y})_{\mathbf{A}}:=\mathbf{x}^T\mathbf{A}\mathbf{y}
\end{align*}
$\vspace{-.65cm}$\\
is an inner-product on $\mathbb{R}^n$, and $\|\cdot\|_{\mathbf{A}}:=(\cdot,\cdot)_{\mathbf{A}}^{1/2}$ is a norm.
\end{minipage}\vspace{.08cm}\\
\begin{minipage}[t]{0.1\textwidth}
\textbf{Pb.$\,$2}
\end{minipage}
\begin{minipage}[t]{0.89\textwidth}
Show that $\mathbf{A}\in\mathbb{R}^{m\times n}$ has rank 1 if and only if there exist non-zero vectors $\mathbf{u}\in\mathbb{R}^m$ and $\mathbf{v}\in\mathbb{R}^n$ such that $\mathbf{A}=\mathbf{u}\mathbf{v}^T$.
\end{minipage}\vspace{.08cm}\\
\begin{minipage}[t]{0.1\textwidth}
\textbf{Pb.$\,$3}
\end{minipage}
\begin{minipage}[t]{0.89\textwidth}
Show that $\|\mathbf{x}\mathbf{y}^T\|_2=\|\mathbf{x}\|_2\cdot\|\mathbf{y}\|_2\;\forall\;\mathbf{x},\mathbf{y}\in\mathbb{R}^n$.
\end{minipage}\vspace{.08cm}\\
\begin{minipage}[t]{0.1\textwidth}
\textbf{Pb.$\,$4}
\end{minipage}
\begin{minipage}[t]{0.89\textwidth}
Determine the orthogonal projector $\mathbf{P}$ onto the subspace spanned by a non-zero vector $\mathbf{w}\in\mathbb{R}^n$.
\end{minipage}\vspace{.08cm}\\
\begin{minipage}[t]{0.1\textwidth}
\textbf{Pb.$\,$5}
\end{minipage}
\begin{minipage}[t]{0.89\textwidth}
Let $\mathbf{A}\in\mathbb{R}^{m\times n}$ have $p\leq\min(m,n)$ non-zero singular values $\sigma_1\geq\dots\geq\sigma_p>0$ with corresponding $\mathbf{U}:=[\mathbf{u}_1,\dots,\mathbf{u}_p]$ and $\mathbf{V}:=[\mathbf{v}_1,\dots,\mathbf{v}_p]$ as left and right singular vectors.
Show that
\begin{itemize}
\item[a.]$\!\mathbf{A}^\dagger\!:=\mathbf{V}\boldsymbol{\Sigma}^{-1}\mathbf{U}^T\!$ is the pseudo-inverse of $\mathbf{A}$ where\\
$\boldsymbol{\Sigma}:=\mathrm{diag}(\sigma_1\dots,\sigma_p)$.\vspace{-.08cm}
\item[b.]$\mathbf{P}:=\mathbf{A}\mathbf{A}^\dagger$ is an orthogonal projector onto $\mathrm{range}(\mathbf{A})$.\vspace{-.08cm}
\item[c.]$\mathbf{P}:=\mathbf{I}_n-\mathbf{A}^\dagger\mathbf{A}$ is an orthogonal projector onto $\mathrm{null}(\mathbf{A})$.
\end{itemize}
\end{minipage}
\end{frame}

\end{document}
