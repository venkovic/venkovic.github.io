\documentclass[t,usepdftitle=false]{beamer}

\input{../../../tex-beamer-custom/preamble.tex}

\title[NLA for CS and IE -- Lecture 10]{Numerical Linear Algebra\\for Computational Science and Information Engineering}
\subtitle{\vspace{.3cm}Lecture 10\\Locally Optimal Block Preconditioned Conjugate Gradient\vspace{-.47cm}}
\hypersetup{pdftitle={NLA-for-CS-and-IE\_Lecture10}}

\date[Summer 2025]{Summer 2025}

\author[nicolas.venkovic@tum.de]{Nicolas Venkovic\\{\small nicolas.venkovic@tum.de}}
\institute[]{Group of Computational Mathematics\\School of Computation, Information and Technology\\Technical University of Munich}

\titlegraphic{\vspace{0cm}\includegraphics[height=1.1cm]{../../../logos/TUM-logo.png}}

\begin{document}
	
\begin{frame}[noframenumbering, plain]
	\maketitle
\end{frame}
	
\myoutlineframe
	
\section{Methods based on optimization}	
	
% Slide 01
\begin{frame}{Extremal generalized eigenvalue problem}
\begin{itemize}
\item \textbf{Generalized eigenvalue problem}: 
\begin{align*}
\text{Find }
(x,\lambda)
\text{ such that }
Ax = \lambda Bx
\end{align*}
\vspace{-.5cm}
\begin{itemize}\normalsize
\item[-] $A$ is symmetric (or Hermitian)\vspace{.07cm}
\item[-] $B$ is symmetric positive definite (SPD)\vspace{.07cm}
\item[-] Applications: structural dynamics, quantum mechanics, data analysis\vspace{.07cm}
\end{itemize}
\item \textbf{Challenges}:
\begin{itemize}\normalsize
\item[-] Computing a few extremal eigenvalues of large sparse matrices\vspace{.07cm}
\item[-] Need for memory-efficient iterative methods with fast convergence\vspace{.07cm}
\item[-] Handling ill-conditioned problems effectively\vspace{.07cm}
\end{itemize}
\item \textbf{Characterization of the extremal generalized eigenpair}:\\
We are looking for an extremal (min or max) generalized eigenpair $(\lambda,x)$.\vspace{.07cm}\\
Since $B$ is SPD, it admits a Cholesky decomposition $B=LL^T$.\vspace{.07cm}\\
Let the generalized Rayleigh quotient of $(A,B)$ be given by
\begin{align*}
\rho(x) = \frac{x^TAx}{x^TBx} \quad \text{for} \quad x^TBx > 0
\end{align*}
\end{itemize}
\end{frame}	
	
% Slide 02
\begin{frame}{Characterization of the extremal generalized eigenpair}
\begin{itemize}
\item[] Making the substitution $y:=L^Tx$, the generalized Rayleigh quotient becomes the standard Rayleigh quotient of $L^{-1}AL^{-T}$:
\begin{align*}
\rho = \frac{x^TAx}{x^TBx} = \frac{(L^{-T}y)^TA(L^{-T}y)}{(L^{-T}y)^TB(L^{-T}y)} = \frac{y^TL^{-1}AL^{-T}y}{y^Ty}
\end{align*}
Since $L^{-1}AL^{-T}$ is symmetric, the Courant-Fischer theorem implies that the extremum of $\rho$ is the extremal eigenvalue $\lambda$ of $L^{-1}AL^{-T}$.\vspace{.07cm}\\
Then, since we have\vspace{-.6cm}
\begin{align*}
L^{-1}AL^{-T}y=&\,\lambda y\\
L^{-1}AL^{-T}L^Tx=&\,\lambda L^Tx\\
Ax=&\, \lambda LL^Tx\\
Ax=&\,\lambda Bx
\end{align*}
the extremum value $\lambda$ of $\rho(x)$ is the extremal eigenvalue of the generalized eigenvalue problem $Ax = \lambda Bx$, achieved with the general eigenvector $x$.
\item Finding an extremal generalized eigen-pair of $(A,B)$ is equivalent to an optimization problem of the generalized Rayleigh quotient $\frac{x^TAx}{x^TBx}$.
\end{itemize}
\end{frame}	
	
% Slide 03
\begin{frame}{Approach by steepest descent}
\begin{itemize}
\item Approaches based on the optimization of the quotient $\rho(x)=\frac{x^TAx}{x^TBx}$ generate a sequence $x_0,x_1,\dots$ of approximate eigenvectors based on a given recurrence formula.
\item Note that the gradient of $\rho(x)$ is given by:
\begin{align*}
\nabla\rho(x) = \frac{2}{x^TBx}(Ax - \rho(x)Bx) = \frac{2}{x^TBx}r(x)\propto r(x)
\end{align*}
where $r(x) = Ax - \rho(x)Bx$ is the generalized eigen-residual.
\item Then, the steepest descent iteration is of the form
\begin{align*}
x_{i+1} = x_i - \alpha_i \nabla\rho(x_i)
\end{align*}
where $\alpha_i$ is a step size, optimally chosen to minimize $\rho(x_{i+1})$.\vspace{.07cm}\\
In other words, the iterate $x_{i+1}$ is searched in a subspace of $\mathrm{span}\{x_i,r_i\}$ where $r_i=Ax_i-\rho(x_i)Bx_i$.
\item In order to accelerate convergence, a preconditioner $T$ may be chosen and applied to $r_i$, in which case the next iterate is searched in $\mathrm{span}\{x_i,z_i\}$ where $z_i=Tr_i$.
\end{itemize}
\end{frame}
			
% Slide 04
\begin{frame}{From steepest descent to locally optimal conjugate gradient}
\begin{itemize}
\item \textbf{Limitations of steepest descent}:
\begin{itemize}\normalsize
\item[-] Slow convergence, especially for ill-conditioned problems
\item[-] Limited to 2-dimensional search space, i.e., $x_{i+1}\in\text{span}\{x_i, Tr_i\}$
\end{itemize}
\end{itemize}
\vspace{-.1cm}
\begin{block}{Locally optimal preconditioned conjugate gradient (LOPCG) method}
The recurrence formula for LOPCG is\vspace{-.4cm}
\begin{align*}
x_{i+1} = \alpha_i x_i + \beta_i p_i + \gamma_i Tr_i
\end{align*}
\vspace{-.8cm}\\
where $p_i$ is a search direction, $A$-conjugate to the previous direction $p_{i-1}$.
\begin{itemize}
\item LOPCG is called "Conjugate Gradient" because:
\begin{enumerate}\normalsize
\item It uses conjugate directions $p_i$ similar to the CG algorithm\vspace{.07cm}
%\item Direction $p_i$ is constructed to be $B$-conjugate to previous directions\vspace{.07cm}
\item It relies on a recurrence formula similar to that of the CG algorithm
\end{enumerate}
\item "Locally Optimal" refers to the fact that the search space of each iterate is composed of three directions, namely\vspace{-.3cm}
\begin{align*}
\text{span}\{x_i, p_i, Tr_i\}=\text{span}\{x_i,x_{i-1}, Tr_i\}
\end{align*}
\vspace{-.8cm}\\
\item The iterate $x_{i+1}$ is formed by Rayleigh Ritz projection to optimize $\rho(x_{i+1})$.
\end{itemize}
\end{block}
\tiny{Knyazev, A. V. (1991). A preconditioned conjugate gradient method for eigenvalue problems and its implementation in a subspace. In Numerical Treatment of Eigenvalue Problems Vol. 5/Numerische Behandlung von Eigenwertaufgaben Band 5: Workshop in Oberwolfach, February 25–March 3, 1990/Tagung in Oberwolfach, 25. Februar–3. März 1990 (pp. 143-154). Birkhäuser Basel.}
\end{frame}
	
% Slide 05
\begin{frame}{Rayleigh-Ritz projection of generalized eigenvalue problems}
\begin{itemize}
\item Rayleigh-Ritz projections are a means to find an optimal iterate in a given search space, e.g., $\mathrm{span}\{x_i,x_{i-1},Tr_i\}$ in the case of LOPCG.
\item Rayleigh-Ritz projection of the generalized eigenvalue problem in a search space $\text{range}(V)$ for some full rank $V\in\mathbb{R}^{n\times m}$ with $m\leq n$:
	\begin{align*}
	\text{Find }
	(x_{i+1},\lambda)\in\text{range}(V)\times \mathbb{R}
	\text{ s.t. }
	(Ax_{i+1}-\lambda Bx_{i+1})\perp\text{range}(V).
	\end{align*}
	\end{itemize}
	\vspace{-.25cm}
	\begin{block}{Rayleigh-Ritz procedure with respect to $\mathcal{R}(V)$}
		\vspace{-.3cm}
		\begin{align*}
			\hspace{-.5cm}
			\text{RR}:
			\mathbb{S}_*^{n}\times\mathbb{S}_{++}^{n}\times\mathbb{R}_*^{n\times m}&\;\rightarrow\mathbb{R}^{m}\times\mathbb{R}\\
			(A,B,V)&\;\mapsto (\hat{x},\lambda)
			\text{ s.t. }
			\boxed{V^TAV\hat{x}=V^TBV\hat{x}\,\lambda }
		\end{align*}
		where $\lambda$ is an extremal \textbf{eigenvalue}	
		of the projected problem with the corresponding eigenvector $\hat{x}.$\\
		\smallskip
		Then, the Rayleigh-Ritz vector is formed by $x_{i+1}:=V\hat{x}$.\\
		\smallskip
		By convention, $\hat{x}^TV^TBV\hat{x}=1$ so that $x_{i+1}^TBx_{i+1}=1$ and $x_{i+1}^TAx_{i+1}=\lambda$.
	\end{block}
\end{frame}		
	
% Slide 06
\begin{frame}{Early LOPCG iteration}
The early form of LOPCG iteration is given by:
\smallskip
\vspace{.1cm}
\begin{center}
$\hspace{-3cm}$Early\_LOPCG($A$, $B$, $x_{-1}$, $T^{-1}$):\tinyskip\\
\small
\fbox{\begin{varwidth}{10cm}
$(\hat{x}_0,\lambda_0)\mapsfrom\text{RR}(A,B,x_{-1})$\tinyskip\\
$x_0:=x_{-1}\hat{x}_0$ ; $r_{0}:=Ax_{0}-Bx_{0}\lambda_{0}\,$\tinyskip\\
\textbf{for} $i=0,1,\dots$ \textbf{do}\tinyskip\\
\hspace*{.4cm}$z_i:=Tr_i$\tinyskip\\
\hspace*{.4cm}\textbf{if} $i==0$ \textbf{then} $V_{i+1}:=[x_i,z_i]$ \textbf{else} $V_{i+1}:=[x_i,z_i,x_{i-1}]$\tinyskip\\
%\hspace*{.4cm}$Z_i\mapsfrom\texttt{Ortho}(Z_i,Q)$\\
\hspace*{.4cm}$(\hat{x}_{i+1},\lambda_{i+1})\mapsfrom\text{RR}(A,B,V_{i+1})$\tinyskip\\
\hspace*{.4cm}$x_{i+1}:=V_{i+1}\hat{x}_{i+1}$; $r_{i+1}:=Ax_{i+1}-Bx_{i+1}\lambda_{i+1}$
\end{varwidth}}\end{center}
\tiny{Knyazev, A. V. (1991). A preconditioned conjugate gradient method for eigenvalue problems and its implementation in a subspace. In Numerical Treatment of Eigenvalue Problems Vol. 5/Numerische Behandlung von Eigenwertaufgaben Band 5: Workshop in Oberwolfach, February 25–March 3, 1990/Tagung in Oberwolfach, 25. Februar–3. März 1990 (pp. 143-154). Birkhäuser Basel.}
\end{frame}
	
\section{Early development of LOBPCG iterations}
	
% Slide 07
\begin{frame}{Locally optimal block preconditioned conjugate gradient}
	\begin{itemize}
	\item We now search for $X\in\mathbb{R}^{n\times k}$ and $\Lambda=\mathrm{diag}(\lambda_1,\dots,\lambda_k)$ for $k\leq n$ s.t.\vspace{-.1cm}
	\begin{align*}AX=BX\Lambda\end{align*}
	\vspace{-.7cm}\\
	where $X^TBX=I_k$ and $\lambda_1,\dots,\lambda_k$ are extremal generalized eigenvalues of the pencil $(A,B)$.
	\item When $k\ll n$ and $n$ is very large  and/or the application of the operators $A$ and $B$ is matrix-free, it is customary to resort to iterative eigensolvers.\\
	\item The least dominant generalized eigenvectors in the columns of $X$ such that $AX=BX\Lambda$ can be defined as the minimizer of the $\text{trace}(X^TAX)$ subjected to the constraint $X^TBX=I_k$.
	\item Knyazev~(2001) introduced LOBPCG by extending LOPCG to a block version, which simultaneously produces iterates for the approximation of multiple dominant eigen-pairs.
	\item Given an initial iterate $X_0$, LOBPCG generates a sequence of iterates $X_1,X_2,\dots$ which, in their earliest form, are obtained by Rayleigh-Ritz projection in locally optimal subspaces $\text{range}([X_{i},TR_i,X_{i-1}])$, where the columns of $R_i$ are eigen-residuals, and $T$ is a preconditioner.
	\end{itemize}
	\medskip
	\tiny{Knyazev, A. V. (2001). Toward the optimal preconditioned eigensolver: Locally optimal block preconditioned conjugate gradient method. SIAM journal on scientific computing, 23(2), 517-541.}
\end{frame}

% Slide 08
\begin{frame}{Rayleigh-Ritz projection of generalized eigenvalue problems}
	\begin{itemize}
	\item Rayleigh-Ritz projection of the generalized eigenvalue problem in $\text{range}(V)$ for some full rank $V\in\mathbb{R}^{n\times m}_*$ with $m\leq n$:
	\begin{align*}
	\text{Find }
	(X_{i+1},\Lambda)\in\mathcal{R}(V)\times \mathbb{R}_*^{k\times k}
	\text{ s.t. }
	(AX_{i+1}-BX_{i+1}\Lambda)\perp\text{range}(V)
	\end{align*}
	where $k\leq m\leq n$ and $\Lambda$ is diagonal.
	\end{itemize}
	\vspace{-.05cm}
	\begin{block}{Rayleigh-Ritz procedure with respect to $\mathcal{R}(V)$}
		\vspace{-.3cm}
		\begin{align*}
			\hspace{-.5cm}
			\text{RR}:
			\mathbb{S}_*^{n}\times\mathbb{S}_{++}^{n}\times\mathbb{R}_*^{n\times m}\times\mathbb{N}^+&\;\rightarrow\mathbb{R}_*^{m\times k}\times\mathbb{R}_*^{k\times k}\\
			(A,B,V,k)&\;\mapsto (\hat{X},\Lambda)
			\text{ s.t. }
			\boxed{V^TAV\hat{X}=V^TBV\hat{X}\Lambda }
		\end{align*}
		where $\Lambda=\mathrm{diag}(\lambda_1,\dots,\lambda_k)$ consists of $k\leq m\leq n$ \textbf{extremal eigenvalues}	
		of the projected problem with corresponding eigenvectors in the columns of $\hat{X}\!.\!\!$\\
		\smallskip
		Then, Rayleigh-Ritz vectors are formed by the columns of $X_{i+1}:=V\hat{X}$.\\
		\smallskip
		By convention, $\hat{X}^TV^TBV\hat{X}=I_{k}$ so that $X_{i+1}^TBX_{i+1}=I_k$ and $X_{i+1}^TAX_{i+1}=\Theta$.
	\end{block}
\end{frame}

% Slide 09
\begin{frame}{Definition of Early\_LOBPCG iterations}
	\begin{itemize}
	\item In their earliest form, LOBPCG iterations are defined by Knyazev (2001) as Rayleigh-Ritz projections w.r.t. $\text{range}([X_{i},Z_i,X_{i-1}])$. We refer to these as\\
	\smallskip
	\vspace{.1cm}
	\begin{center}
	$\hspace{-3cm}$Early\_LOBPCG($A$, $B$, $X_{-1}$, $T$, $k$):\tinyskip\\
	\small
	\fbox{\begin{varwidth}{10cm}
	\hfill{\color{gray}{$\triangleright\,X_{-1}\in \mathbb{R}_*^{n\times m}$, $k\leq m\leq n$}}\\
	$(\hat{X}_0,\Lambda_0)\mapsfrom\text{RR}(A,B,X_{-1},m)$\tinyskip\\
	$X_0:=X_{-1}\hat{X}_0$ ; $R_{0}:=AX_{0}-BX_{0}\Lambda_{0}\,$\tinyskip\\
	\textbf{for} $i=0,1,\dots$ \textbf{do}\tinyskip\\
	\hspace*{.4cm}$Z_i:=TR_i$\tinyskip\\
	\hspace*{.4cm}\textbf{if} $i==0$ \textbf{then} $V_{i+1}:=[X_i,Z_i]$ \textbf{else} $V_{i+1}:=[X_i,Z_i,X_{i-1}]$\tinyskip\\
	%\hspace*{.4cm}$Z_i\mapsfrom\texttt{Ortho}(Z_i,Q)$\\
	\hspace*{.4cm}$(\hat{X}_{i+1},\Lambda_{i+1})\mapsfrom\text{RR}(A,B,V_{i+1},m)$\tinyskip\\
	\hspace*{.4cm}$X_{i+1}:=V_{i+1}\hat{X}_{i+1}$; $R_{i+1}:=AX_{i+1}-BX_{i+1}\Lambda_{i+1}$
	\end{varwidth}}\end{center}
	\medskip
	\item As Early\_LOBPCG converges, $[X_i,Z_i,X_{i-1}]$ becomes ill-conditioned which, when relying on finite arithmetic, leads to error propagation through the Rayleigh-Ritz procedure, eventually making the method unstable.
	\end{itemize}
	%\vspace{.1cm}
	%Remarks:
	%\begin{itemize}
	%\item $T:=I_n$ and $X_{-1}^TBX_{-1}=I_k$ $\implies$ $V_{i+1}^TBV_{i+1}=I_{\bullet}$.
	%\end{itemize}
	\medskip
	\tiny{Knyazev, A. V. (2001). Toward the optimal preconditioned eigensolver: Locally optimal block preconditioned conjugate gradient method. SIAM journal on scientific computing, 23(2), 517-541.}
\end{frame}


\section{Basic\_LOBPCG iterations (Knyazev, 2001)}

% Slide 10
\begin{frame}{Definition of Basic\_LOBPCG iterations}
	\begin{itemize}
	\item To circumvent the stability issue of Early\_LOBPCG, Knyazev (2001) proposes to improve the conditioning of the matrices $V_2,V_3,\dots$, while preserving their ranges. 
	We refer to these iterations as Basic\_LOBPCG.
	\item We denote the variables of Basic\_LOBPCG by $\widetilde{X}_i,\widetilde{Z}_i,...$. Then,\smallskip
	\begin{itemize}
	\item The iterates for $i=0$ are the same as in Early\_LOBPCG:\vspace{-.1cm}
	\begin{align*}
	\hspace{-3.1cm}
	(\hat{\widetilde{X}}_0,\widetilde{\Lambda}_0)\mapsfrom\text{RR}(A,B,X_{-1},m)
	&\implies\;\hat{\widetilde{X}}_0=\hat{X}_0,\,\widetilde{\Lambda}_0=\Lambda_0\\
	\hspace{-3.1cm}
	\widetilde{X}_0:=X_{-1}\hat{\widetilde{X}}_0
	&\implies\;\widetilde{X}_0=X_0\\
	\hspace{-3.1cm}
	\widetilde{R}_0:=A\widetilde{X}_0-B\widetilde{X}_0\widetilde{\Lambda}_0
	\,;\;Z_0:=T^{-1}R_0
	&\implies\;\widetilde{R}_0=R_0,\,\widetilde{Z}_0=Z_0
	\end{align*}
	\vspace{-.5cm}\\
	\item The iterates for $i=1$ are also the same as in Early\_LOBPCG:\vspace{-.1cm}
	\begin{align*}
	\hspace{-1.1cm}
	\widetilde{V}_1:=[\widetilde{X}_0,\widetilde{Z}_0]
	&\implies\;\widetilde{V}_1=V_1\\
	\hspace{-1.1cm}
	(\hat{\widetilde{X}}_1,\widetilde{\Lambda}_1)\mapsfrom\text{RR}(A,B,\widetilde{V}_{1},m)
	&\implies\;\hat{\widetilde{X}}_1=\hat{X}_1,\,\widetilde{\Lambda}_1=\Lambda_1\\
	\hspace{-1.1cm}
	\widetilde{X}_1:=\widetilde{V}_{1}\hat{\widetilde{X}}_1=
	\widetilde{X}_{0}\hat{\widetilde{X}}_{1|\widetilde{X}_0}+
	\widetilde{Z}_{0}\hat{\widetilde{X}}_{1|\widetilde{Z}_0}
	&\implies\;\widetilde{X}_1=X_1=X_{0}\hat{X}_{1|X_0}+Z_{0}\hat{X}_{1|Z_0}\\
	\hspace{-1.1cm}
	\widetilde{R}_1:=A\widetilde{X}_1-B\widetilde{X}_1\widetilde{\Lambda}_1\,;\;\widetilde{Z}_1:=T^{-1}\widetilde{R}_1
	&\implies\;\widetilde{R}_1=R_1,\,\widetilde{Z}_1=Z_1
	\end{align*}
	\end{itemize}
	\end{itemize}
	\vspace{-.1cm}
	\tiny{Knyazev, A. V. (2001). Toward the optimal preconditioned eigensolver: Locally optimal block preconditioned conjugate gradient method. SIAM journal on scientific computing, 23(2), 517-541.}
\end{frame}

% Slide 11
\begin{frame}{Definition of Basic\_LOBPCG iterations, cont'd\textsubscript{1}}
	\begin{itemize}
	\item[]
	\begin{itemize}
	\item Then, blocks of search directions are introduced, and used to define $\widetilde{V}_{i+1}$.\\
	For $i=1$, this is done by\vspace{-.1cm}
	\begin{align*}
		\widetilde{P}_1:=\widetilde{Z}_0\hat{\widetilde{X}}_{1|\widetilde{Z}_0}
		&\implies\widetilde{P}_1=
		\widetilde{X}_1-\widetilde{X}_0\hat{\widetilde{X}}_{1|\widetilde{X}_0}=
		X_1-X_0\hat{X}_{1|X_0}\\
		\widetilde{V}_2:=[\widetilde{X}_1,\widetilde{Z}_1,\widetilde{P}_1]
		&\implies\mathcal{R}(\widetilde{V}_2)=\mathcal{R}([\widetilde{X}_1,\widetilde{Z}_1,\widetilde{P}_1])\\
		&\hspace{2.05cm}=\mathcal{R}([X_1,Z_1,X_1-X_0\hat{X}_{1|X_0}])\\
		&\hspace{2.05cm}=\mathcal{R}([X_1,Z_1,X_0])\\
		&\hspace{2.05cm}=\mathcal{R}(V_2)
	\end{align*}
	\vspace{-.6cm}\\
	\item Consequently, the iterates for $i=2$ are such that\vspace{-.1cm}
	\begin{align*}
	\hspace{-1.1cm}
	(\hat{\widetilde{X}}_2,\widetilde{\Lambda}_2)\mapsfrom\text{RR}(A,B,\widetilde{V}_{2},m)
	&\implies\;\widetilde{X}_2=X_2,\,\widetilde{\Lambda}_2=\Lambda_2\\
	\hspace{-1.1cm}
	\widetilde{R}_2:=A\widetilde{X}_2-B\widetilde{X}_2\widetilde{\Lambda}_2\,;\;\widetilde{Z}_2:=T^{-1}\widetilde{R}_2
	&\implies\;\widetilde{R}_2=R_2,\,\widetilde{Z}_2=Z_2\\
	\widetilde{P}_2:=\widetilde{Z}_1\hat{\widetilde{X}}_{2|\widetilde{Z}_1}+\widetilde{P}_1\hat{\widetilde{X}}_{2|\widetilde{P}_1}
	&\implies\widetilde{P}_2=
	\widetilde{X}_2-\widetilde{X}_1\hat{\widetilde{X}}_{2|\widetilde{X}_1}\\
	&\hspace{1.33cm}=X_2-X_1\hat{\widetilde{X}}_{2|\widetilde{X}_1}
	\end{align*}	
	\end{itemize}
	\end{itemize}
	\vspace*{-.1cm}
	\tiny{Knyazev, A. V. (2001). Toward the optimal preconditioned eigensolver: Locally optimal block preconditioned conjugate gradient method. SIAM journal on scientific computing, 23(2), 517-541.}
\end{frame}

% Slide 12
\begin{frame}{Definition of Basic\_LOBPCG iterations, cont'd\textsubscript{2}}
	\begin{itemize}
	\item[]
	\begin{itemize}
	\item Then, the iterates for $i=3$ are such that\vspace{-.1cm}
	\begin{align*}
	\widetilde{V}_3:=[\widetilde{X}_2,\widetilde{Z}_2,\widetilde{P}_2]
	&\implies\mathcal{R}(\widetilde{V}_3)=\mathcal{R}([\widetilde{X}_2,\widetilde{Z}_2,\widetilde{P}_2])\\
	&\hspace{2.05cm}=\mathcal{R}([X_2,Z_2,X_2-X_2\hat{\widetilde{X}}_{3|\widetilde{X}_0}])\\
	&\hspace{2.05cm}=\mathcal{R}([X_2,Z_2,X_1])=\mathcal{R}(V_3)\\
	(\hat{\widetilde{X}}_3,\widetilde{\Lambda}_3)\mapsfrom\text{RR}(A,B,\widetilde{V}_{3},m)
	&\implies\;\widetilde{X}_3=X_3,\,\widetilde{\Lambda}_3=\Lambda_3\\
	\;\;\;\;\vdots
	\end{align*}
	\vspace{-.65cm}\\	
	\item In general, for all $i>0$, we have\vspace{-.1cm}
	\begin{align*}
	\widetilde{P}_{i+1}:=
	\widetilde{Z}_i\hat{\widetilde{X}}_{i+1|\widetilde{Z}_i}+\widetilde{P}_i\hat{\widetilde{X}}_{i+1|\widetilde{P}_i}=
	X_{i+1}-X_i\hat{\widetilde{X}}_{i+1|\widetilde{X}_i}
	\end{align*}
	\vspace{-.5cm}\\
	so that $\mathcal{R}(\widetilde{V}_{i+1})\!=\!\mathcal{R}(V_{i+1})$ which, in turn, implies
	$\widetilde{X}_{i+1}=X_{i+1}$, $\widetilde{\Lambda}_{i+1}=\Lambda_{i+1},\hspace{-1cm}$\\ $\widetilde{R}_{i+1}=R_{i+1}$ and $\widetilde{Z}_{i+1}=Z_{i+1}.$\smallskip
	\item So, in exact arithmetic, the iterates of Basic\_LOBPCG are equivalent to those of Early\_LOBPCG, the difference being that, when nearing convergence, $\widetilde{V}_{i+1}$ is presumably better conditioned than $V_{i+1}$.
	\end{itemize}
	\end{itemize}
	\smallskip
	\tiny{Knyazev, A. V. (2001). Toward the optimal preconditioned eigensolver: Locally optimal block preconditioned conjugate gradient method. SIAM journal on scientific computing, 23(2), 517-541.}
\end{frame}

% Slide 13
\begin{frame}{Definition of Basic\_LOBPCG iterations, cont'd\textsubscript{3}}
	\begin{itemize}
	\item Dropping the $\widetilde{\bullet}$ notation, a pseudocode for the Basic\_LOBPCG iterations is given as follows:
	\smallskip
	\vspace{.05cm}
	\begin{center}
	$\hspace{-4.9cm}$Basic\_LOBPCG($A$, $B$, $X_{-1}$, $T$, $k$):\tinyskip\\
	\small
	\fbox{\begin{varwidth}{17cm}
		\hfill{\color{gray}{$\triangleright\,X_{-1}\in \mathbb{R}_*^{n\times m}$, $k\leq m\leq n$}}\\
		$(\hat{X}_0,\Lambda_0)\mapsfrom\text{RR}(A,B,X_{-1},m)$\tinyskip\\
		$X_0:=X_{-1}\hat{X}_0$ ; $R_{0}:=AX_{0}-BX_{0}\Lambda_{0}\,$\tinyskip\\
		\textbf{for} $i=0,1,\dots$ \textbf{do}\tinyskip\\
		\hspace*{.4cm}$Z_i:=TR_i$\tinyskip\\
		\hspace*{.4cm}\textbf{if} $i==0$ \textbf{then} $V_{i+1}:=[X_i,Z_i]$ \textbf{else} $V_{i+1}:=[X_i,Z_i,P_{i}]$\tinyskip\\
		%\hspace*{.4cm}$Z_i\mapsfrom\texttt{Ortho}(Z_i,Q)$\\
		\hspace*{.4cm}$(\hat{X}_{i+1},\Lambda_{i+1})\mapsfrom\text{RR}(A,B,V_{i+1},m)$\tinyskip\\
		\hspace*{.4cm}$X_{i+1}:=V_{i+1}\hat{X}_{i+1}$; $R_{i+1}:=AX_{i+1}-BX_{i+1}\Lambda_{i+1}$\tinyskip\\
		\hspace*{.4cm}\textbf{if} $i==0$ \textbf{then} $P_{i+1}:=Z_i\hat{X}_{i+1|Z_i}$ \textbf{else} $P_{i+1}:=Z_i\hat{X}_{i+1|Z_i}+P_{i}\hat{X}_{i+1|P_i}$
	\end{varwidth}}\end{center}
	\medskip
	\item In some implementations, the iterate $X_{i+1}$ is updated as\vspace{-.1cm}
	\begin{align*}
	X_{i+1}:=P_{i+1}+X_i\hat{X}_{i+1|X_i},
	\end{align*}
	\vspace{-.45cm}\\
	which is equivalent to setting $X_{i+1}:=V_{i+1}\hat{X}_{i+1}$.	
	\end{itemize}
	%\smallskip
	%\tiny{Knyazev, A. V. (2001). Toward the optimal preconditioned eigensolver: Locally optimal block preconditioned conjugate gradient method. SIAM journal on scientific computing, 23(2), 517-541.}
\end{frame}

% Slide 14
\begin{frame}{Rayleigh-Ritz procedure in Basic\_LOBPCG}
	\begin{itemize}
		\item Consider the reduced eigenvalue problems solved in the Rayleigh-Ritz procedure of Basic\_LOBPCG.\\
		For $i>1$, the following matrices need to be assembled:
		\begin{align*}
			V_{i+1}^TAV_{i+1}=
			\begin{bmatrix}
				X_{i}^TAX_i&X_{i}^TAZ_i&X_{i}^TAP_{i}\\
				.&Z_{i}^TAZ_i&Z_{i}^TAP_{i}\\
				.&.&P_{i}^TAP_{i}
			\end{bmatrix}
		\end{align*}
		and 
		\begin{align*}
			V_{i+1}^TBV_{i+1}=
			\begin{bmatrix}
				X_{i}^TBX_i&X_{i}^TBZ_i&X_{i}^TBP_{i}\\
				.&Z_{i}^TBZ_i&Z_{i}^TBP_{i}\\
				.&.&P_{i}^TBP_{i}
			\end{bmatrix}
		\end{align*}
		where, by construction/convention, we have:
		\begin{align*}
			\hspace{-.4cm}
			X_i^TAX_i=\Lambda_i
			\text{ and }
			X_i^TBX_i=I_{m}.
		\end{align*}
	\end{itemize}
\end{frame}

% Slide 15
\begin{frame}{Rayleigh-Ritz procedure in Basic\_LOBPCG, cont'd}
	\begin{itemize}
	\item[] Unless the basis $V_{i+1}$ is orthogonalized, the remaining blocks
	\begin{align*}
	&X_{i}^TAZ_i\;,\;
	X_{i}^TAP_{i}\;,\;
	Z_{i}^TAZ_i\;,\;
	Z_{i}^TAP_{i},\;
	P_i^TAP_i\\
	\text{and }
	&X_{i}^TBZ_i\;,\;
	X_{i}^TBP_{i}\;,\;
	Z_{i}^TBZ_i\;,\;
	Z_{i}^TBP_{i},\;
	P_i^TBP_i.
	\end{align*}
	do not have any specific structures.
	\item We observed that, in practical implementations, which rely on implicit updates of the products $AX,BX,AP$ and $BP$, the stability of LOBPCG is enhanced by explicitly computing $X^TAX$ rather than assuming that $X^TAX=\Lambda$ stands in finite precision.
	\end{itemize}
\end{frame}

% Slide 16
\begin{frame}{Implicit product updates in Basic\_LOBPCG}
	\begin{itemize}
	\item From the fact that $P_1=Z_0\hat{X}_{1|Z_0}$ we can compute the products $AP_1$ and $BP_1$ from $AZ_0$ and $BZ_0$ as follows:
	\begin{align*}
	AP_1:=AZ_0\hat{X}_{1|Z_0}
	\text{ and }
	BP_1:=BZ_0\hat{X}_{1|Z_0}.
	\end{align*}
	\item From the fact that $X_1=X_0\hat{X}_{1|X_0}+Z_0\hat{X}_{1|Z_0}$, the products $AX_1$ and $BX_1$ can be formed from $AX_0$, $AP_1$, $BX_0$ and $BP_1$ as follows:
	\begin{align*}
	AX_1:=AX_0\hat{X}_{1|X_0}+AP_1
	\text{ and }
	BX_1:=BX_0\hat{X}_{1|X_0}+BP_1.
	\end{align*}
	\item For $i>0$, from the fact that $P_{i+1}=Z_{i}\hat{X}_{i+1|Z_i}+P_i\hat{X}_{i+1|P_i}$, the $AP_{i+1}$ and $BP_{i+1}$ can be calculated as follows from $AZ_i$, $AP_i$, $BZ_i$ and $BP_i$:
	\begin{align*}
	\hspace{-.575cm}
	AP_{i+1}:=AZ_i\hat{X}_{i+1|Z_i}+AP_i\hat{X}_{i+1|P_i}
	\text{ and }
	BP_{i+1}:=&BZ_i\hat{X}_{i+1|Z_i}+BP_i\hat{X}_{i+1|P_i}.
	\end{align*}
	\item For $i>0$, from the fact that $X_{i+1}=P_{i+1}+X_{i}\hat{X}_{i+1|X_i}$, $AX_{i+1}$ and $BX_{i+1}$ can be calculated as follows from $AP_{i+1}$, $AX_i$, $BP_{i+1}$ and $BX_{i+1}$:
	\begin{align*}
		AX_{i+1}:=AP_{i+1}+AX_i\hat{X}_{i+1|X_i}
		\text{ and }
		BX_{i+1}:=BP_{i+1}+BX_i\hat{X}_{i+1|X_i}.
	\end{align*}
	\end{itemize}
\end{frame}

% Slide 17
\begin{frame}{Implementation of Basic\_LOBPCG iterations}
	\begin{itemize}
		\item Making use of the relations and implicit product updates presented, the implementation of Basic\_LOBPCG iterations takes the following form:
		\vspace{-.18cm}
		\begin{algorithm}[H]
			\algsetup{linenosize=\tiny}
			\tiny
			\caption{\texttt{Basic\_LOBPCG}($A$, $B$, $X$, $T$, $k$) $\mapsto$ $(X,\Lambda)$}
			\begin{algorithmic}[1]
				\STATE{Allocate memory for $Z,P\in\mathbb{R}^{n\times m}$}
				\COMMENT{$X\in\mathbb{R}_*^{n\times m}$, $k\leq m\leq n$}
				\STATE{Allocate memory for $AX,AZ,AP\in\mathbb{R}^{n\times m}$}
				\STATE{Allocate memory for $BX,BZ,BP\in\mathbb{R}^{n\times m}$}
				\STATE{Compute $AX,BX$}
				\STATE{$(\hat{X},\Lambda)\mapsfrom \texttt{RR}(X,AX,BX,m)$}
				\STATE{$X:=X\hat{X}$}
				\STATE{$[AX,BX]:=[AX\hat{X},BX\hat{X}]$}
				\COMMENT{implicit product updates}
				\STATE{$Z:=AX-BX\Lambda$}
				\FOR{$j=0,1,\dots$}
				\STATE{$Z:=TZ$}
				\STATE{Compute $AZ,BZ$}
				\IF{$i==0$}
				\STATE{$(\hat{X},\Lambda)\mapsfrom \texttt{RR}(X,Z,AZ,BZ,\Lambda,m)$}
				\COMMENT{$\hat{X}=[\hat{X}_X^T,\hat{X}_Z^T]^T$}
				\STATE{$P:=Z\hat{X}_Z$}
				\STATE{$[AP,BP]:=[AZ\hat{X}_Z,BZ\hat{X}_Z]$}
				\COMMENT{implicit product updates}
				\ELSE
				\STATE{$(\hat{X},\Lambda)\mapsfrom \texttt{RR}(X,Z,P,AZ,AP,BZ,BP,\Lambda,m)$}
				\COMMENT{$\hat{X}=[\hat{X}_X^T,\hat{X}_Z^T,\hat{X}_P^T]^T$}
				\STATE{$P:=Z\hat{X}_Z+P\hat{X}_P$}
				\STATE{$[AP,BP]:=[AZ\hat{X}_Z,BZ\hat{X}_Z]+[AP\hat{X}_P,BP\hat{X}_P]$}
				\COMMENT{implicit product updates}
				\ENDIF
				\STATE{$X:=P+X\hat{X}_X$}
				\STATE{$[AX,BX]:=[AP,BP]+[AX\hat{X}_X,BX\hat{X}_X]$}
				\COMMENT{implicit product updates}
				\STATE{$Z:=AX-BX\Lambda$}	
				\ENDFOR
			\end{algorithmic}
			\label{algo:Basic_LOBPCG}
		\end{algorithm}	
	\end{itemize}	
\end{frame}

% Slide 18
\begin{frame}{Implementation of Basic\_LOBPCG iterations, cont'd\textsubscript{1}}
	\begin{itemize}
		\item The Rayleigh-Ritz procedures of \texttt{Basic\_LOBPCG} are as follows:
		\vspace{-.18cm}
		\begin{algorithm}[H]
			\algsetup{linenosize=\footnotesize}
			\footnotesize
			\caption{\texttt{RR}($X$, $AX$, $BX$)}
			\begin{algorithmic}[1]
				\STATE{}
				\COMMENT{$X\in\mathbb{R}_*^{n\times m}$, $m\leq n$}
				\STATE{Compute $X^TAX,X^TBX\in\mathbb{R}^{m\times m}$}
				\STATE{Solve for $\hat{X}\in\mathbb{R}_*^{m\times m}$ and $\Lambda=\mathrm{diag}(\lambda_1,\dots,\lambda_m)$ s.t. $G_A\hat{X}\!=\!G_B\hat{X}\Lambda$ and $\hat{X}^T\!G_B\hat{X}\!=\!I_m\hspace{-1cm}$\\ where $G_A:=X^TAX$ and $G_B:=X^TBX$.}
				\RETURN{$\hat{X},\Lambda$}
			\end{algorithmic}
		\end{algorithm}	
		\vspace{-.35cm}
		\begin{algorithm}[H]
		\algsetup{linenosize=\footnotesize}
		\footnotesize
		\caption{\texttt{RR}($X$, $Z$, $AX$, $AZ$, $BZ$)}
		\begin{algorithmic}[1]
			\STATE{}
			\COMMENT{$[X,Z]\in\mathbb{R}_*^{n\times 2m}$, $2m\leq n$}
			\STATE{Compute $X^TAX,X^TAZ,Z^TAZ,X^TBZ,Z^TBZ\in\mathbb{R}^{m\times m}$}
			\STATE{Solve for $\hat{X}\!\in\!\mathbb{R}_*^{2m\times m}\!$ and $\Theta=\mathrm{diag}(\lambda_1,\dots,\lambda_m)$ s.t. $\!G_A\hat{X}\!=G_B\hat{X}\Lambda$ and $\hat{X}^T\!G_B\hat{X}\!=\!I_m\hspace{-1cm}$\\
			where $G_A:=\begin{bmatrix}X^TAX&X^TAZ\\.&Z^TAZ\end{bmatrix}$, $G_B:=\begin{bmatrix}I_m&X^TBZ\\.&Z^TBZ\end{bmatrix}$ and $\lambda_1,\dots,\lambda_m$ are extremal generalized eigenvalues of $(G_A,G_B)$.}
			\RETURN{$\hat{X},\Lambda$}
		\end{algorithmic}
	\end{algorithm}	
	\end{itemize}	
\end{frame}

% Slide 19
\begin{frame}{Implementation of Basic\_LOBPCG iterations, cont'd\textsubscript{2}}
	\begin{itemize}
		\item[] 
		\begin{algorithm}[H]
			\algsetup{linenosize=\footnotesize}
			\footnotesize
			\caption{\texttt{RR}($X$, $Z$, $P$, $AX$, $AZ$, $AP$, $BZ$, $BP$)}
			\begin{algorithmic}[1]
				\STATE{}
				\COMMENT{$[X,Z,P]\in\mathbb{R}_*^{n\times 3m}$, $3m\leq n$}
				\STATE{Compute $X^TAX,X^TAZ,X^TAP,Z^TAZ,Z^TAP,P^TAP\in\mathbb{R}^{m\times m}$}
				\STATE{Compute $X^TBZ,X^TBP,Z^TBZ,Z^TBP,P^TBP\in\mathbb{R}^{m\times m}$}
				\STATE{Solve for $\hat{X}\!\in\!\mathbb{R}_*^{3m\times m}\!$ and $\Lambda=\mathrm{diag}(\lambda_1,\dots,\lambda_m)$ s.t. $\!G_A\hat{X}\!=\!G_B\hat{X}\Lambda$ and $\hat{X}^TG_B\hat{X}\!=\!I_m\hspace{-1cm}$\\ where 
				$G_A:=\begin{bmatrix}X^TAX&X^TAZ&X^TAP\\.&Z^TAZ&Z^TAP\\.&.&P^TAP\end{bmatrix}$, $G_B:=\begin{bmatrix}I_m&X^TBZ&X^TBP\\.&Z^TBZ&Z^TBP\\.&.&P^TBP\end{bmatrix}$
				and $\lambda_1,\dots,\lambda_m$ are extremal generalized eigenvalues of $(G_A,G_B)$.}
				\RETURN{$\hat{X},\Lambda$}
			\end{algorithmic}
		\end{algorithm}	
	\end{itemize}	
\end{frame}

% Slide 20
\begin{frame}{Sources of instability in Basic\_LOBPCG iterations}
	The instability of Basic\_LOBPCG was showcased and related to the ill-conditioning of $V_{i+1}^TBV_{i+1}$. This was explained as follows in the works of Hetmaniuk and Lehoucq~(2006), Knyazev et al.~(2007) and Duersch~(2015):\smallskip
	\begin{enumerate}
		\item[1.] RR($A,B,V_{i+1},m$) needs to solve for $(\hat{X},\Lambda)$ in the reduced equation
		\begin{align*}
			V_{i+1}^TAV_{i+1}\hat{X}=V_{i+1}^TBV_{i+1}\hat{X}\Lambda.
		\end{align*}
		This is done by computing the Cholesky decomposition $LL^T=V_{i+1}^TBV_{i+1}\hspace{-.5cm}$\\
		before solving for $(\hat{Y},\Lambda)$ in the standard symmetric eigenvalue problem
		\begin{align*}
			L^{-1}V_{i+1}^TAV_{i+1}L^{-T}\hat{Y}=\hat{Y}\Lambda
		\end{align*}
		and letting $\hat{X}:=L^{-T}\hat{Y}$.\\\smallskip
		When $V_{i+1}^TBV_{i+1}$ is ill-conditioned, the Cholesky factorization may fail or lead to significant round-off error upon factor deployment.	
	\end{enumerate}	
	\medskip
	\tiny{Hetmaniuk, U., \& Lehoucq, R. (2006). Basis selection in LOBPCG. Journal of Computational Physics, 218(1), 324-332.}\tinyskip\\
	\tiny{Knyazev, A. V., Argentati, M. E., Lashuk, I., \& Ovtchinnikov, E. E. (2007). Block locally optimal preconditioned eigenvalue Xolvers (BLOPEX) in Hypre and PETSc. SIAM Journal on Scientific Computing, 29(5), 2224-2239.}\tinyskip\\	
	\tiny{Duersch, J. A. (2015). High Efficiency Spectral Analysis and BLAS-3 Randomized QRCP with Low-Rank Approximations. University of California, Berkeley.}
\end{frame}

% Slide 21
\begin{frame}{Sources of instability in Basic\_LOBPCG iterations, cont'd\textsubscript{1}}
	\begin{enumerate}
		\item[2.] As we denote the generalized eigenvalues of $(A,B)$ and $(V^TAV,V^TBV)$ for some $V\in\mathbb{R}_*^{n\times m}$ with $m\leq n$ by $\lambda_1\leq\dots\leq\lambda_n$ and $\hat{\lambda}_1\leq\dots\leq\hat{\lambda}_m$, respectively, assuming $A$ is positive definite, Parlett~(1998, see Theorem 11.10.1) states that 
		\begin{align*}
			|\lambda_i-\hat{\lambda}_i|\leq 
			\frac{\|AV-BVL^{-1}V^TAVL^{-T}\|_{B^{-1}}}{\sqrt{\hat{\lambda}_1\vphantom{I^{I^I}}}}
		\end{align*}
		where $LL^T$ is the Cholesky decomposition of $V^TBV$.\\\smallskip
		Consequently, the error $|\lambda_i-\hat{\lambda}_i|$ of a Ritz value $\hat{\lambda}_i$ increases as the basis in the columns of $V$ departs from $B$-orthonormality.	
	\end{enumerate}	
	\medskip
	\tiny{Parlett, B. N. (1998). The symmetric eigenvalue problem. Society for Industrial and Applied Mathematics.}\tinyskip\\
	\tiny{Hetmaniuk, U., \& Lehoucq, R. (2006). Basis selection in LOBPCG. Journal of Computational Physics, 218(1), 324-332.}\tinyskip\\
	\tiny{Knyazev, A. V., Argentati, M. E., Lashuk, I., \& Ovtchinnikov, E. E. (2007). Block locally optimal preconditioned eigenvalue Xolvers (BLOPEX) in Hypre and PETSc. SIAM Journal on Scientific Computing, 29(5), 2224-2239.}\tinyskip\\	
	\tiny{Duersch, J. A. (2015). High Efficiency Spectral Analysis and BLAS-3 Randomized QRCP with Low-Rank Approximations. University of California, Berkeley.}
\end{frame}

% Slide 22
\begin{frame}{Sources of instability in Basic\_LOBPCG iterations, cont'd\textsubscript{2}}
	\begin{itemize}
	\item Additionally, the following observations were made:\smallskip
	\begin{itemize}
		\item The Gram matrix $V_{i+1}^TBV_{i+1}$ can become ill-conditioned irrespective of the conditioning of $B$.
		\item When the number $k$ of approximated eigenvectors is large, $V_{i+1}^TBV_{i+1}$ can become ill-conditioned before any eigenvector is accurately approximated.
	\end{itemize}	
	\end{itemize}
	\medskip
	\tiny{Hetmaniuk, U., \& Lehoucq, R. (2006). Basis selection in LOBPCG. Journal of Computational Physics, 218(1), 324-332.}\tinyskip\\
	\tiny{Knyazev, A. V., Argentati, M. E., Lashuk, I., \& Ovtchinnikov, E. E. (2007). Block locally optimal preconditioned eigenvalue Xolvers (BLOPEX) in Hypre and PETSc. SIAM Journal on Scientific Computing, 29(5), 2224-2239.}\tinyskip\\	
	\tiny{Duersch, J. A. (2015). High Efficiency Spectral Analysis and BLAS-3 Randomized QRCP with Low-Rank Approximations. University of California, Berkeley.}
\end{frame}

% Slide 23
\begin{frame}{More stable LOBPCG iterations}
	\begin{itemize}
	\item Alternative LOBPCG iterations were introduced to fix the stability issues of Basic\_LOBPCG:
	\begin{itemize}
	\item {\color{blue}{\underline{\href{https://github.com/lobpcg/blopex}{BLOPEX}}}} was developed as an alternative implementation since 2005,
	and became the standard with adoption through {\color{blue}{\underline{\href{https://computing.llnl.gov/projects/hypre-scalable-linear-solvers-multigrid-methods}{Hypre}}}} and PETSc (i.e., {\color{blue}{\underline{\href{https://slepc.upv.es/}{SLEPc}}}}), see Knyazev et al.~(2007).
	\begin{itemize}
	\item In BLOPEX, Knyazev et al.~(2007) $B$-orthogonalize the $Z_i$ and $P_i$ blocks of $V_{i+1}$ independently, but not between themselves.
	The method is shown to work for specific examples.
	\end{itemize}
	\item Hetmaniuk and Lehoucq (2006) investigate the choice of basis used in the Rayleigh-Ritz procedure and its effect on stability with more details.
	\begin{itemize}
	\item Hetmaniuk and Lehoucq (2006) argue that the strategy adopted in BLOPEX to handle ill-conditioned $V_{i+1}^TBV_{i+1}$ matrices has no theoretical justication, and it does not solve the problem when $[X_i,Z_i]$ is ill-conditioned.
	\item In order to improve the stability of Basic\_LOBPCG, they propose to $B$-orthonormalize $V_{i+1}$ at each iteration.
	We call this Ortho\_LOBPCG.
	\end{itemize}
	\item Duersch et al. (2018) improve the performance of Ortho\_LOBPCG, and showcase its better stability in comparison to BLOPEX.
	\end{itemize}
	\end{itemize}	
	\smallskip
	\tiny{Hetmaniuk, U., \& Lehoucq, R. (2006). Basis selection in LOBPCG. Journal of Computational Physics, 218(1), 324-332.}\tinyskip\\
	\tiny{Knyazev, A. V., Argentati, M. E., Lashuk, I., \& Ovtchinnikov, E. E. (2007). Block locally optimal preconditioned eigenvalue Xolvers (BLOPEX) in Hypre and PETSc. SIAM Journal on Scientific Computing, 29(5), 2224-2239.}\tinyskip\\	
	\tiny{Duersch, J. A., Shao, M., Yang, C., \& Gu, M. (2018). A robust and efficient implementation of LOBPCG. SIAM Journal on Scientific Computing, 40(5), C655-C676.}
\end{frame}

\section{Ortho\_LOBPCG iterations (Hetmaniuk and Lehoucq, 2006)}

% Slide 24
\begin{frame}{Definition of Ortho\_LOBPCG iterations}
	\begin{itemize}
	\item LOBPCG is made more robust by making $V_{i+1}$ $B$-orthonormal, i.e., by\\ 
	making sure that $V_{1}^TBV_{1}=I_{2m}$ and $V_{i+1}^TBV_{i+1}=I_{3m}$ for $i=1,2,\dots$
	\item To do so, Hetmaniuk and Lehoucq (2006), rely on a generic procedure
	\begin{align*}
	\text{Ortho}:
	\mathbb{R}_*^{n\times p}\times \mathbb{S}_{++}^n\times \mathbb{R}_*^{n\times q}&\rightarrow \mathbb{R}_*^{n\times p}\\
	(Z,B,W)&\mapsto V
	\end{align*}
	such that $V^TBV=I_{p}$, $V^TBW=0_{p\times q}$ and $\mathcal{R}(Z)\subseteq\mathcal{R}(V)$.
	\item The following observations are made to define Ortho\_LOBPCG iterations:
	\begin{itemize}
    \item Let $Z_0\mapsfrom\text{Ortho}(T^{-1}R_0,B,X_0)$ and $V_{1}:=[X_0,Z_0]$,
	so that $V_1^TBV_1=I_{2m}$ and $\hat{X}_1^T\hat{X}_1=\hat{X}_{1|X_0}^T\hat{X}_{1|X_0}+\hat{X}_{1|Z_0}^T\hat{X}_{1|Z_0}=I_m$.
	\item Now, if we were to let $P_1:=Z_0\hat{X}_{1|Z_0}$ as in Basic\_LOBPCG, we would have 
	\begin{align*}
	\hspace{-.55cm}
	P_1^TBX_1
	\!=\!(Z_0\hat{X}_{1|Z_0})^TBV_1\hat{X}_1
	\!=\!\hat{X}_{1|Z_0}^TZ_0^TB[X_0,Z_0]\hat{X}_1
	\!=\!\hat{X}_{1|Z_0}^T\hat{X}_{1|Z_0}
	\neq I_m
	\end{align*}
	so that $P_1$ also needs to be formed by $B$-orthonormalization against $X_1$.
	\end{itemize}
	\end{itemize}
	\medskip
	\tiny{Hetmaniuk, U., \& Lehoucq, R. (2006). Basis selection in LOBPCG. Journal of Computational Physics, 218(1), 324-332.}
\end{frame}

% Slide 25
\begin{frame}{Definition of Ortho\_LOBPCG iterations, cont'd\textsubscript{1}}
	\begin{itemize}
	\item[] 
	\begin{itemize}
	\item[]
	Note however that letting $P_1\mapsfrom\text{Ortho}(Z_0\hat{X}_{1|Z_0},B,X_1)$ is equivalent to
	\begin{align*}
	\hat{Y}_1\,\mapsfrom&\,\text{Ortho}([0_{m\times m},\hat{X}_{1|Z_0}]^T,V_1^TBV_1,\hat{X}_1)\\
	P_1:=&\,V_1\hat{Y}_1
	\end{align*}
	as we have\medskip
	\begin{enumerate}
	\item[1.]$\hat{Y}_1^TV_1^TBV_1\hat{Y}_1=I_{m}
	\implies
	(V_1\hat{Y}_1)^TBV_1\hat{Y}_1=
	P_1^TBP_1=I_{m}$\smallskip		
	\item[2.]$\hat{Y}_1^TV_1^TBV_1\hat{X}_1=0_{m\times m}
	\implies
	(V_1\hat{Y}_1)^TBX_1=
	P_1^TBX_1=0_{m\times m}$\smallskip
	\item[3.] $\mathcal{R}([0_{m\times m},\hat{X}_{1|Z_0}]^T)\subseteq\mathcal{R}(\hat{Y}_1)
	\implies
	\mathcal{R}(V_1[0_{m\times m},\hat{X}_{1|Z_0}]^T)\subseteq\mathcal{R}(V_1\hat{Y}_1)$\\
	\hspace{4.12cm}$\implies
	\mathcal{R}(Z_0\hat{X}_{1|Z_0})\subseteq\mathcal{R}(P_1)$
	\end{enumerate}\medskip
	Moreover, remember that $V_1^TBV_1=I_{2m}$.\\
	Consequently, $P_1$ may be constructed as follows:
	\begin{align*}
	\hat{Y}_1\,\mapsfrom&\,\text{Ortho}([0_{m\times m},\hat{X}_{1|Z_0}]^T,I_{2m},\hat{X}_1)\\
	P_1:=&\,V_1\hat{Y}_1
	\end{align*}	
	\end{itemize}
	\end{itemize}
	\medskip
	\tiny{Hetmaniuk, U., \& Lehoucq, R. (2006). Basis selection in LOBPCG. Journal of Computational Physics, 218(1), 324-332.}
\end{frame}

% Slide 26
\begin{frame}{Definition of Ortho\_LOBPCG iterations, cont'd\textsubscript{2}}
	\begin{itemize}
	\item[]
	\begin{itemize} 
	\item Then, as we let $Z_1\mapsfrom\text{Ortho}(T^{-1}R_1,B,[X_1,P_1])$ and $V_{2}:=[X_1,Z_1,P_1]$, we have $V_2^TBV_2=I_{3m}$ and
	\begin{align*}
	\hat{X}_2^T\hat{X}_2=\hat{X}_{2|X_1}^T\hat{X}_{2|X_1}+\hat{X}_{2|Z_1}^T\hat{X}_{2|Z_1}+\hat{X}_{2|P_1}^T\hat{X}_{2|P_1}=I_m.
	\end{align*}	
	Now, if we were to let $P_2:=Z_1\hat{X}_{2|Z_1}+P_1\hat{X}_{2|P_1}$, we still would have 
	\begin{align*}
	\hspace{-.55cm}
	P_2^TBX_2
	=&\,(Z_1\hat{X}_{2|Z_1}+P_1\hat{X}_{2|P_1})^TBV_2\hat{X}_2\\
	=&\,\hat{X}_{2|Z_1}^TZ_1^TBV_2\hat{X}_2+\hat{X}_{2|P_1}^TP_1^TBV_2\hat{X}_2\\
	=&\,\hat{X}_{2|Z_1}^TZ_1^TBZ_1\hat{X}_{2|Z_1}+\hat{X}_{2|P_1}^TP_1^TBP_1\hat{X}_{2|P_1}\\
	=&\,\hat{X}_{2|Z_1}^T\hat{X}_{2|Z_1}+\hat{X}_{2|P_1}^T\hat{X}_{2|P_1}\\
	\neq&\, I_m
	\end{align*}
	so that $P_2$ needs to be formed by $B$-orthonormalization against $X_2$.
	\end{itemize}
	\end{itemize}
	\medskip
	\tiny{Hetmaniuk, U., \& Lehoucq, R. (2006). Basis selection in LOBPCG. Journal of Computational Physics, 218(1), 324-332.}
\end{frame}

% Slide 27
\begin{frame}{Definition of Ortho\_LOBPCG iterations, cont'd\textsubscript{3}}
	\begin{itemize}
	\item[]
	\begin{itemize}
	\item[]
	Similarly as before, letting $P_2\mapsfrom\text{Ortho}(Z_1\hat{X}_{2|Z_1}+P_1\hat{X}_{2|P_1},B,X_2)$ is equivalent to
	\begin{align*}
		\hat{Y}_2\,\mapsfrom&\,\text{Ortho}([0_{m\times m},\hat{X}_{2|Z_1}^T,\hat{X}_{2|P_1}^T]^T,V_2^TBV_2,\hat{X}_2)\\
		P_2:=&\,V_2\hat{Y}_2
	\end{align*}
	as we have\medskip	
	\begin{enumerate}
	\item[1.]$\hat{Y}_2^TV_2^TBV_2\hat{Y}_2=I_{m}
	\implies
	(V_2\hat{Y}_2)^TBV_2\hat{Y}_2=
	P_2^TBP_2=I_{m}$\smallskip
	\item[2.]$\hat{Y}_2^TV_2^TBV_2\hat{X}_2=0_{m\times m}
	\implies
	(V_2\hat{Y}_2)^TBX_2=
	P_2^TBX_2=0_{m\times m}$\smallskip
	\item[3.] $\mathcal{R}([0_{m\times m},\hat{X}_{2|Z_1}^T,\hat{X}_{2|P_1}^T]^T)\subseteq\mathcal{R}(\hat{Y}_2)
	\!\implies\!
	\mathcal{R}(V_2[0_{m\times m},\hat{X}_{2|Z_1}^T,\hat{X}_{2|P_1}^T]^T)\!\subseteq\!\mathcal{R}(V_2\hat{Y}_2)\hspace{-1cm}$\\
	\hspace{4.98cm}$\implies
	\mathcal{R}(Z_1\hat{X}_{2|Z_1}+P_1\hat{X}_{2|P_1})\subseteq\mathcal{R}(P_2)$
	\end{enumerate}\medskip
	Moreover, remember that $V_2^TBV_2=I_{3m}$.\\
	Consequently, $P_2$ is best built by
	\begin{align*}
	\hat{Y}_2\,\mapsfrom&\,\text{Ortho}([0_{m\times m},\hat{X}_{2|Z_1}^T,\hat{X}_{2|P_1}^T]^T,I_{3m},\hat{X}_2)\\
	P_2:=&\,V_2\hat{Y}_2
	\end{align*}
	\item Subsequent iterates are defined similarly.
	\end{itemize}
	\end{itemize}
	\medskip
	\tiny{Hetmaniuk, U., \& Lehoucq, R. (2006). Basis selection in LOBPCG. Journal of Computational Physics, 218(1), 324-332.}
\end{frame}

% Slide 28
\begin{frame}{Definition of Ortho\_LOBPCG iterations, cont'd\textsubscript{4}}
	\begin{itemize}
	\item In summary, Ortho\_LOBPCG iterations are defined as follows:
	\smallskip
	\hspace*{-.75cm}
	\begin{center}
	$\hspace{-6.7cm}$Ortho\_LOBPCG($A$, $B$, $X_{-1}$, $T$, $k$):\tinyskip\\
	\small
	$\hspace{-.5cm}$\fbox{\begin{varwidth}{17cm}
	\hfill{\color{gray}{$\triangleright\,X_{-1}\in \mathbb{R}_*^{n\times m}$, $k\leq m\leq n$}}\\
	$(\hat{X}_0,\Lambda_0)\mapsfrom\text{RR}(A,B,X_{-1},m)$\tinyskip\\
	$X_0:=X_{-1}\hat{X}_0$ ; $R_{0}:=AX_{0}-BX_{0}\Lambda_{0}\,$\tinyskip\\
	\textbf{for} $i=0,1,\dots$ \textbf{do}\tinyskip\\
	\hspace*{.4cm}\textbf{if} $i==0$ \textbf{then} $Z_i\mapsfrom\text{Ortho}(TR_i,B,X_i)$ \textbf{else} $Z_i\mapsfrom\text{Ortho}(TR_i,B,[X_i,P_i])$\tinyskip\\
	\hspace*{.4cm}\textbf{if} $i==0$ \textbf{then} $V_{i+1}:=[X_i,Z_i]$ \textbf{else} $V_{i+1}:=[X_i,Z_i,P_i]$ \tinyskip\\
	%\hspace*{.4cm}$Z_i\mapsfrom\texttt{Ortho}(Z_i,Q)$\\
	\hspace*{.4cm}$(\hat{X}_{i+1},\Lambda_{i+1})\mapsfrom\text{RR}(A,B,V_{i+1},m)$\tinyskip\\
	\hspace*{.4cm}$X_{i+1}:=V_{i+1}\hat{X}_{i+1}$; $R_{i+1}:=AX_{i+1}-BX_{i+1}\Lambda_{i+1}$\tinyskip\\
	\hspace*{.4cm}\textbf{if} $i==0$ \textbf{then}\\
	\hspace*{.8cm}$\hat{Y}_{i+1}\mapsfrom\text{Ortho}([0_{m\times m},\hat{X}_{i+1|Z_i}^T]^T,I_{2m},\hat{X}_{i+1})$\\
	\hspace*{.4cm}\textbf{else}\\
	\hspace*{.8cm}$\hat{Y}_{i+1}\mapsfrom\text{Ortho}([0_{m\times m},\hat{X}_{i+1|Z_i}^T,\hat{X}_{i+1|P_i}^T]^T,I_{3m},\hat{X}_{i+1})$\\
	\hspace*{.4cm}$P_{i+1}:=V_{i+1}\hat{Y}_{i+1}$
	\end{varwidth}}\end{center}
	\end{itemize}
	\smallskip
	\tiny{Hetmaniuk, U., \& Lehoucq, R. (2006). Basis selection in LOBPCG. Journal of Computational Physics, 218(1), 324-332.}
\end{frame}

% Slide 29
\begin{frame}{Implicit product updates in Ortho\_LOBPCG}
	\begin{itemize}
		\item From the fact that $X_1=X_0\hat{X}_{1|X_0}+Z_0\hat{X}_{1|Z_0}$, the products $AX_1$ and $BX_1$ can still be formed from $AX_0$, $AZ_0$, $BX_0$ and $BZ_0$ as follows:
		\begin{align*}
			AX_1:=AX_0\hat{X}_{1|X_0}+AZ_0\hat{X}_{1|Z_0}
			\text{ and }
			BX_1:=BX_0\hat{X}_{1|X_0}+BZ_0\hat{X}_{1|Z_0}.
		\end{align*}
		\item Similarly, from the fact that $P_1=X_0\hat{Y}_{1|X_0}+Z_0\hat{Y}_{1|Z_0}$, we have:
		\begin{align*}
		AP_1:=AX_0\hat{Y}_{1|X_0}+AZ_0\hat{Y}_{1|Z_0}
		\text{ and }
		BP_1:=BX_0\hat{Y}_{1|X_0}+BZ_0\hat{Y}_{1|Z_0}.
		\end{align*}	
		\item For $i>0$, from the fact that $X_{i+1}=X_{i}\hat{X}_{i+1|X_i}+Z_{i}\hat{X}_{i+1|Z_i}+P_{i}\hat{X}_{i+1|P_i}$, $AX_{i+1}$ and $BX_{i+1}$ can be calculated as follows from $AX_{i}$, $AZ_i$, $AP_i$, $BX_i$, $BZ_i$ and $BP_i$:
		\begin{align*}
			AX_{i+1}&\,:=AX_{i}\hat{X}_{i+1|X_i}+AZ_{i}\hat{X}_{i+1|Z_i}+AP_{i}\hat{X}_{i+1|P_i}\\
			\text{and }
			BX_{i+1}&\,:=BX_{i}\hat{X}_{i+1|X_i}+BZ_{i}\hat{X}_{i+1|Z_i}+BP_{i}\hat{X}_{i+1|P_i}.
		\end{align*}
		\item Similarly, $AP_{i+1}$ and $BP_{i+1}$ can be calculated as follows:
		\begin{align*}
		AP_{i+1}&\,:=AX_{i}\hat{Y}_{i+1|X_i}+AZ_{i}\hat{Y}_{i+1|Z_i}+AP_{i}\hat{Y}_{i+1|P_i}\\
		\text{and }
		BP_{i+1}&\,:=BX_{i}\hat{Y}_{i+1|X_i}+BZ_{i}\hat{Y}_{i+1|Z_i}+BP_{i}\hat{Y}_{i+1|P_i}.
	\end{align*}	
	\end{itemize}
\end{frame}

% Slide 30
\begin{frame}{Implementation of Ortho\_LOBPCG iterations}
	\begin{itemize}
		\vspace{-.1cm}
		\item Making use of the relations and implicit product updates presented, the implementation of Ortho\_LOBPCG iterations takes the following form:
		\vspace{-.3cm}
		\begin{algorithm}[H]
			\algsetup{linenosize=\tiny}
			\tiny
			\caption{\texttt{Ortho\_LOBPCG}($A$, $B$, $X$, $T$, $k$) $\mapsto$ $(X,\Lambda)$}
			\begin{algorithmic}[1]
				\STATE{Allocate memory for $Z,P,W\in\mathbb{R}^{n\times m}$}
				\COMMENT{$X\in\mathbb{R}_*^{n\times m}$, $k\leq m\leq n$}
				\STATE{Allocate memory for $AX,AZ,AP\in\mathbb{R}^{n\times m}$}
				\STATE{Allocate memory for $BX,BZ,BP\in\mathbb{R}^{n\times m}$}
				\STATE{Compute $AX,BX$}
				\STATE{$(\hat{X},\Lambda)\mapsfrom \texttt{RR}(X,AX,BX)$}
				\STATE{$[X,AX,BX]:=[X\hat{X},AX\hat{X},BX\hat{X}]$}
				\COMMENT{implicit product updates}
				\STATE{$Z:=AX-BX\Lambda$}
				\FOR{$j=0,1,\dots$}
				\STATE{\textbf{if} $j==0$ \textbf{then} $Z\mapsfrom\text{Ortho}(TZ,B,X)$ \textbf{else} $Z\mapsfrom\text{Ortho}(TZ,B,[X,P])$ }
				\STATE{Compute $AZ,BZ$}
				\IF{$i==0$}
				%\STATE{$(\hat{X},\Lambda)\mapsfrom \texttt{RR}(X,Z,AX,AZ,BX,BZ,\Lambda,k)$}
				\STATE{$(\hat{X},\Lambda)\mapsfrom \texttt{RR}(X,Z,AX,AZ)$}
				\COMMENT{$\hat{X}=[\hat{X}_X^T,\hat{X}_Z^T]^T$}
				\STATE{$\hat{Y}\mapsfrom\text{Ortho}([0_{m\times m},\hat{X}_{Z}^T]^T,I_{2m},\hat{X})$}
				\STATE{$[AP,BP]:=[AX\hat{Y}_X,BX\hat{Y}_X]+[AZ\hat{Y}_Z,BZ\hat{Y}_Z]$}
				\COMMENT{implicit product updates}
				\STATE{$[AX,BX]:=[AX\hat{X}_X,BX\hat{X}_X]+[AZ\hat{X}_Z,BZ\hat{X}_Z]$}
				\COMMENT{implicit product updates}				
				\STATE{$W:=X\hat{X}_X+Z\hat{X}_Z$ ; $P:=X\hat{Y}_X+Z\hat{Y}_Z$ ; $X:=W$}
				\ELSE
				%\STATE{$(\hat{X},\Lambda)\mapsfrom \texttt{RR}(X,Z,P,AX,AZ,BX,BZ,AP,BP,\Lambda,k)$}
				\STATE{$(\hat{X},\Lambda)\mapsfrom \texttt{RR}(X,Z,P,AX,AZ,AP)$}
				\COMMENT{$\hat{X}=[\hat{X}_X^T,\hat{X}_Z^T,\hat{X}_P^T]^T$}		
				%\STATE{$P:=Z\hat{X}_2+P\hat{X}_3$}
				\STATE{$\hat{Y}\mapsfrom\text{Ortho}([0_{m\times m},\hat{X}_{Z}^T,\hat{X}_P^T]^T,I_{3m},\hat{X})$}
				\STATE{$[W,AX]\!:=\![AX\hat{Y}_{\!X},AX\hat{X}_{\!X}]\!+\![AZ\hat{Y}_{\!Z},AZ\hat{X}_{\!Z}]\!+\![AP\hat{Y}_{\!P},AP\hat{X}_{\!P}]$\,; $AP:=W$}
				\COMMENT{implicit product updates}
				\STATE{$[W,BX]\!:=\![BX\hat{Y}_{\!X},BX\hat{X}_{\!X}]\!+\![BZ\hat{Y}_{\!Z},BZ\hat{X}_{\!Z}]\!+\![BP\hat{Y}_{\!P},BP\hat{X}_{\!P}]$\,; $BP:=W$}
				\COMMENT{implicit product updates}
				\STATE{$W:=X\hat{X}_X+Z\hat{X}_Z+P\hat{X}_P$ ; $P:=X\hat{Y}_X+Z\hat{Y}_Z+P\hat{Y}_P$ ; $X:=W$}
				\ENDIF
				%\STATE{Compute $AP,BP$}
				\STATE{$Z:=AX-BX\Lambda$}	
				\ENDFOR
			\end{algorithmic}
			\label{algo:Ortho_LOBPCG}
		\end{algorithm}	
	\end{itemize}	
\end{frame}

% Slide 31
\begin{frame}{Implementation of Ortho\_LOBPCG iterations, cont'd}
	\begin{itemize}
		\item The Rayleigh-Ritz procedure \texttt{RR}($X$, $AX$, $BX$, $k$) is the same as before. 
		The other Rayleigh-Ritz procedures in \texttt{Basic\_LOBPCG} are 
		\vspace{-.2cm}
		\begin{algorithm}[H]
			\algsetup{linenosize=\footnotesize}
			\footnotesize
			\caption{\texttt{RR}($X$, $Z$, $AX$, $AZ$)}
			\begin{algorithmic}[1]
				\STATE{}
				\COMMENT{$[X,Z]\in\mathbb{R}_*^{n\times 2m}$, $2m\leq n$}
				\STATE{Compute $X^TAX,X^TAZ,Z^TAZ\in\mathbb{R}^{m\times m}$}
				\STATE{Solve for $\hat{X}\in\mathbb{R}_*^{2m\times m}$ and $\Lambda=\mathrm{diag}(\lambda_1,\dots,\lambda_m)$ s.t. $\!G_A\hat{X}=\hat{X}\Theta$ and $\hat{X}^T\hat{X}=I_m$ where $G_A:=\begin{bmatrix}X^TAX&X^TAZ\\.&Z^TAZ\end{bmatrix}$ and $\lambda_1,\dots,\lambda_m$ are extremal eigenvalues of $G_A$.}
				\RETURN{$\hat{X},\Lambda$}
			\end{algorithmic}
		\end{algorithm}	
		\vspace{-.55cm}
		\begin{algorithm}[H]
		\algsetup{linenosize=\footnotesize}
		\footnotesize
		\caption{\texttt{RR}($X$, $Z$, $P$, $AX$, $AZ$, $AP$)}
		\begin{algorithmic}[1]
		\STATE{}
		\COMMENT{$[X,Z,P]\in\mathbb{R}_*^{n\times 3m}$, $3m\leq n$}
		\STATE{Compute $X^TAX, X^TAZ,X^TAP,Z^TAZ,Z^TAP,P^TAP\in\mathbb{R}^{m\times m}$}
		\STATE{Solve for $\hat{X}\in\mathbb{R}_*^{3m\times m}$ and $\Lambda=\mathrm{diag}(\lambda_1,\dots,\lambda_m)$ s.t. $\!G_A\hat{X}=\hat{X}\Lambda$ and $\hat{X}^T\hat{X}=I_m$ where 
		$G_A:=\begin{bmatrix}X^TAX&X^TAZ&X^TAP\\.&Z^TAZ&Z^TAP\\.&.&P^TAP\end{bmatrix}$
		and $\lambda_1,\dots,\lambda_m$ are extremal eigenvalues of $G_A$.}
		\RETURN{$\hat{X},\Lambda$}
		\end{algorithmic}
		\end{algorithm}		
	\end{itemize}	
\end{frame}

% Slide 32
\begin{frame}{Choice of $B$-ortonormalization procedure}
	\begin{itemize}
	\item The following orthogonalization procedures need be implemented in order to deploy Ortho\_LOBPCG iterations:
	\begin{align*}
	Z&\,\mapsfrom\text{Ortho}(Z,B,X)\\
	Z&\,\mapsfrom\text{Ortho}(Z,B,[X,P])\\
	\hat{X}&\,\mapsfrom\text{Ortho}([0_{m\times m},\hat{X}_Z^T]^T,I_{2m},\hat{X})\\
	\hat{X}&\,\mapsfrom\text{Ortho}([0_{m\times m},\hat{X}_Z^T,\hat{X}_P^T]^T,I_{3m},\hat{X})
	\end{align*}
	Different methods can be used for this purpose:
	\begin{itemize}
	\item In Hetmaniuk \& Lehoucq (2006) and Duersch et al. (2018):\\
	SVD-based $B$-orthogonalization using SVQB from Stathopoulos \& Wu (2002),$\hspace{-1cm}$\\
	which is cache-efficient, highly stable, with low synchronization cost.
	\item Householder-QR, which is highly stable, but difficult to implement for $B\neq I_n$.
	\item Gram-Schmidt procedures, which can be stable, but less efficient than SVQB.
	\item Cholesky-QR, which is not very stable.
	\end{itemize}
	\end{itemize}	
	\smallskip
	\tiny{Hetmaniuk, U., \& Lehoucq, R. (2006). Basis selection in LOBPCG. Journal of Computational Physics, 218(1), 324-332.}\tinyskip\\
	\tiny{Stathopoulos, A., \& Wu, K. (2002). A block orthogonalization procedure with constant synchronization requirements. SIAM Journal on Scientific Computing, 23(6), 2165-2182.}\tinyskip\\
	\tiny{Duersch, J. A., Shao, M., Yang, C., \& Gu, M. (2018). A robust and efficient implementation of LOBPCG. SIAM Journal on Scientific Computing, 40(5), C655-C676.}
\end{frame}

% Slide 33
\begin{frame}{Definition of the Ortho procedure}
\begin{itemize}
\item In order to $B$-orthogonalize $U\in\mathbb{R}_*^{n\times p}$ against $V\in\mathbb{R}_*^{n\times q}$, the Ortho procedure is defined as follows using the SVQB method from Stathopoulos \& Wu (2002):\medskip\\
\begin{minipage}{.4\framewidth}
\begin{center}
$\hspace{-2.1cm}$Ortho($U$, $B$, $V$):\tinyskip\\
\small
\hspace*{0cm}\fbox{\begin{varwidth}{17cm}
\hfill{\color{gray}{$\triangleright\,U\in \mathbb{R}_*^{n\times p}$,$V\in \mathbb{R}_*^{n\times q}$, $p,q\leq n$}}\tinyskip\\
\textbf{do}\tinyskip\\
\hspace*{.4cm}$U:=U-V(V^TBU)$\tinyskip\\
\hspace*{.4cm}\textbf{do}\tinyskip\\
\hspace*{.8cm}$U:=\text{SVQB}(B,U)$\tinyskip\\
\hspace*{.4cm}\textbf{while} $\frac{\|U^TBU-I_p\|}{\|BU\|\|U\|}<\tau_{ortho}$\tinyskip\\
\textbf{while} $\frac{\|V^TBU\|}{\|BV\|\|U\|}<\tau_{ortho}$\tinyskip\\
\textbf{return} $U$
\end{varwidth}}\end{center}
\end{minipage}
\begin{minipage}{.55\framewidth}
\begin{center}
$\hspace{-3.75cm}$SVQB($U$, $B$):\tinyskip\\
\small
\hspace*{0cm}\fbox{\begin{varwidth}{17cm}
\hfill{\color{gray}{$\triangleright\,U\in \mathbb{R}_*^{n\times p}$, $p\leq n$}}\tinyskip\\
$D:=(\text{diag}(U^TBU))^{-1/2}$\tinyskip\\
Solve for eigen-pairs $Z,\Theta$ of $DU^TBUD$\\
such that $DU^TBUDZ=Z\Theta$\tinyskip\\
$\theta_{max}:=\max_{i}|\Theta_{ii}|$\tinyskip\\
\textbf{for} $i=1,\dots,p$ \textbf{do}\tinyskip\\
\hspace*{.4cm}\textbf{if} $\Theta_{ii}<\tau\,\theta_{max}$ \textbf{then} $\Theta_{ii}:=\tau\,\theta_{max}$ \tinyskip\\
\textbf{return} $UDZ\Theta^{-1/2}$
\end{varwidth}}\end{center}
\end{minipage}
\end{itemize}
\medskip
where $\tau_{ortho}$ and $\tau$ are set to modest multiples of the machine precision.\smallskip\\
\smallskip
\tiny{Stathopoulos, A., \& Wu, K. (2002). A block orthogonalization procedure with constant synchronization requirements. SIAM Journal on Scientific Computing, 23(6), 2165-2182.}
\end{frame}

\section{BLOPEX\_LOBPCG iterations (Knyazev et al., 2007)}

% Slide 34
\begin{frame}{Definition of BLOPEX\_LOBPCG iterations}
	\begin{itemize}
		\item BLOPEX\_LOBPCG iterations are summarized as follows:
		\smallskip
		%\vspace{.05cm}
		\begin{center}
			$\hspace{-4cm}$BLOPEX\_LOBPCG($A$, $B$, $X_{-1}$, $T$, $k$):\tinyskip\\
			\small
			\fbox{\begin{varwidth}{17cm}
					\hfill{\color{gray}{$\triangleright\,X_{-1}\in \mathbb{R}_*^{n\times m}$, $k\leq m\leq n$}}\\
					$B$-orthogonalize $X_{-1}$: $L\mapsfrom\text{chol}(X_{-1}^TBX_{-1})$ ; $X_{-1}:=X_{-1}L^{-T}$\tinyskip\\
					$(\hat{X}_0,\Lambda_0)\mapsfrom\text{RR}(A,B,X_{-1},m)$ ; $X_0:=X_{-1}\hat{X}_0$\tinyskip\\
					\textbf{for} $i=0,1,\dots$ \textbf{do}\tinyskip\\
					\hspace*{.4cm}$R_{i}:=AX_{i}-BX_{i}\Lambda_{i}$ ; $Z_i:=TR_i$\tinyskip\\
					\hspace*{.4cm}$B$-orthogonalize $Z_{i}$: $L\mapsfrom\text{chol}(Z_{i}^TBZ_{i})$ ; $Z_{i}:=Z_{i}L^{-T}$\tinyskip\\
					\hspace*{.4cm}\textbf{if} $i==0$ \textbf{then} $V_{i+1}:=[X_i,Z_i]$\\
					\hspace*{.4cm}\textbf{else}\\ 
					\hspace*{.8cm}$B$-orthogonalize $P_{i}$: $L\mapsfrom\text{chol}(P_{i}^TBP_{i})$ ; $P_{i}:=P_{i}L^{-T}$\tinyskip\\
					\hspace*{.8cm}$V_{i+1}:=[X_i,Z_i,P_{i}]$\tinyskip\\
					%\hspace*{.4cm}$Z_i\mapsfrom\texttt{Ortho}(Z_i,Q)$\\
					\hspace*{.4cm}$(\hat{X}_{i+1},\Lambda_{i+1})\mapsfrom\text{RR}(A,B,V_{i+1},m)$\tinyskip\\
					\hspace*{.4cm}\textbf{if} $i==0$ \textbf{then} $P_{i+1}:=Z_i\hat{X}_{i+1|Z_i}$ \textbf{else} $P_{i+1}:=Z_i\hat{X}_{i+1|Z_i}+P_i\hat{X}_{i+1|P_i}$\\ 					
					\hspace*{.4cm}$X_{i+1}:=X_{i}\hat{X}_{i+1|X_i}+P_{i+1}$
		\end{varwidth}}\end{center}
	\end{itemize}
	\smallskip
	\tiny{Knyazev, A. V., Argentati, M. E., Lashuk, I., \& Ovtchinnikov, E. E. (2007). Block locally optimal preconditioned eigenvalue Xolvers (BLOPEX) in Hypre and PETSc. SIAM Journal on Scientific Computing, 29(5), 2224-2239.}
\end{frame}

% Slide 35
\begin{frame}{Implementation of BLOPEX\_LOBPCG iterations}
	\begin{itemize}
		\item Making use of implicit product updates and other relations, BLOPEX\_LOBPCG iterations can be implemented as follows:
		\vspace{-.18cm}
		\begin{algorithm}[H]
			\algsetup{linenosize=\tiny}
			\tiny
			\caption{\texttt{BLOPEX\_LOBPCG}($A$, $B$, $X$, $T$, $k$) $\mapsto$ $(X,\Lambda)$}
			\begin{algorithmic}[1]
				\STATE{Allocate memory for $Z,P\in\mathbb{R}^{n\times m}$}
				\COMMENT{$X\in\mathbb{R}_*^{n\times m}$, $k\leq m\leq n$}
				\STATE{Allocate memory for $AX,AZ,AP\in\mathbb{R}^{n\times m}$}
				\STATE{Allocate memory for $BX,BZ,BP\in\mathbb{R}^{n\times m}$}
				\STATE{Compute $BX$}
				\STATE{$B$-orthogonalize $X$: $L\mapsfrom\text{chol}(X^TBX)$ ; $X:=XL^{- T}$ ; $BX:=BXL^{-T}$}
				\COMMENT{$LL^T=X^TBX$}
				\STATE{Compute $AX$}
				\STATE{$(\hat{X},\Lambda)\mapsfrom \texttt{RR}(X,AX,k)$}
				\STATE{$[X,AX,BX]:=[X\hat{X},AX\hat{X},BX\hat{X}]$}
				\COMMENT{implicit product updates}
				\FOR{$j=0,1,\dots$}
				\STATE{$Z:=AX-BX\Lambda$ ; $Z:=TZ$}
				\STATE{Compute $BZ$}
				\STATE{$B$-orthogonalize $Z$: $L\mapsfrom\text{chol}(Z^TBZ)$ ; $Z:=ZL^{- T}$ ; $BZ:=BZL^{-T}$}		
				\COMMENT{$LL^T=Z^TBZ$}
				\STATE{Compute $AZ$}		
				\IF{$i==0$}
				%\STATE{$(\hat{X},\Lambda)\mapsfrom \texttt{RR}(X,Z,AX,AZ,BX,BZ,\Lambda,k)$}
				\STATE{$(\hat{X},\Lambda)\mapsfrom \texttt{RR\_BLOPEX}(X,Z,AX,AZ,BZ)$}
				\COMMENT{$\hat{X}=[\hat{X}_X^T,\hat{X}_Z^T]^T$}
				\STATE{$[P,AP,BP]:=[Z\hat{X}_Z,AZ\hat{X}_Z,BZ\hat{X}_Z]$}
				\COMMENT{implicit product updates}
				%\STATE{$\hat{Y}\mapsfrom\text{Ortho}([0_{k\times k},\hat{X}_{Z}],I_{2k},\hat{X})$}
				\ELSE
				\STATE{$B$-orthogonalize $P$: $L\mapsfrom\text{chol}(P^TBP)$ ; $P:=PL^{- T}$ ; $BP:=BPL^{-T}$}	
				\COMMENT{$LL^T=P^TBP$}
				\STATE{$(\hat{X},\Lambda)\mapsfrom \texttt{RR\_BLOPEX}(X,Z,P,AX,AZ,AP,BZ,BP)$}
				\COMMENT{$\hat{X}=[\hat{X}_X^T,\hat{X}_Z^T,\hat{X}_P^T]^T$}
				\STATE{$[P,AP,BP]:=[Z\hat{X}_Z,AZ\hat{X}_Z,BZ\hat{X}_Z]+[P\hat{X}_P,AP\hat{X}_P,BP\hat{X}_P]$}
				\COMMENT{implicit product updates}
				\ENDIF
				\STATE{$[X,AX,BX]:=[X\hat{X}_X,AX\hat{X}_X,BX\hat{X}_X]+[P,AP,BP]$}
				\COMMENT{implicit product updates}
				\ENDFOR
			\end{algorithmic}
			\label{algo:BLOPEX_LOBPCG}
		\end{algorithm}	
	\end{itemize}	
\end{frame}

% Slide 36
\begin{frame}{Implementation of BLOPEX\_LOBPCG iterations, cont'd\textsubscript{1}}
	\begin{itemize}
		\item The Rayleigh-Ritz procedures of \texttt{BLOPEX\_LOBPCG} are as follows:
		\vspace{-.18cm}
		\begin{algorithm}[H]
			\algsetup{linenosize=\footnotesize}
			\footnotesize
			\caption{\texttt{RR}($X$, $AX$)}
			\begin{algorithmic}[1]
				\STATE{}
				\COMMENT{$X\in\mathbb{R}_*^{n\times m}$, $m\leq n$}
				\STATE{Compute $X^TAX\in\mathbb{R}^{m\times m}$}
				\STATE{Solve for $\hat{X}\in\mathbb{R}_*^{m\times m}$ and $\Lambda=\mathrm{diag}(\lambda_1,\dots,\lambda_m)$ s.t. $G_A\hat{X}=\hat{X}\Lambda$ and $\hat{X}^T\hat{X}=I_m$ where $G_A:=X^TAX$.}
				\RETURN{$\hat{X},\Lambda$}
			\end{algorithmic}
		\end{algorithm}	
		\vspace{-.35cm}
		\begin{algorithm}[H]
			\algsetup{linenosize=\footnotesize}
			\footnotesize
			\caption{\texttt{RR\_BLOPEX}($X$, $Z$, $AX$, $AZ$, $BZ$)}
			\begin{algorithmic}[1]
				\STATE{}
				\COMMENT{$[X,Z]\in\mathbb{R}_*^{n\times 2m}$, $2m\leq n$}
				\STATE{Compute $X^TAX,X^TAZ,Z^TAZ,X^TBZ\in\mathbb{R}^{m\times m}$}
				\STATE{Solve for $\hat{X}\!\in\!\mathbb{R}_*^{2m\times m}\!$ and $\Lambda=\mathrm{diag}(\lambda_1,\dots,\lambda_m)$ s.t. $\!G_A\hat{X}\!=\!G_B\hat{X}\Lambda$ and $\hat{X}^TG_B\hat{X}\!=\!I_m\hspace{-1cm}$\\
					where $G_A:=\begin{bmatrix}X^TAX&X^TAZ\\.&Z^TAZ\end{bmatrix}$, $G_B:=\begin{bmatrix}I_m&X^TBZ\\.&I_m\end{bmatrix}$ and $\lambda_1,\dots,\lambda_m$ are extremal generalized eigenvalues of $(G_A,G_B)$.}
				\RETURN{$\hat{X},\Lambda$}
			\end{algorithmic}
		\end{algorithm}	
	\end{itemize}	
\end{frame}

% Slide 37
\begin{frame}{Implementation of BLOPEX\_LOBPCG iterations, cont'd\textsubscript{2}}
	\begin{itemize}
		\item[] 
		\begin{algorithm}[H]
			\algsetup{linenosize=\footnotesize}
			\footnotesize
			\caption{\texttt{RR\_BLOPEX}($X$, $Z$, $P$, $AX$, $AZ$, $AP$, $BZ$, $BP$)}
			\begin{algorithmic}[1]
				\STATE{}
				\COMMENT{$[X,Z,P]\in\mathbb{R}_*^{n\times 3m}$, $3m\leq n$}
				\STATE{Compute $X^TAX,X^TAZ,X^TAP,Z^TAZ,Z^TAP,P^TAP\in\mathbb{R}^{m\times m}$}
				\STATE{Compute $X^TBZ,X^TBP,Z^TBP\in\mathbb{R}^{m\times m}$}
				\STATE{Solve for $\hat{X}\!\in\!\mathbb{R}_*^{3m\times m}\!$ and $\Lambda=\mathrm{diag}(\lambda_1,\dots,\lambda_m)$ s.t. $\!G_A\hat{X}\!=\!G_B\hat{X}\Lambda$ and $\hat{X}^TG_B\hat{X}\!=\!I_m\hspace{-1cm}$\\
				where 
				$G_A:=\begin{bmatrix}X^TAX&X^TAZ&X^TAP\\.&Z^TAZ&Z^TAP\\.&.&P^TAP\end{bmatrix}$, $G_B:=\begin{bmatrix}I_m&X^TBZ&X^TBP\\.&I_m&Z^TBP\\.&.&I_m\end{bmatrix}$
					and $\lambda_1,\dots,\lambda_m$ are extremal generalized eigenvalues of $(G_A,G_B)$.}
				\RETURN{$\hat{X},\Lambda$}
			\end{algorithmic}
		\end{algorithm}	
	\end{itemize}	
\end{frame}


\section{Skip\_ortho\_LOBPCG iterations (Duersch et al., 2018)}

% Slide 38
\begin{frame}{Definition of Skip\_ortho\_LOBPCG iterations}
	\begin{itemize}
	\item Duersch et al.~(2018) point out that the $B$-orthonormalization of $Z_i$ against $[X_i,P_i]$ represents a significant cost of Ortho\_LOBPCG iterations.
	\item However, the $B$-orthonormalization of $Z_i$ is not always necessary for a stable implementation of LOBPCG.
	\item Consequently, they propose to skip parts of the $B$-orthonormalization:\tinyskip
	\begin{enumerate}
	\item[1.] Start by Ortho\_LOBPCG iterations, without the $B$-orthonormalization of $Z_i$.\tinyskip
	\begin{itemize}
	\item[-] As seen with Ortho\_LOBPCG iterations, the $B$-orthonormalization of $P_{i+1}$ against $X_{i+1}$ can be done implicitly, at relatively low-cost.
	Indeed, we saw that
	\begin{align*}
	\hat{Y}_{i+1}\,\mapsfrom&\,\text{Ortho}([0_{m\times m},\hat{X}_{{i+1}|Z_i}^T,\hat{X}_{{i+1}|P_i}^T]^T,V_{i+1}^TBV_{i+1},\hat{X}_{i+1})\\
	P_{i+1}:=&\,V_{i+1}\hat{Y}_{i+1}
	\end{align*}		
	is equivalent to
	$P_{i+1}\mapsfrom\text{Ortho}(Z_i\hat{X}_{i+1|Z_i}+P_i\hat{X}_{i+1|P_i},B,X_{i+1})$ where, if $Z_i$ is $B$-orthonormal w.r.t. $\![X_i,P_i]$, we have $V_{i+1}^TBV_{i+1}=I_{3m}$.
	\item[-] For this reason, the $B$-orthonormalization of $P_{i+1}$ against $X_{i+1}$ is never skipped by Duersch et al.~(2018).\\
	Then, as long as the $B$-orthonormalization of $Z_i$ w.r.t. $\![X_i,P_i]$ is skipped, the Gram matrix $V_{i+1}^TBV_{i+1}$ is not identity, and must be accounted for a proper implicit $B$-orthonormalization of $P_{i+1}$ against $X_{i+1}$.
	\end{itemize}
	\end{enumerate}
	\end{itemize}	
	\smallskip
	\tiny{Duersch, J. A., Shao, M., Yang, C., \& Gu, M. (2018). A robust and efficient implementation of LOBPCG. SIAM Journal on Scientific Computing, 40(5), C655-C676.}
\end{frame}

% Slide 39
\begin{frame}{Definition of Skip\_ortho\_LOBPCG iterations, cont'd\textsubscript{1}}
\begin{itemize}
\item[]
\begin{enumerate}
\item[2.] For every Rayleigh-Ritz procedure, as long as $V_{i+1}$ is not $B$-orthonormal, solving the reduced eigenvalue problem requires to factorize the Gram matrix $V_{i+1}^TBV_{i+1}$.\tinyskip
\begin{itemize}
\item[-] Duersch et al.~(2018) investigate the conditioning of those factors, and then decide when to trigger the $B$-orthonormalization of $Z_{i}$ w.r.t. $\![X_i,P_i]$.\\
To do so, the reduced eigenvalue problem is scaled as follows:
\begin{align*}
1.\,&D:=\text{diag}(V_{i+1}^TBV_{i+1})^{-1/2}\\
2.\,&\text{Compute Cholesky decomposition }LL^T=DV_{i+1}^TBV_{i+1}D\\
3.\,&\text{Solve\ for\ small\ eigenpairs\ in }(\Lambda,\hat{X})\text{ s.t. }\!L^{-1}DV_{i+1}^TBV_{i+1}DL^{-T}\hat{X}=\hat{X}\Lambda\\
4.\,&\text{Form Rayleigh-Ritz vectors as }X:=V_{i+1}DL ^{-T}\hat{X}
\end{align*}
\end{itemize}
\vspace{-.1cm}
\item[3.] Duersch et al.~(2018) set a criterion on the conditioning of $L$ to decide whether to keep the basis $V_{i+1}$ as is, or not.\\
\begin{itemize}
\item[-] Since 3 triangular solves need be applied to form the Rayleigh-Ritz vectors, Duersch et al.~(2018) check if $\text{cond}(L)^{-3}$ is greater than a modest multiple of machine precision.
If so, the procedure is maintained as is.\\
Otherwise, the Rayleigh-Ritz procedure is aborted and, from that point on, $Z_i$ is always $B$-orthonormalized w.r.t. $\![X_i,P_i]$.
\end{itemize}
\end{enumerate}
\end{itemize}
\tinyskip	
\tiny{Duersch, J. A., Shao, M., Yang, C., \& Gu, M. (2018). A robust and efficient implementation of LOBPCG. SIAM Journal on Scientific Computing, 40(5), C655-C676.}
\end{frame}

% Slide 40
\begin{frame}{Definition of Skip\_ortho\_LOBPCG iterations, cont'd\textsubscript{2}}
\begin{itemize}
\item We refer to these iterations as Skip\_ortho\_LOBPCG defined as follows:
\smallskip
\hspace*{-.75cm}
\begin{center}
$\hspace{-5.5cm}${\footnotesize Skip\_ortho\_LOBPCG($A$, $B$, $X_{-1}$, $T$, $k$, $\tau_{skip}$):}\tinyskip\\
\footnotesize
$\hspace{-.5cm}$\fbox{\begin{varwidth}{17cm}
$\text{skipOrtho}:=\text{True}$\hfill{\color{gray}{$\triangleright\,X_{-1}\in \mathbb{R}_*^{n\times m}$, $k\leq m\leq n$}}\\
$(\hat{X}_0,\Lambda_0)\mapsfrom\text{RR}(A,B,X_{-1},m)$\tinyskip\\
$X_0:=X_{-1}\hat{X}_0$ ; $R_{0}:=AX_{0}-BX_{0}\Lambda_{0}\,$\tinyskip\\
\textbf{for} $i=0,1,\dots$ \textbf{do}\tinyskip\\
%\hspace*{.4cm}\textbf{if} $\text{skipOrtho}$ \textbf{then} $Z_i:=T^{-1}R_i$\tinyskip\\
\hspace*{.4cm}$Z_i:=TR_i$\tinyskip\\
%\hspace*{.4cm}\textbf{else}\tinyskip\\
\hspace*{.4cm}\textbf{if} $\text{skipOrtho}$\tinyskip\\
\hspace*{.8cm}\textbf{if} $i==0$ \textbf{then} $Z_i\mapsfrom\text{Ortho}(Z_i,B,X_i)$ \textbf{else} $Z_i\mapsfrom\text{Ortho}(Z_i,B,[X_i,P_i])$\tinyskip\\
\hspace*{.4cm}\textbf{if} $i==0$ \textbf{then} $V_{i+1}:=[X_i,Z_i]$ \textbf{else} $V_{i+1}:=[X_i,Z_i,P_i]$ \tinyskip\\
\hspace*{.4cm}$(\hat{X}_{i+1},\Lambda_{i+1})\mapsfrom\text{RR}(A,B,V_{i+1},m)$
\hfill{\color{gray}{$\triangleright\,L$ is a by-product s.t. $\!LL^T=DV_{i+1}^TBV_{i+1}D$}}\tinyskip\\
\hspace*{.4cm}\textbf{if} skipOrtho \textbf{then} \textbf{if} $\text{cond}(L)^{-3}<\tau_{skip}$ \textbf{then} $\text{skipOrtho}:=\text{False}$ ; restart $i$-th iteration\tinyskip\\
\hspace*{.4cm}$X_{i+1}:=V_{i+1}\hat{X}_{i+1}$; $R_{i+1}:=AX_{i+1}-BX_{i+1}\Lambda_{i+1}$\tinyskip\\
\hspace*{.4cm}\textbf{if} $i==0$ \textbf{then}\\
\hspace*{.8cm}$\hat{Y}_{i+1}\mapsfrom\text{Ortho}([0_{m\times m},\hat{X}_{i+1|Z_i}^T]^T,V_{i+1}^TBV_{i+1},\hat{X}_{i+1})$\\
\hspace*{.4cm}\textbf{else}\\
\hspace*{.8cm}$\hat{Y}_{i+1}\mapsfrom\text{Ortho}([0_{m\times m},\hat{X}_{i+1|Z_i}^T,\hat{X}_{i+1|P_i}^T]^T,V_{i+1}^TBV_{i+1},\hat{X}_{i+1})$\\
\hspace*{.4cm}$P_{i+1}:=V_{i+1}\hat{Y}_{i+1}$
\end{varwidth}}\end{center}
\end{itemize}
\end{frame}

% Slide 41
\begin{frame}{Implementation of Skip\_ortho\_LOBPCG iterations}
%Skip\_ortho\_LOBPCG can be implemented as follows:
\vspace{-.25cm}
\begin{algorithm}[H]
\algsetup{linenosize=\tiny}
\tiny
\caption{\texttt{Skip\_ortho\_LOBPCG}($A$, $B$, $X$, $T$, $k$, $\tau_{skip}$) $\mapsto$ $(X,\Lambda)$}
\begin{algorithmic}[1]
\STATE{Allocate memory for $Z,P\in\mathbb{R}^{n\times m}$}
\COMMENT{$X\in\mathbb{R}_*^{n\times m}$, $k\leq m\leq n$}
\STATE{Allocate memory for $AX,AZ,AP,BX,BZ,BP\in\mathbb{R}^{n\times m}$}
\STATE{$\text{skipOrtho}:=\text{True}$}
\STATE{Compute $AX,BX$}
\STATE{$(\hat{X},\Lambda)\mapsfrom \texttt{RR}(X,AX,BX)$}
\STATE{$[X,AX,BX]:=[X\hat{X},AX\hat{X},BX\hat{X}]$}
\COMMENT{implicit product updates}
\STATE{$Z:=AX-BX\Lambda$}
\FOR{$j=0,1,\dots$}
\STATE{$Z:=TZ$}
\IF{not skipOrtho}
\STATE{\textbf{if} $j==0$ \textbf{then} $Z\mapsfrom\text{Ortho}(Z,B,X)$ \textbf{else} $Z\mapsfrom\text{Ortho}(Z,B,[X,P])$ }
\ENDIF
\STATE{Compute $AZ,BZ$}
\IF{skipOrtho}
\STATE{\textbf{if} $i==0$ \textbf{then} $(\hat{X},\Lambda)\mapsfrom \texttt{RR}(X,Z,AX,AZ,BZ)$ \textbf{else} $(\hat{X},\Lambda)\mapsfrom \texttt{RR}(X,Z,P,AX,AZ,AP,BZ,BP)$}
\ELSE
\STATE{\textbf{if} $i==0$ \textbf{then} $(\hat{X},\Lambda)\mapsfrom \texttt{RR}(X,Z,AX,AZ)$ \textbf{else} $(\hat{X},\Lambda)\mapsfrom \texttt{RR}(X,Z,P,AX,AZ,AP)$}
\ENDIF
\STATE{\textbf{if} skipOrtho \textbf{then} \textbf{if} $\text{cond}(L)^{-3}<\tau_{skip}$ \textbf{then} $\text{skipOrtho}:=\text{False}$ ; go to 13}
\IF{$i==0$}
%\STATE{$(\hat{X},\Lambda)\mapsfrom \texttt{RR}(X,Z,AX,AZ,BX,BZ,\Lambda,k)$}
%\STATE{$(\hat{X},\Lambda)\mapsfrom \texttt{RR}(X,Z,AZ,\Lambda,k)$}
%\COMMENT{$\hat{X}=[\hat{X}_X^T,\hat{X}_Z^T]^T$}
\STATE{$[AX,BX]:=[AX\hat{X}_X,BX\hat{X}_X]+[AZ\hat{X}_Z,BZ\hat{X}_Z]$}
\COMMENT{implicit product updates}
\STATE{$\hat{Y}\mapsfrom\text{Ortho}([0_{m\times m},\hat{X}_{Z}^T]^T,V_{i+1}^TBV_{i+1},\hat{X})$}
\STATE{$[AP,BP]:=[AX\hat{Y}_X,BX\hat{Y}_X]+[AZ\hat{Y}_Z,BZ\hat{Y}_Z]$}
\COMMENT{implicit product updates}
\STATE{$AZ:=[X,Z]\hat{X}$ ; $P:=[X,Z]\hat{Y}$ ; $X:=AZ$}
\ELSE
%\STATE{$(\hat{X},\Lambda)\mapsfrom \texttt{RR}(X,Z,P,AX,AZ,BX,BZ,AP,BP,\Lambda,k)$}
%\STATE{$(\hat{X},\Lambda)\mapsfrom \texttt{RR}(X,Z,P,AZ,AP,\Lambda,k)$}
%\COMMENT{$\hat{X}=[\hat{X}_X^T,\hat{X}_Z^T,\hat{X}_P^T]^T$}		
%\STATE{$P:=Z\hat{X}_2+P\hat{X}_3$}
\STATE{$[AX,BX]:=[AX\hat{X}_X,BX\hat{X}_X]+[AZ\hat{X}_Z,BZ\hat{X}_Z]+[AP\hat{X}_P,BP\hat{X}_P]$}
\COMMENT{implicit product updates}
\STATE{$\hat{Y}\mapsfrom\text{Ortho}([0_{m\times m},\hat{X}_{Z}^T,\hat{X}_P^T]^T,V_{i+1}^TBV_{i+1},\hat{X})$}
\STATE{$[AP,BP]:=[AX\hat{Y}_X,BX\hat{Y}_X]+[AZ\hat{Y}_Z,BZ\hat{Y}_Z]+[AP\hat{Y}_P,BP\hat{Y}_P]$}
\COMMENT{implicit product updates}
\STATE{$AZ:=[X,Z,P]\hat{X}$ ; $P:=[X,Z,P]\hat{Y}$ ; $X:=AZ$}
\ENDIF
%\STATE{Compute $AP,BP$}
\STATE{$Z:=AX-BX\Lambda$}	
\ENDFOR
\end{algorithmic}
\label{algo:Skip_ortho_LOBPCG}
\end{algorithm}	
\end{frame}

% Slide 42
\begin{frame}{Improvement of stability}
	\begin{itemize}
	\item Even though Ortho\_LOBPCG is more stable than BLOPEX\_LOBPCG, Duersch et al.~(2018) further improve the stability of Ortho\_LOBPCG by:
	\begin{itemize}
	\item\underline{Basis truncation}:\tinyskip\\
	The $B$-orthogonalization of $Z_i$ against $[X_i,P_i]$ using the SVQB procedure can sometimes yield a poorly conditioned $V_{i+1}$, even when $B:=I_n$.\smallskip\\
	To circumvent this issue, the basis generated by the SVQB procedure is truncated by discarding contributions below a round-off error threshold to represent the Gram matrix $Z^TBZ$, as previously done in Sathopoulos \& Wu~(2002) and Yamamoto et al.~(2015).\smallskip\\
	However, in order to deploy basis truncation, one needs to accommodate for $\text{Orto}(Z,B,[X,P])$ to return a variable number of $B$-othonormalized preconditioned residuals, unless artificially maintaining the rank of $Z^TBZ$'s decomposition to a prescribed level.
	\end{itemize}
	\end{itemize}	
	\smallskip
	\tiny{Duersch, J. A., Shao, M., Yang, C., \& Gu, M. (2018). A robust and efficient implementation of LOBPCG. SIAM Journal on Scientific Computing, 40(5), C655-C676.}\tinyskip\\
	\tiny{Stathopoulos, A., \& Wu, K. (2002). A block orthogonalization procedure with constant synchronization requirements. SIAM Journal on Scientific Computing, 23(6), 2165-2182.}\tinyskip\\
	\tiny{Yamamoto, Y., Nakatsukasa, Y., Yanagisawa, Y., \& Fukaya, T. (2015). Roundoff error analysis of the CholeskyQR2 algorithm. Electron. Trans. Numer. Anal, 44(01), 306-326.}
\end{frame}

% Slide 43
\begin{frame}{Improvement of stability, cont'd\textsubscript{1}}
	\begin{itemize}
		\item Even though Ortho\_LOBPCG is more stable than BLOPEX\_LOBPCG, Duersch et al.~(2018) further improve the stability of Ortho\_LOBPCG by:
		\begin{itemize}
			\item\underline{Improved basis selection strategy}:\tinyskip\\
			Changes can also made to the way $[X_i,P_i]$ is implicitly $B$-orthogonalized, 
			in order to improve both stability and performance.\tinyskip\\
			Let us consider the case when $V$ is $B$-orthonormal, so that the Raleigh-Ritz procedure relies on solving the eigendecomposition $\hat{Z}\Theta\hat{Z}^T=V^TAV$ with the block structure given by\vspace{-.35cm}
			\begin{align*}
			\hat{Z}=[\hat{X},\hat{X}_\perp]
			\,,\;\,
			\Theta=\text{diag}(\Lambda,\Lambda_\perp)
			\;\,\text{where}\;\,
			\hat{X}=\begin{bmatrix}\hat{X}_X\\\hat{X}_Z\\\hat{X}_P\end{bmatrix}
			\;\,\text{and}\;\,
			\hat{X}_\perp=\begin{bmatrix}\hat{X}_{\perp|X}\\\hat{X}_{\perp|Z}\\\hat{X}_{\perp|P}\end{bmatrix}.
			\end{align*}
		    Then, the $B$-orthonormalization of $[X,P]$ is done implicitly by letting $P:=V\hat{Y}$, in which $\hat{Y}$ is such that $\hat{Y}^T\hat{Y}=I_m$, $\hat{Y}^T\hat{X}=0_{m\times m}$ and
		    \begin{align*}
		    \mathcal{R}([0_{m\times m},\hat{X}_Z^T,\hat{X}_P^T]^T)\subseteq\mathcal{R}(\hat{Y}).
		    \end{align*}
	        $\vspace*{-.7cm}$\\
	    	This can be achieved by letting $\hat{Y}$ be a normalized $(I_{3m}-\hat{X}\hat{X}^T)\begin{bmatrix}0_{m\times m}\\\hat{X}_Z\\\hat{X}_P\end{bmatrix}$.
		\end{itemize}
	\end{itemize}	
	\medskip
	\tiny{Duersch, J. A., Shao, M., Yang, C., \& Gu, M. (2018). A robust and efficient implementation of LOBPCG. SIAM Journal on Scientific Computing, 40(5), C655-C676.}
\end{frame}

% Slide 44
\begin{frame}{Improvement of stability, cont'd\textsubscript{2}}
	\begin{itemize}
		\item Even though Ortho\_LOBPCG is more stable than BLOPEX\_LOBPCG, Duersch et al.~(2018) further improve the stability of Ortho\_LOBPCG by:
		\begin{itemize}
			\item\underline{Improved basis selection strategy}, cont'd:\tinyskip\\
			But since we have $\hat{X}\hat{X}^T+\hat{X}_\perp\hat{X}\perp^T=I_3m$, we can actually focus on 
			\begin{align*}
			\hat{X}_\perp\hat{X}_\perp^T\begin{bmatrix}0_{m\times m}\\\hat{X}_Z\\\hat{X}_P\end{bmatrix}
			=&\;
			\hat{X}_\perp[\hat{X}_{\perp|X}^T,\hat{X}_{\perp|Z}^T,\hat{X}_{\perp|P}^T]\begin{bmatrix}0_{m\times m}\\\hat{X}_Z\\\hat{X}_P\end{bmatrix}\\
			=&\;
			\hat{X}_\perp\left(\hat{X}_{\perp|Z}^T\hat{X}_Z+\hat{X}_{\perp|P}^T\hat{X}_P\right)\\
			=&\;
			-\hat{X}_\perp\hat{X}_{\perp|X}^T\hat{X}_X
			\end{align*}
		\end{itemize}
	where $\hat{X}_\perp$ is orthonormal and $\hat{X}_{\perp|X}^T\in\mathbb{R}^{2m\times m}$, so that it suffices to let 
	\begin{align*}
	\hat{Y}:=\hat{X}_\perp Q_{\perp|X}^T
	\end{align*} 
    where $Q^T_{\perp|X}$ is the $Q$-factor of a QR decomposition $Q^T_{\perp|X}L^T_{\perp|X}$ of $X^T_{\perp|X}$.
	\end{itemize}	
	\medskip
	\tiny{Duersch, J. A., Shao, M., Yang, C., \& Gu, M. (2018). A robust and efficient implementation of LOBPCG. SIAM Journal on Scientific Computing, 40(5), C655-C676.}
\end{frame}

% Slide 45
\begin{frame}{Improvement of convergence detection}
	\label{convergence}
	\begin{itemize}
	\item Most commonly, the convergence of an approximate eigenpair $(\lambda_i,x_i)$ is determined by a stopping criterion of the form
	\begin{align*}
	\frac{\|r_i\|}{|\lambda_i|\|x_i\|_B}=\frac{\|Ax_i-\lambda_iBx_i\|}{|\lambda_i|\|x_i\|_B}\leq\tau.
	\end{align*}
	However, as explained in Duersch et al.~(2018), difficulties can occur when diagnosing convergence of iterative eigensolvers with this criterion:\tinyskip\\
	\begin{enumerate}
	\item[1.] For generalized problems, i.e., when $B\neq I_n$, the lack of scaling invariance of the stopping criterion makes convergence detection somewhat arbitrary.\tinyskip\\
	In particular, the smaller the matrix norm $\|B\|_2$, the more premature the convergence detection may be.\tinyskip
	\item[2.] Even for standard problems, i.e., when $B=I_n$, if the magnitude of an approximate eigenvalue $\lambda_i$ is too small compared to the matrix norm $\|A\|_2$, the eigenresidual norm $\|r_i\|_2$ cannot be computed in floating point arithmetic to satisfy the stopping criterion, in which case convergence may not be detected.
	\end{enumerate}
	\end{itemize}
	\smallskip
	\tiny{Duersch, J. A., Shao, M., Yang, C., \& Gu, M. (2018). A robust and efficient implementation of LOBPCG. SIAM Journal on Scientific Computing, 40(5), C655-C676.}
\end{frame}

% Slide 46
\begin{frame}{Improvement of convergence detection, cont'd}
	\begin{itemize}
	\item As a means to circumvent issues 1. and 2., Duersch et al.~(2018) use the backward stable criterion
	\begin{align*}
	\frac{\|r_i\|_2}{(\|A\|_2+|\lambda_i|\|B\|_2)\|x_i\|_2}=\frac{\|Ax_i-\lambda_iBx_i\|_2}{(\|A\|_2+|\lambda_i|\|B\|_2)\|x_i\|_2}\leq\tau.
	\end{align*}
	Since evaluating the matrix 2-norms $\|A\|_2$ and $\|B\|_2$ can be costly, Duersch et al.~(2018) suggest to rely on random sketching $x\mapsto\Omega x$ s.t.
	\begin{align*}
	\|A\|_2^{(\Omega)}:=
	\frac{\|\Omega A\|_F}{\|\Omega\|_F}\leq\|A\|_2.
	\end{align*}
	Then, convergence is monitored with the following criterion instead:
	\begin{align*}
	\frac{\|r_i\|_2}{(\|A\|_2+|\lambda_i|\|B\|_2)\|x_i\|_2}
	\leq
	\frac{\|r_i\|_2}{(\|A\|_2^{(\Omega)}+|\lambda_i|\|B\|_2^{(\Omega)})\|x_i\|_2}\leq\tau.
	\end{align*}
	\end{itemize}
	\smallskip
	\tiny{Duersch, J. A., Shao, M., Yang, C., \& Gu, M. (2018). A robust and efficient implementation of LOBPCG. SIAM Journal on Scientific Computing, 40(5), C655-C676.}
\end{frame}

\section{Monitoring and handling convergence}

% Slide 47
\begin{frame}{Handling convergence}
\begin{itemize}
\item Irrespective of the criterion used, see~{\color{blue}{\underline{\hyperlink{convergence}{slide on convergence detection}}}}, different eigenvectors may converge at different stages of the iteration.
Maintaining converged eigenvectors to perform subsequent iterations
\begin{enumerate}
\item requires unnecessary computational work,
\item can lead to instabilities.
\item[$\implies$] A robust and efficient implementation of LOBPCG needs to detect, and properly handle converged eigenvectors.
\end{enumerate}	
\item Two approaches possible, see Knyazev (2004) and Knyazev et al. (2007):
\begin{itemize}
\item \textbf{Hard locking}: converged eigenvectors are set aside, kept unchanged, and $B$-orthogonalized against by the non-converged, still iterated eigenvectors.
\begin{itemize}
\item As the number of hard locked vectors increases, the attainable accuracy of the iterated eigenvectors may decrease, possibly making convergence unachievable.
\end{itemize}
\item \textbf{Soft locking}: the residuals and search directions of converged eigenvectors are set aside, and kept unchanged, but the corresponding locked eigenvectors still participate to subsequent Rayleigh-Ritz procedures.
\begin{itemize}
\item The locked eigenpairs keep getting more accurate over subsequent iterations, and the $B$-orthogonality is maintained implicitly through the Rayleigh-Ritz procedures.
\end{itemize}
\end{itemize}
\end{itemize}		
\medskip
\tiny{Knyazev, A. V. (2004). Hard and soft locking in iterative methods for symmetric eigenvalue problems. In Presentation at the eighth copper mountain conference on iterative methods.}\tinyskip\\
\tiny{Knyazev, A. V., Argentati, M. E., Lashuk, I., \& Ovtchinnikov, E. E. (2007). Block locally optimal preconditioned eigenvalue Xolvers (BLOPEX) in Hypre and PETSc. SIAM Journal on Scientific Computing, 29(5), 2224-2239.}
\end{frame}

% Slide 48
\begin{frame}{Handling convergence, cont'd}
\begin{itemize}
\item Soft locking is more computationally demanding than hard locking, but it enables more robust convergence behaviors when more accurate solutions are needed.
\item In practice, convergence may be detected in unordered fashions, i.e., the inner eigenpairs converge before the smallest eigenpairs.
\item We denote two distinct approaches to deal with this situation:
\begin{itemize}
	\item \textbf{out-of-order locking}: if locking is implemented out of order, one needs to re-order the stored iterates so as to seamlessly rely on standard BLAS libraries, which operate most efficiently on contiguous data.
	\item \textbf{in-order locking}: more commonly in practice, locking is implemented in order, disregarding the fact that some inner eigenpairs may converge before the sought least dominant eigenpairs.\\
	\begin{itemize}
	\item[-]Maintaining such unlocked but converged eigenvectors in the iterations can lead to unstable behaviors of LOBPCG.
	\end{itemize}
\end{itemize}
\item The theoretical underpinnings of locking, particularly out-of-order locking, are not well studied.
\end{itemize}		
\smallskip
\tiny{Knyazev, A. V. (2004). Hard and soft locking in iterative methods for symmetric eigenvalue problems. In Presentation at the eighth copper mountain conference on iterative methods.}\tinyskip\\
\tiny{Knyazev, A. V., Argentati, M. E., Lashuk, I., \& Ovtchinnikov, E. E. (2007). Block locally optimal preconditioned eigenvalue Xolvers (BLOPEX) in Hypre and PETSc. SIAM Journal on Scientific Computing, 29(5), 2224-2239.}
\end{frame}

\section{Landscape of existing software}

% Slide 49
\begin{frame}{Existing implementations of LOBPCG}
	Different implementations of LOBPCG have been developed over the years.
	In particular, we know of implementations and bindings in the following libraries:\smallskip
	\begin{itemize}
	\item {\color{blue}{\underline{\href{https://github.com/lobpcg/blopex}{BLOPEX}}}}: C implementation with MPI support after Knyazev et al.~(2007).$\hspace{-.5cm}$\\
	On GitHub at \texttt{lobpcg/blopex}.\tinyskip
	\begin{itemize}
	\item BLOPEX also available in 
	{\color{blue}{\underline{\href{https://www.mathworks.com/matlabcentral/fileexchange/48-locally-optimal-block-preconditioned-conjugate-gradient}{Matlab}}}},
	{\color{blue}{\underline{\href{https://slepc.upv.es/documentation/current/src/eps/impls/cg/lobpcg/lobpcg.c}{SLEPc}}}}
	and
	{\color{blue}{\underline{\href{https://hypre.readthedocs.io/en/latest/solvers-lobpcg.html}{Hypre}}}}.
	\end{itemize}\tinyskip
	\item {\color{blue}{\underline{\href{https://github.com/CEED/MAGMA/blob/master/sparse/src/zlobpcg.cpp}{MAGMA}}}}: C++ implementation based on BLOPEX for $B:=I_n$ and $T^{-1}:=I_n$ with GPU support. 
	On GitHub at
	\begin{center}\texttt{CEED/MAGMA/sparse/src/zlobpcg.cpp}\end{center}\tinyskip
	\item {\color{blue}{\underline{\href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lobpcg.html}{SciPy}}}}: Python implementation based on BLOPEX.
	On GitHub at\\\tinyskip
	\begin{center}\texttt{scipy/sparse/linalg/eigen/lobpcg/lobpcg.py}\end{center}\tinyskip
	\item {\color{blue}{\underline{\href{https://iterativesolvers.julialinearalgebra.org/stable/eigenproblems/lobpcg/}{IterativeSolvers.jl}}}}: Julia implementation based on BLOPEX with multithreaded BLAS support. 
	On GitHub at
	\begin{center}\texttt{JuliaLinearAlgebra/IterativeSolvers.jl/src/lobpcg.jl}\end{center}
	\end{itemize}	
	\medskip
	\tiny{Knyazev, A. V., Argentati, M. E., Lashuk, I., \& Ovtchinnikov, E. E. (2007). Block locally optimal preconditioned eigenvalue Xolvers (BLOPEX) in Hypre and PETSc. SIAM Journal on Scientific Computing, 29(5), 2224-2239.}
\end{frame}

% Slide 50
\begin{frame}{Existing implementations of LOBPCG, cont'd}
	\begin{itemize}
		\item BLOPEX from Knyazev et al.~(2007) has become the most common reference among widely used implementations of LOBPCG.
		\item At the moment, there seems to be no widely used implementations of 
		\begin{itemize}
			\item \texttt{Ortho\_LOBPCG} (Hetmaniuk and Lehoucq, 2006)\tinyskip
			\item \texttt{Skip\_ortho\_LOBPCG} (Duersch et al., 2018)\tinyskip
			\item Mixed precision LOBPCG (Kressner et al., 2023)\tinyskip
			\item Randomized LOBPCG (Xiang, 2024)
		\end{itemize}
	\end{itemize}
	\medskip
	\tiny{Knyazev, A. V., Argentati, M. E., Lashuk, I., \& Ovtchinnikov, E. E. (2007). Block locally optimal preconditioned eigenvalue Xolvers (BLOPEX) in Hypre and PETSc. SIAM Journal on Scientific Computing, 29(5), 2224-2239.}\tinyskip\\	
	\tiny{Hetmaniuk, U., \& Lehoucq, R. (2006). Basis selection in LOBPCG. Journal of Computational Physics, 218(1), 324-332.}\tinyskip\\
	\tiny{Duersch, J. A., Shao, M., Yang, C., \& Gu, M. (2018). A robust and efficient implementation of LOBPCG. SIAM Journal on Scientific Computing, 40(5), C655-C676.}\tinyskip\\
	\tiny{Kressner, D., Ma, Y., \& Shao, M. (2023). A mixed precision LOBPCG algorithm. Numerical Algorithms, 94(4), 1653-1671.}\tinyskip\\
	\tiny{Xiang, Y. (2024). Randomized LOBPCG algorithm with dimension reduction maps. Technical Report of Inria, hal-04517617.}
\end{frame}	

\section{Homework problems}
% Slide 51
\begin{frame}{Homework problem}\vspace{.1cm}
Turn in \textbf{your own} solution to \textbf{Pb.$\,$21}:\vspace{.15cm}\\
\begin{minipage}[t]{0.1\textwidth}
\textbf{Pb.$\,$21}
\end{minipage}
\begin{minipage}[t]{0.89\textwidth}
Show that the gradient of the generalized Rayleigh quotient of a symmetric pencil $(A,B)$ with symmetric $A$ and SPD $B$ given by
\begin{align*}
\rho(x)=\frac{x^TAx}{x^TBx}\text{ for }x\neq 0
\end{align*}
is $\nabla\rho(x)=\frac{2}{x^TBx}(Ax-\rho(x)Bx)$.
\end{minipage}\vspace{.15cm}

\begin{minipage}[t]{0.1\textwidth}
\textbf{Pb.$\,$22}
\end{minipage}
\begin{minipage}[t]{0.89\textwidth}
Let $A$ be symmetric and $B$ be SPD.
Then show that the matrix $X=[x_1,\dots,x_k]$ of the smallest general eigenvectors $x_1,\dots,x_k$ of $(A,B)$ is such that $\text{trace}(X^TAX)$ is minimized, subjected to $X^TBX=I_k$.
\end{minipage}
\end{frame}

\section{Practice session}
% Slide 52
\begin{frame}[fragile]{Practice session}
\begin{enumerate}
\item Implement Basic\_LOBPCG.
\item Implement BLOPEX\_LOBPCG.
\item Implement Ortho\_LOBPCG.
\item Implement Skip\_ortho\_LOBPCG.
\item Use the matrices \texttt{bcsstk12} and \texttt{bcsstm12} from the SuiteSparse collection for $A$ and $B$, respectively.
Then, using $m=200$, benchmark each algorithm by monitoring the convergence of the 10 smallest generalized eigenvalues, first with $\tau=10^{-5}$, then with $\tau=10^{-6}$.
Use a preconditioner.
What do you observe?
\end{enumerate}
\end{frame}



\end{document}















