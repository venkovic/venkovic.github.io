\documentclass[t,usepdftitle=false]{beamer}

\input{~/Dropbox/Git/tex-beamer-custom/preamble.tex}

\title[NLA for CS and IE -- Lecture 07]{Numerical Linear Algebra\\for Computational Science and Information Engineering}
\subtitle{\vspace{.3cm}Lecture 07\\Orthogonalization and Least-Squares Problems}
\hypersetup{pdftitle={NLA-for-CS-and-IE\_Lecture07}}

\date[Summer 2025]{Summer 2025}

\author[nicolas.venkovic@tum.de]{Nicolas Venkovic\\{\small nicolas.venkovic@tum.de}}
\institute[]{Group of Computational Mathematics\\School of Computation, Information and Technology\\Technical University of Munich}

\titlegraphic{\vspace{0cm}\includegraphics[height=1.1cm]{~/Dropbox/Git/logos/TUM-logo.png}}

\begin{document}
	
\begin{frame}[noframenumbering, plain]
	\maketitle
\end{frame}
	
\myoutlineframe
	
\section{QR factorization\\{\small Section 4 in Darve \& Wootters (2021)}}

% Slide 01
\begin{frame}{QR factorization}
\begin{itemize}
\item A \textbf{QR factorization decomposes} a matrix as the \textbf{product of an orthogonal matrix $Q$ with an upper-triangular matrix $R$}. 
\item Recall that a matrix $Q$ is orthogonal if $Q^TQ=I$.
\item If a matrix is orthogonal, then $\|Qx\|_2=\|x\|_2$ for all $x$, i.e., $Q$ doesn't change the length of vectors.\vspace{.1cm}\\
The operations that do not change the length of vectors are \textbf{rotations} and \textbf{reflections}, so an orthogonal matrix can be thought of as a map that combines a rotation with a reflection.
\begin{center}
\vspace{.3cm}
\includegraphics[height=3cm]{images/rotation-reflection.png}
\end{center}
\end{itemize}
\end{frame}

% Slide 02
\begin{frame}{QR factorization, cont'd}
\begin{block}{QR factorization}
\begin{itemize}
\item[-] Let $A$ be a real $m\times n$ matrix with $m\geq n$.
Then, there is an orthogonal matrix $Q$ and an upper-triangular matrix $R$ such that $A=QR$.
This is called the \textbf{QR factorization}.
\item[-] When $A$ is complex, there is still a factorization $A=QR$, but $Q$ is unitary, i.e., $Q^HQ=I$.
\end{itemize}
\end{block}
For the rest this lecture, we assume $A$ is real, but QR factorizations do exist for complex matrices.
\begin{itemize}
\item There are different forms of QR factorizations, depending on the $\!$shape $\!$of $\!A$:$\hspace{-1cm}$
\end{itemize}
\begin{center}
\vspace{.2cm}
\includegraphics[height=4cm]{images/qr-shapes.png}
\end{center}
\end{frame}

% Slide 03
\begin{frame}{Applications of the QR factorization}
The QR factorization has several applications in numerical linear algebra:
\begin{enumerate}
\item It can be used to solve least-squares problems, i.e., problems of the form $\arg\min_x\|Ax-b\|_2$ where $A$ is tall and skiny.
\item It is used as part of eigen- and singular value algorithms for small dense matrices.
\item It is also used in iterative methods to solve linear systems and compute eigenvalues, such as in Krylov methods.
\end{enumerate}
\end{frame}

% Slide 04
\begin{frame}{QR factorization and least-squares problems}
To see why the QR factorization can be useful, let's look briefly at the least-squares problem:
\begin{itemize}
\item Let $A\in\mathbb{R}^{m\times n}$ with $m>n$.
We want to find $x$ such that $Ax$ is closest to $b$ in Euclidean distance.
That is 
\begin{align*}
x^*=\arg\min_x\|Ax-b\|_2.
\end{align*}
To do this, we use the QR factorization, with a square $Q$, i.e., case $\mathrm{III}$ from slide \#2:
\begin{align*}
\|Ax-b\|_2=
\|Q^T(Ax-b)\|_2=
\|Q^T(QRx-b)\|_2=
\|Rx-Q^Tb\|_2
\end{align*}
where we used the fact that for any vector $y$, $\|Q^Ty\|_2=\|y\|_2$ because
\begin{align*}
\hspace{-.6cm}\|Q^Ty\|^2_2=(Q^Ty)^T(Q^Ty)=y^TQQ^Ty=y^T(Q^TQ)^Ty=y^TI^Ty=y^Ty=\|y\|_2^2.
\end{align*}
As it turns out, it is easier to find $x$ that minimizes $\|Rx-Q^Tb\|_2$ than it is to minimize $\|Ax-b\|_2$.
\end{itemize}
\end{frame}

\section{Householder reflections\\{\small Section 4.1 in Darve \& Wootters (2021)}}

% Slide 05
\begin{frame}{Householder reflections}
\begin{itemize}
\item Householder reflections are one of the most reliable methods to compute a QR factorization with a square $Q$, i.e., cases $\mathrm{I}$ and $\mathrm{III}$.
\item That is, we ask the question, does there exists a matrix $Q$ s.t. $Q^TA=R$.
\item Our goal is thus to create zero entries below the diagonal. 
Starting by the first column, we have:
\begin{center}
\vspace{-.1cm}
\includegraphics[height=2cm]{images/householder-first-column.png}
\end{center}
\item We need to apply an orthogonal transformation $Q_1^T$ to transform the first column of $A$ into a vector in the direction of $e_1$.\vspace{.1cm}\\
Let's write $A=[a_1|\dots|a_n]$.
Then, since $Q_1^T$ does not change the norm of $a_1$, we should have:
\begin{center}
\vspace{-.1cm}
\includegraphics[height=1.75cm]{images/householder-first-column-2.png}
\end{center}
\end{itemize}
\end{frame}

% Slide 06
\begin{frame}{Householder reflections, cont'd\textsubscript{1}}
\begin{itemize}
\item A logical choice for $Q_1^T$ would be a rotation that maps $a_1$ parallel to $e_1$.\vspace{.1cm}\\
However, rotations in high dimensions are not so easy to set up.\vspace{.1cm}\\
Thus, we'll instead choose $Q_1^T$ to be a \textbf{reflection} that maps $a_1$ parallel to $e_1$:
\begin{center}
\vspace{.1cm}
\includegraphics[height=3.5cm]{images/reflection.png}
\end{center}
\vspace{.2cm}
Now that we have an idea of what the reflection should be doing, we need to figure out its mathematical formula.
\end{itemize}
\end{frame}

% Slide 07
\begin{frame}{Householder reflections, cont'd\textsubscript{2}}
\begin{itemize}
\item Let us consider reflections in general.
A reflection is defined by a vector:
\begin{center}
\vspace{.1cm}
\includegraphics[height=2cm]{images/vector-v.png}
\end{center}
\vspace{.2cm}
Given $v$, we can reason geometrically about what a reflection is:\vspace{.2cm}
\begin{center}
\vspace{.1cm}
\includegraphics[height=4cm]{images/target.png}
\end{center}
\end{itemize}
\end{frame}

% Slide 08
\begin{frame}{Householder reflections, cont'd\textsubscript{3}}
\begin{block}{Reflection}
Let $P$ be the matrix which represents a reflection over the hyperplane orthogonal to some vector $v$.
Then $P$ is given by
\begin{align*}
P=I-\beta vv^T
\text{  where  }
\beta=\frac{2}{v^Tv}.
\end{align*}
\end{block}
\begin{itemize}
\item Now we need to pick $v$ to arrive at a Householder reflection, i.e., to get a transformation from $x$ to $\|x\|_2e_1$.\vspace{.1cm}\\
The following geometric argument shows that $v=x-\|x\|_2e_1$ will work
\begin{center}
\vspace{.1cm}
\includegraphics[height=3cm]{images/v-vector-householder-reflection.png}
\end{center}
\end{itemize}
\end{frame}

% Slide 09
\begin{frame}{Householder reflections, cont'd\textsubscript{3}}
Let $v=x-\|x\|_2e_1$, then we see that
\begin{align*}
Px
=&\,\left(I-2\frac{vv^T}{v^Tv}\right)x\\
=&\,\left(I-2\frac{(x-\|x\|_2e_1)(x-\|x\|_2e_1)^T}{(x-\|x\|_2e_1)^T(x-\|x\|_2e_1)}\right)x\\
=&\,\left(I-2\frac{(x-\|x\|_2e_1)(x-\|x\|_2e_1)^T}{2(\|x\|^2_2-2\|x\|_2x_1)}\right)x\\
=&\,x-\frac{(x-\|x\|_2e_1)(\|x\|_2^2-\|x\|_2x_1)}{(\|x\|^2_2-2\|x\|_2x_1)}\\
=&\,x-(x-\|x\|_2e_1)\\
=&\,\|x\|_2e_1
\end{align*}
so that, indeed, a reflection over the hyperplane orthogonal $\!$to $\!v=x-\|x\|_2e_1$,$\hspace{-1cm}$\\ is a Householder reflection of $x$.
\end{frame}

% Slide 10
\begin{frame}{Iterating Householder reflections}
\begin{itemize}
\item Now that we know how to operate a first Householder reflection from $a_1$ to $\|a_1\|_2e_1$, we can apply a series of Householder transformations to progressively reduce $A$ to a upper-triangular form.\vspace{.1cm}\\
We proceed by first zeroing entries in the first column, then in the second column, and so on.\vspace{.1cm}\\
In the end, for $A\in\mathbb{R}^{m\times n}$ with $m\geq n$, we have
\begin{align*}
Q_{n-1}^T\dots Q_1^TA=R,
\end{align*}
which is equivalent to 
\begin{align*}
A=Q_1\dots Q_{n-1}R=QR
\end{align*}
where $Q=Q_1\dots Q_{n-1}\in\mathbb{R}^{m\times m}$, and $R$ has zeros in the $m-n$ rows if $m>n$.
\item In practice, when reducing $A$ to $R$, the matrix $P$ is never formed explicitly, instead we compute $PA=A-\beta v(v^TA)$ which carries a cost $\mathcal{O}(2mn)$, instead of $\mathcal{O}(m^2n)$ when $P$ is assembled and applied, i.e., as $P$ is dense. 
\end{itemize}
\end{frame}

\section{Givens rotations\\{\small Section 4.2 in Darve \& Wootters (2021)}}

% Slide 11
\begin{frame}{Givens rotations}
\begin{itemize}
\item When the matrix $A$ is upper Hessenberg, i.e., where $a_{ij}=0$ for all $i>j+1$, most of the subdiagonal components are already zero, and using Householder transformations in this situation is a bit of an overkill:
\begin{center}
\vspace{.2cm}
\includegraphics[height=2.5cm]{images/Upper-Hessenberg-QR.jpg}
\end{center}
\item On every column of $A$, only one entry needs to be zeroed, so that 2D rotations, which are easy to set up, can be deployed for the job:
\begin{center}
\vspace{.2cm}
\includegraphics[height=2.5cm]{images/2d-rotation.png}
\end{center}
\end{itemize}
\end{frame}

% Slide 12
\begin{frame}{Givens rotations, cont'd\textsubscript{1}}
\begin{itemize}
\item Zeroing a single subdiagonal entry can be reduced to considering a 2D vector $u=(u_1,u_2)$ and finding a rotation $G^T$ such that the vector $u$ becomes aligned with $e_1$:
\begin{center}
\vspace{.2cm}
\includegraphics[height=3.5cm]{images/multiplication-by-Gt.png}
\end{center}
With some algebra, we find:\vspace{-.1cm}
\begin{block}{Givens rotation}
A \textbf{Givens rotation} which rotates $u=(u_1,u_2)^T$ to $\|u\|_2e_1$ is the 2 x 2 matrix defined by\vspace{-.75cm}
\begin{align*}
G^T=\begin{bmatrix}
c&-s\\s&c
\end{bmatrix}
\;,\;\;
c=\frac{u_1}{\|u\|_2}
\;,\;\;
s=-\frac{u_2 }{\|u\|_2}.
\end{align*} 
\end{block}
\end{itemize}
\end{frame}

% Slide 13
\begin{frame}{Givens rotations, cont'd\textsubscript{2}}
\begin{itemize}
\item For an upper Hessenberg matrix $A$ of size $m \times n$, we can compute its QR factorization using a sequence of Givens rotations.
\item The algorithm is as follows:
\begin{itemize}
\item[1.] For each column $j=1,\dots,n-1$ :\vspace{.1cm}
\item[2.]\hspace{.4cm}Construct a Givens rotation matrix $G^T$ that zeros $a_{j+1,j}$\vspace{.1cm}
\item[3.]\hspace{.4cm}Apply $G^T$ to rows $j$ and $j+1$ of $A$
\end{itemize}
\end{itemize}
\end{frame}

\section{CholeskyQR}

% Slide 14
\begin{frame}{CholeskyQR factorization}
\begin{itemize}
\item The \textbf{CholeskyQR} algorithm builds an economic factorization (i.e., case III) of $A\in\mathbb{R}^{m\times n}$, typically for $n\ll m$.
\item It proceeds by first obtaining the $R$ factor through a Cholesky factorization of the Gram matrix $A^TA$, then retrieving the $Q$ factor by forward substitution:\vspace{.1cm}\\
The algorithm is as follows:
\begin{itemize}
\item[1.]$X:=A^TA${\color{gray}{ // BLAS 3}}\vspace{.1cm}
\item[2.]Find the upper triangular $R$ s.t. $R^TR=X$
{\color{gray}{ // Cholesky factorization}}\vspace{.1cm}
\item[3.]$Q:=AR^{-1}${\color{gray}{ // triangular solves}}
\end{itemize}
\item CholeskyQR reaches higher arithmetic intensity and requires less synchronizations than Householder QR:\vspace{.1cm}\\
\begin{center}$\implies$ favored for distributed implementations.\end{center}
\item But, unlike Householder QR, CholeskyQR suffers from instability, with
\begin{align*}
\text{LOO}(Q)\in\mathcal{O}(u\cdot\kappa(A)).
\end{align*}
\end{itemize}
\end{frame}

% Slide 15
\begin{frame}{Reorthogonalized variants}
\begin{itemize}
\item The lack of stability of an orthogonlization procedure can be partly remedied by repetition.
The repitition of CholeskyQR is referred to as \textbf{CholeskyQR2}:
\begin{itemize}
\item[1.]$(Q_1,R_1)\mapsfrom\text{CholeskyQR}(A)$\vspace{.1cm}
\item[2.]$\;\,(Q,R_2)\mapsfrom\text{CholeskyQR}(Q_1)$\vspace{.1cm}
\item[3.]$R:=R_2R_1$
\end{itemize}
\item[]Repeating CholeskyQR significantly improves orthogonality, yielding $\text{LOO}(Q)\in\mathcal{O}(u)$ under the condition that $\kappa(A)\in\mathcal{O}(u^{-1/2})$.
\item Another way to improve the stability of CholeskyQR is to shift the Gram matrix, decreasing its condition number, thus improving the stability of the Cholesky factorization.
The resulting \textbf{Shifted CholeskyQR} is given by:
\begin{itemize}
\item[1.]$X:=A^TA$\vspace{.1cm}
\item[2.]$s:=11(mn+n(n+1))u\|A\|_2^2${\color{gray}{ // calculate shift}}\vspace{.1cm}
\item[3.]$X:=X+sI_n${\color{gray}{ // shift Gram matrix}}\vspace{.1cm}
\item[4.]Find the upper triangular $R$ s.t. $R^TR=X$
{\color{gray}{ // Cholesky factorization}}\vspace{.1cm}
\item[5.]$Q:=AR^{-1}${\color{gray}{ // triangular solves}}
\end{itemize}
which ensures $\kappa(Q)\in\mathcal{O}(u^{-1/2})$ as long as $\kappa(A)\in\mathcal{O}(u^{-1})$.
%Although stability is achieved, we still have $\text{LOO}(Q)\in\mathcal{O}(u\cdot \kappa^2(A))$.
\end{itemize}
\end{frame}

% Slide 16
\begin{frame}{Reorthogonalized variants, cont'd}
\begin{itemize}
\item Therefore, Shifted CholeskyQR can be used as a preconditioner to CholeskyQR2. 
This procedure is referred to as \textbf{Shifted CholeskyQR3}:
\begin{itemize}
\item[1.]$(Q_1,R_1)\mapsfrom\text{Shifted\,CholeskyQR}(A)$\vspace{.1cm}
\item[2.]$\;\,(Q,R_2)\mapsfrom\text{CholeskyQR2}(Q_1)$\vspace{.1cm}
\item[3.]$R:=R_2R_1$
\end{itemize}
which yields $\text{LOO}(Q)\in\mathcal{O}(u)$ as long as $\kappa(A)\in\mathcal{O}(u^{-1})$.
\end{itemize}
\medskip
\tiny{Yamamoto, Y., Nakatsukasa, Y., Yanagisawa, Y., \& Fukaya, T. (2015). Roundoff error analysis of the CholeskyQR2 algorithm. Electron. Trans. Numer. Anal, 44(01), 306-326.}\smallskip\\
\tiny{Fukaya, T., Kannan, R., Nakatsukasa, Y., Yamamoto, Y., \& Yanagisawa, Y. (2020). Shifted Cholesky QR for computing the QR factorization of ill-conditioned matrices. SIAM Journal on Scientific Computing, 42(1), A477-A503.}
\end{frame}

\section{Tall-and-skinny QR}

% Slide 17
\begin{frame}{Tall-and-skinny QR (TSQR)}
\begin{itemize}
\item Householder QR is unconditionally stable but memory-bound, and CholeskyQR variants, although they achieve high arithmetic intensity, offer limited stability.
\item Tall-and-skinny (TSQR) algorithms offer both unconditional stability and high arithmetic intensity.\vspace{.1cm}\\
TSQR is particularly relevant when only the upper triangular factor $R$ is needed.
\item The key idea of TSQR is to \textbf{partition the matrix $A$ into blocks} and compute QR factorizations hierarchically.
\end{itemize}
\end{frame}

% Slide 18
\begin{frame}{Tall-and-skinny QR (TSQR), cont'd\textsubscript{1}}
\begin{itemize}
\item For a matrix $A\in\mathbb{R}^{m\times n}$ with $m\gg n$, we partition $A$ into $p$ blocks:
\begin{align*}
A = \begin{bmatrix}
A_1 \\
A_2 \\
\vdots \\
A_p
\end{bmatrix}
\end{align*}
where each $A_i \in \mathbb{R}^{(m/p) \times n}$.
\item We then compute the QR factorization of each block independently:
\begin{align*}
A_i=Q_i R_i,\quad i=1,\ldots,p.
\end{align*}
\item This gives us:
\begin{align*}
A = \begin{bmatrix}
Q_1 R_1 \\
Q_2 R_2 \\
\vdots \\
Q_p R_p
\end{bmatrix} = \begin{bmatrix}
Q_1 & & & \\
& Q_2 & & \\
& & \ddots & \\
& & & Q_p
\end{bmatrix} \begin{bmatrix}
R_1 \\
R_2 \\
\vdots \\
R_p
\end{bmatrix}.
\end{align*}
\end{itemize}
\end{frame}

% Slide 19
\begin{frame}{Tall-and-skinny QR (TSQR), cont'd\textsubscript{2}}
\begin{itemize}
\item Next, we need to compute the QR factorization of the stacked $R$ matrices:
\begin{align*}
\begin{bmatrix}
R_1 \\
R_2 \\
\vdots \\
R_p
\end{bmatrix} = \tilde{Q} \tilde{R}
\end{align*}
where $\tilde{Q} \in \mathbb{R}^{(pn) \times n}$ and $\tilde{R} \in \mathbb{R}^{n \times n}$.
\item The final QR factorization is then:
\begin{align*}
A = \begin{bmatrix}
Q_1 & & & \\
& Q_2 & & \\
& & \ddots & \\
& & & Q_p
\end{bmatrix} \tilde{Q} \tilde{R} = Q \tilde{R}
\end{align*}
where $Q = \text{diag}(Q_1, Q_2, \ldots, Q_p) \tilde{Q}$.
\item \textbf{Key advantage}: Each block can be processed independently, making TSQR highly parallelizable.
\end{itemize}
\end{frame}

% Slide 20
\begin{frame}{Tall-and-skinny QR (TSQR), cont'd\textsubscript{3}}
\begin{itemize}
\item The TSQR algorithm is as follows:
\begin{itemize}
\item[1.] Partition $A$ into $p$ blocks: $A = [A_1^T, A_2^T, \ldots, A_p^T]^T$\vspace{.1cm}
\item[2.] Parallelizable step: Compute $(Q_i, R_i) = \text{QR}(A_i)$ for $i = 1, \ldots, p$\vspace{.1cm}
\item[3.] Stack the $R$ factors: $\tilde{A} = [R_1^T, R_2^T, \ldots, R_p^T]^T$\vspace{.1cm}
\item[4.] Compute $(\tilde{Q}, \tilde{R}) = \text{QR}(\tilde{A})$\vspace{.1cm}
\item[5.] If only $R$ is needed, return $\tilde{R}$. Otherwise, $Q = \text{diag}(Q_1, \ldots, Q_p) \tilde{Q}$
\end{itemize}
\item The loss of orthogonality of TSQR is such that $\text{LOO}(Q)\in\mathcal{O}(u)$ irrespective of $A$, i.e., TSQR is unconditionally stable.
\item TSQR combines the best of both worlds: numerical stability of Householder QR with high arithmetic intensity and parallelizability.
\item[]\begin{center}$\implies$ TSQR favored for high-performance implementations.\end{center}
\end{itemize}
\end{frame}

\section{Gram-Schmidt procedures\\{\small Section 4.3 in Darve \& Wootters (2021)}}

% Slide 21
\begin{frame}{Gram-Schmidt procedures}
\begin{itemize}
\item Householder reflections and Givens rotations produce a square matrix $Q\in\mathbb{R}^{m\times m}$, even when $A\in\mathbb{R}^{m\times n}$ with $m>n$, i.e., case $\mathrm{III}$.\vspace{.1cm}\\
On the other hand, \textbf{Gram-Schmidt} procedures will \textbf{produce a rectangular, tall-and-skinny matrix} $Q\in\mathbb{R}^{m\times n}$, i.e., like in case $\textrm{II}$.
\item Another peculiarity of \textbf{Gram-Schmidt} procedures is that they \textbf{work column-by-column}, i.e., to compute $q_i$ in $Q=[q_1,\dots,q_n]$, you only need access to $a_i$ from $A=[a_1,\dots,a_n]$ and $q_1,\dots,q_{k-1}$.\vspace{.1cm}\\
This feature of the Gram-Schmidt procedures is particularly \textbf{useful in Krylov methods} where $A$ is not available all at once, and the new column $a_i$ to orthogonalize is only available after an performing a full iteration of computations.
\item The first $k$ columns $q_1,\dots,q_k$ formed by Gram-Schmidt procedure in $Q$ are an \textbf{orthonormal basis} of the subspace spanned by $a_1,\dots,a_k$.
\end{itemize}
\end{frame}

% Slide 22
\begin{frame}{Gram-Schmidt procedures, cont'd\textsubscript{1}}
\begin{itemize}
\item Visualizing the column $a_k=QR_{:,k}$, and the fact that $R_{:,k}$ has $k$ non-zero entries followed by $m-k$ zeros on the subdiagonal, we can write $a_k=Q_{:,1:k}R_{1:k,k}$:
\begin{center}
\vspace{.2cm}
\includegraphics[height=3.5cm]{images/gram-schmidt-column.png}
\end{center}
That is, $a_k$ is formed by linear combination of $q_1,\dots,q_k$:
\begin{align*}
a_k=r_{1k}q_1+\dots+r_{kk}q_k.
\end{align*}
\item Thus, instead of searching for the matrix $Q$ that makes $Q^TA$ upper triangular, we are rather going to search for the upper triangular matrix $R$ such that every $a_k$ is given by linear combination of $q_1,\dots,q_k$.
\end{itemize}
\end{frame}

% Slide 23
\begin{frame}{Gram-Schmidt procedures, cont'd\textsubscript{2}}
\begin{itemize}
\item First, since we have $a_1=r_{11}q_1$, and $q_1$ has unit norm, we set $r_{11}=\|a_1\|_2$ and $q_1=a_1/r_{11}$.
\item Then, we continue iteratively, i.e., $a_2=r_{12}q_1+r_{22}q_2$ so that $q_2$ is a unit vector in $\mathrm{span}\{q_1,a_2\}=\mathrm{span}\{a_1,a_2\}$, orthogonal to $q_1$:\vspace{.2cm}
\begin{center}
\vspace{.2cm}
\includegraphics[height=3.5cm]{images/gram-schmidt-q2.png}\vspace{.2cm}
\end{center}
Then $r_{12}$ and $r_{22}$ are found to close the system.
\end{itemize}
\end{frame}

% Slide 24
\begin{frame}{Classical Gram-Schmidt (CGS) procedure}
\begin{itemize}
\item More formally, for each $1\leq k\leq n$, we write
\begin{align*}
a_k=\sum_{i=1}^kr_{ik}q_i=r_{kk}q_k+\sum_{i=1}^{k-1}r_{ik}q_i
\end{align*}
Assuming we already know $q_j$ and $r_{ij}$ for all $j<k$ and $i\leq j$, we can then use this formula to find expressions for the $r_{ik}$'s and $q_k$.\vspace{.1cm}\\
First, multiplying by $q_i^T$ and invoking the orthonormality of the basis given by $q_1,\dots,q_k$, we get
\begin{align*}
q_i^Ta_k=\sum_{j=1}^kr_{jk}q_i^Tq_j=r_{ik}
\implies
r_{ik}=q_i^Ta_k
\text{ for }
i< k.
\end{align*}
Next, to find $r_{kk}$, we have $q_kr_{kk}=a_k-\sum_{i=1}^{k-1}r_{ik}q_i$ where $q_k$ has unit norm so that
\begin{align*}
r_{kk}=\left\|a_k-\sum_{i=1}^{k-1}r_{ik}q_i\right\|_2.
\end{align*}
\end{itemize}
\end{frame}

% Slide 25
\begin{frame}{Classical Gram-Schmidt (CGS) procedure, cont'd\textsubscript{1}}
\begin{itemize}
\item[]
Note that $r_{kk}$ could also be chosen to be negative. 
However, it is standard to let $R$ have positive components on the diagonal.\vspace{.1cm}\\
Finally, we have
\begin{align*}
q_k=\frac{1}{r_{kk}}\left(a_k-\sum_{i=1}^{k-1}r_{ik}q_i\right).
\end{align*}
This procedure is referred to as the \textbf{classical Gram-Schmidt} algorithm.\vspace{.1cm}\\
We see indeed that, in order to compute $q_k$, you need access to $a_k$ and $q_1,\dots,q_{k-1}$.
\end{itemize}
\end{frame}

% Slide 26
\begin{frame}{Classical Gram-Schmidt (CGS) procedure, cont'd\textsubscript{2}}
\begin{center}
\includegraphics[height=8cm]{images/cgs-sketch.png}
\end{center}
\end{frame}

% Slide 27
\begin{frame}{Classical Gram-Schmidt (CGS) procedure, cont'd\textsubscript{3}}
\begin{itemize}
\item So the CGS algorithm is implemented as follows:\vspace{.2cm}
\begin{itemize}
\item[1.] $r_{11}:=\|a_1\|_2$; $q_1:=a_1/r_{11}$ \vspace{.1cm}
\item[2.] For each $k=2,\dots,n$ :\vspace{.1cm}
\item[3.]\hspace{.4cm}$R_{1:k-1,k}:=Q_{:,1:k-1}^Ta_k${\color{gray}{ // BLAS 2}}\vspace{.1cm}
\item[4.]\hspace{.4cm}$q_k:=a_k-Q_{:,1:k-1}R_{1:k-1,k}${\color{gray}{ // BLAS 2}}\vspace{.1cm}
\item[5.]\hspace{.4cm}$r_{kk}:=\|q_k\|_2$; $q_{k}:=q_k/r_{kk}$
\end{itemize}\vspace{.2cm}
so that CGS relies on two BLAS 2 calls per iteration.\vspace{.1cm}\\
Similarly, we can write 
\vspace{.2cm}
\begin{itemize}
\item[1.] $r_{11}:=\|a_1\|_2$; $q_1:=a_1/r_{11}$ \vspace{.1cm}
\item[2.] For each $k=2,\dots,n$ :\vspace{.1cm}
\item[3.]\hspace{.4cm}$q_k:=\Pi_{k-1}a_k$\vspace{.1cm}
\item[4.]\hspace{.4cm}$r_{kk}:=\|q_k\|_2$; $q_{k}:=q_k/r_{kk}$
\end{itemize}\vspace{.2cm}
where $\Pi_{k-1}:=I_m-Q_{:,1:k-1}Q_{:,1:k-1}^T$ is an \textbf{orthogonal projector} onto the subspace $\mathrm{range}(Q_{:,1:k-1})^\perp$ so, indeed, $q_k$ is made orthogonal to the previously formed vectors $q_1,\dots,q_{k-1}$.
\end{itemize}
\end{frame}

% Slide 28
\begin{frame}{Instability of CGS}
\begin{itemize}
\item CGS is known to \textbf{not} being very \textbf{stable}.
\item For example, consider the matrix $A=\begin{bmatrix}1&1&1\\\varepsilon&0&0\\0&\varepsilon&0\\0&0&\varepsilon\end{bmatrix}$.\vspace{.4cm}\\
If we assume $\varepsilon^2$ is smaller than the unit roundoff $u$, then the $Q$ matrix generated by CGS is $Q=\begin{bmatrix}1&0&0\\\varepsilon&-1/\sqrt{2}&-1/\sqrt{2}\\0&1/\sqrt{2}&0\\0&0&1/\sqrt{2}\end{bmatrix}$.\\
Then we see that $q_2$ and $q_3$ are far from being orthogonal as we have $q_2^Tq_3=1/2$.
\item Numerical stability is measured with respect to the loss of orthogonality, LOO, defined by $\text{LOO}(Q):=\|I_m-Q^TQ\|_2$.
\item With CGS, the LOO depends on $A$, i.e., $\mathrm{LOO}\in\mathcal{O}(u\cdot\kappa^{n-1}(A))$, irrespective of $\kappa(A)$, although this bound is not sharp.
\end{itemize}
\end{frame}

% Slide 29
\begin{frame}{Re-orthogonalization, CGS2}
\begin{itemize}
\item An alternative is to orthogonalize twice by CGS, leading to CGS2:\\
\begin{minipage}[lt]{0.51\textwidth}\vspace{-.2cm}
\begin{itemize}
\item[1.] $r_{11}:=\|a_1\|_2$; $q_1:=a_1/r_{11}$ \vspace{-.1cm}
\item[2.] For each $k=2,\dots,n$ :\vspace{-.1cm}
\item[3.]\hspace{.4cm}$q_k:=\Pi_{k-1}a_k$\vspace{-.1cm}
\item[4.]\hspace{.4cm}$q_k:=\Pi_{k-1}q_k$\vspace{-.1cm}
\item[5.]\hspace{.4cm}$r_{kk}:=\|q_k\|_2$; $q_{k}:=q_k/r_{kk}$
\end{itemize}\vspace{.2cm}
where $\Pi_{k-1}:=I_m-Q_{:,1:k-1}Q_{:,1:k-1}^T.\hspace{-1cm}$
\end{minipage}
\begin{minipage}[lt]{0.43\textwidth}\vspace{.3cm}
\begin{itemize}
\item[1.] $r_{11}:=\|a_1\|_2$; $q_1:=a_1/r_{11}$ \vspace{-.1cm}
\item[2.] For each $k=2,\dots,n$ :\vspace{-.1cm}
\item[3.]\hspace{.4cm}$R_{1:k-1,k}:=Q_{:,1:k-1}^Ta_k$ \vspace{-.1cm}
\item[4.]\hspace{.4cm}$q_k:=a_k-Q_{:,1:k-1}R_{1:k-1,k}$ \vspace{-.1cm}
\item[5.]\hspace{.4cm}$S_{1:k-1}:=Q_{:,1:k-1}^Tq_k$ \vspace{-.1cm}
\item[6.]\hspace{.4cm}$q_k:=q_k-Q_{:,1:k-1}S_{1:k-1}$ \vspace{-.1cm}
\item[7.]\hspace{.4cm}$r_{kk}:=\|q_k\|_2$; $q_{k}:=q_k/r_{kk}$
\end{itemize}
\vspace{.2cm}
\end{minipage}\vspace{.08cm}\\
\item The loss of orthogonality becomes $\mathrm{LOO}(Q)\in\mathcal{O}(u)$ under the assumption that $\kappa(A)\in\mathcal{O}(u^{-1})$.
\item However, CGS2 requires $4mn^2$ FLOPs instead of $2mn^2$ for CGS.
\end{itemize}
\end{frame}

% Slide 30
\begin{frame}{Modified Gram-Schmidt, MGS}
\begin{itemize}
\item Another alternative to CGS, referred to as modified Gram-Schmidt (MGS), is obtained by letting $\Pi_{k-1}:=(I_m-q_{:,k-1}q_{:,k-1}^T)\dots(I_m-q_{:,1}q_{:,1}^T)$ in\vspace{.2cm}
\begin{itemize}
\item[1.] $r_{11}:=\|a_1\|_2$; $q_1:=a_1/r_{11}$ \vspace{.1cm}
\item[2.] For each $k=2,\dots,n$ :\vspace{.1cm}
\item[3.]\hspace{.4cm}$q_k:=\Pi_{k-1}a_k$\vspace{.1cm}
\item[5.]\hspace{.4cm}$r_{kk}:=\|q_k\|_2$; $q_{k}:=q_k/r_{kk}$
\end{itemize}\vspace{.2cm}
Assuming perfect arithmetic, this is equivalent to CGS, but it relies on BLAS 1 instead BLAS 2 operations:\vspace{.2cm}
\begin{itemize}
\item[1.] $r_{11}:=\|a_1\|_2$; $q_1:=a_1/r_{11}$ \vspace{.1cm}
\item[2.] For each $k=2,\dots,n$ :\vspace{.1cm}
\item[3.]\hspace{.4cm}$q_k:=a_k$\vspace{.1cm}
\item[4.]\hspace{.4cm}For each $\ell=1,\dots,k-1$ :\vspace{.1cm}
\item[3.]\hspace{.8cm}$r_{\ell k}:=q_\ell^Tq_k${\color{gray}{ // BLAS 1}}\vspace{.1cm}
\item[4.]\hspace{.8cm}$q_k:=q_k-r_{\ell k}q_\ell${\color{gray}{ // BLAS 1}}\vspace{.1cm}
\item[5.]\hspace{.4cm}$r_{kk}:=\|q_k\|_2$; $q_{k}:=q_k/r_{kk}$
\end{itemize}
\end{itemize}
\end{frame}

% Slide 31
\begin{frame}{Modified Gram-Schmidt, MGS, cont'd}
\begin{itemize}
\item The loss of orthogonality of MGS is $\mathrm{LOO}(Q)\in\mathcal{O}(u\cdot\kappa(A))$, irrespective of $\kappa(A)$, so that it is more stable than CGS.
\item Considering once again the matrix $A=\begin{bmatrix}1&1&1\\\varepsilon&0&0\\0&\varepsilon&0\\0&0&\varepsilon\end{bmatrix}$ where $\varepsilon^2<u$,\vspace{.2cm}\\
MGS yields a $Q$ matrix $A=\begin{bmatrix}1&0&0\\\varepsilon&-1/\sqrt{2}&-1/\sqrt{6}\\0&1/\sqrt{2}&-1/\sqrt{6}\\0&0&\sqrt{2}/\sqrt{3}\end{bmatrix}$.\vspace{.4cm}\\
Contrarily to CGS, we see that $q_2$ and $q_3$ are exactly orthogonal, i.e., $q_2^Tq_3=0$,
and $q_1$ is nearly orthogonal to $q_2$ and $q_3$, with $|q_1^Tq_2|=\varepsilon/\sqrt{2}$ and $|q_1^Tq_3|=\varepsilon/\sqrt{6}$.
\item In practice, MGS and CGS2 are used instead of CGS.
\item MGS is often preferred by default, but CGS2 is more stable and reaches higher arithmetic intensity.
\end{itemize}
\end{frame}

\section{Least-squares problems\\{\small Section 4.4 in Darve \& Wootters (2021)}}

% Slide 32
\begin{frame}{Geometry of least-squares}
\begin{itemize}
\item One of the applications of the QR decomposition is the solving of least-squares problems $\arg\min_x\|Ax-b\|_2$:\vspace{.1cm}
\begin{center}
\includegraphics[height=2.5cm]{images/lsq-problem.png}
\end{center}
with a tall-and-skinny matrix $A\in\mathbb{R}^{m\times n}$ and a vector $b\in\mathbb{R}^n$.
\item To minimize the 2-norm from a point $b$ to a subspace $\{Ax,\;x\in\mathbb{R}^n\}$, we can just do an orthogonal projection:
\begin{center}
\includegraphics[height=3cm]{images/orthogonal-projection-lsq.png}
\end{center}
\end{itemize}
\end{frame}

% Slide 33
\begin{frame}{Method of normal equations}
\begin{itemize}
\item As per property of orthogonal projections, the $x$ that minimizes $\|Ax-b\|_2$ has an error $e:=Ax-b$ which is orthogonal to the range of $A$.
This can be written as
\begin{align}\label{normal-eqs}
A^T(Ax-b)=0.
\end{align}
Assuming $A$ is full-rank, this equation can be used to solve for $x$ by a method called \textbf{normal equations}.\vspace{.1cm}\\
Eq.~\eqref{normal-eqs} may also be derived from calculus, namely, the optimal $x$ which minimizes the cost function
\begin{align*}
f(x)=\|Ax-b\|_2^2=(Ax-b)^T(Ax-b)=x^TA^TAx-2x^TA^Tb+b^Tb
\end{align*}
is obtained for $\nabla f(x)=0$ where
\begin{align*}
\nabla f(x)=2A^TAx-2A^Tb,
\end{align*}
which equivalently yields Eq.~\eqref{normal-eqs}.
\end{itemize}
\end{frame}

% Slide 34
\begin{frame}{Method of normal equations, cont'd}
\begin{itemize}
\item Assuming $A$ is full-rank, $A^TA$ is SPD so that we may compute its Cholesky factorization and solve for $x$ in $A^TAx=A^Tb$.
\begin{block}{Normal equations}
Finding the solution $x$ to the least-suqares problem $\arg\min\|Ax-b\|_2$ by solving the system $A^TAx=A^Tb$ is called the method of \textbf{normal equations}.
\end{block}
\item Since the condition number of $A^TA$ is the square of that of $A$, the method of normal equations can run into issues when $A$ is poorly conditioned.
\item For cases where $A$ is poorly conditioned, the QR factorization can be used to yield a more accurate computation of the solution $x$ to the least-squares problem.
\end{itemize}
\end{frame}

% Slide 35
\begin{frame}{QR factorization for least-squares problems}
\begin{itemize}
\item The origin of the method of normal equations stems from saying that the error $Ax-b$ is orthogonal to the range of $A$.\vspace{.1cm}\\
But if we know a QR factorization $A=QR$ where $Q\in\mathbb{R}^{m\times n}$, then the range of $A$ is the same as the range of $Q$.\vspace{.1cm}\\
The orthogonality condition can then be re-stated as 
\begin{align}\label{normal-eqs-qr}
Q^T(Ax-b)=0.
\end{align}
Since $Q$ is orthogonal, it is necessarily well-conditioned, and the conditioning problem of the method of normal equations can be avoided.\vspace{.1cm}\\
Since $A=QR$, due to the orthogonality of $Q$, we have $Q^TA=R$ so that Eq.~\eqref{normal-eqs-qr} becomes
\begin{align*}
Rx=Q^Tb
\end{align*}
where $R$ is non-singular as long as $A$ is non-singular, so that there exists a unique solution $x$.
\end{itemize}
\end{frame}

% Slide 36
\begin{frame}{Case of rank-deficient $A$}
\begin{itemize}
\item If $A$ is rank-deficient, the null space of $A$ is non-trivial.
Then, for some $x$ that minimizes $\|Ax-b\|_2$, there are infinitely many $\delta x\in\mathrm{null}(A)$ such that $A(x+\delta x)=Ax$.
Hence, the solution to the least-squares problem is not unique.
\item In case of non-uniqueness of solution, one can search for the unique $x_0$ which minimizes both $\|Ax-b\|_2$ and $\|x\|_2$:\vspace{.3cm}
\begin{center}
\includegraphics[height=3.5cm]{images/lsq-rank-deficient.png}
\end{center}
We can see the $x_0$ we are after is orthogonal to the null space of $A$, while any other solution $x_1$ is of the form $x_0+\delta x$.
\end{itemize}
\end{frame}

% Slide 37
\begin{frame}{SVD method for solving least-squares with rank-deficient $A$}
\begin{itemize}
\item Let $A\in\mathbb{R}^{m\times n}$ be of rank $r<n<m$ have an SVD given by $A=U\Sigma V^T$ with $U\in\mathbb{R}^{m\times m},V\in\mathbb{R}^{n\times n}$ and $\Sigma\in\mathbb{R}^{m\times n}$ where $\Sigma$ has zeros from row $r+1$ to $m$.\vspace{.1cm}\\
Then we can ignore the columns of $U$ and $V$ that correspond to zeros in $\Sigma$ to create the thin SVD $A=\widetilde{U}\widetilde{\Sigma}\widetilde{V}^T$:\vspace{.2cm}
\begin{center}
\includegraphics[height=2.75cm]{images/svd-lsq.png}
\end{center}\vspace{.2cm}
\item Now, $Ax=0$ if and only if $\widetilde{V}^Tx=0$, which means that the null space of $A$ is the same as that of $\widetilde{V}^T$, i.e., $\mathrm{null}(A)=\mathrm{null}(\widetilde{V}^T)$.
\end{itemize}
\end{frame}

% Slide 38
\begin{frame}{SVD method for solving least-squares with rank-deficient $A$}
\begin{itemize}
\item We know that any solution $x$ to the least-squares problem satisfies
\begin{align*}
A^TAx=&\;A^Tb\\
(\widetilde{U}\widetilde{\Sigma}\widetilde{V}^T)^T\widetilde{U}\widetilde{\Sigma}\widetilde{V}^Tx=&\;(\widetilde{U}\widetilde{\Sigma}\widetilde{V}^T)^Tb\\
\widetilde{V}\widetilde{\Sigma}^T\widetilde{\Sigma}\widetilde{V}^Tx=&\;\widetilde{V}\widetilde{\Sigma}^T\widetilde{U}^Tb\\
\widetilde{\Sigma}\widetilde{V}^Tx=&\;\widetilde{U}^Tb
\end{align*}
where $r<n$ so that $\widetilde{\Sigma}\widetilde{V}^T$ is not full-column-rank and this equation admits infinitely many solutions.
\item However, we can find one solution as follows.\vspace{.1cm}\\
First, let's solve the system $\widetilde{\Sigma}\omega=\widetilde{U}^Tb$ for $\omega\in\mathbb{R}^r$. This gives
\begin{align*}
\omega_i=\frac{\widetilde{u}_i^Tb}{\widetilde{\sigma}_{ii}}
\end{align*}
where $\widetilde{U}=[\widetilde{u}_1,\dots,\widetilde{u}_r]$ and $\widetilde{\Sigma}=\mathrm{diag}(\widetilde{\sigma}_{11},\dots,\widetilde{\sigma}_{rr})$.
\end{itemize}
\end{frame}

% Slide 39
\begin{frame}{SVD method for solving least-squares with rank-deficient $A$}
\begin{itemize}
\item[] Then, since $\omega=\widetilde{\Sigma}^{-1}\widetilde{U}^Tb$, we have
\begin{align*}
\widetilde{\Sigma}\widetilde{V}^T(\widetilde{V}\omega)=\widetilde{\Sigma}\widetilde{V}^T\widetilde{V}\tilde{\Sigma}^{-1}\widetilde{U}^Tb=\widetilde{U}^Tb
\end{align*}
so that $x_0:=\widetilde{V}\omega$ is solution of $\widetilde{\Sigma}\widetilde{V}^Tx_0=\widetilde{U}^Tb$ and thus, as explained before, it is also solution of the least-squares problem.\vspace{.1cm}\\
Note that $x_0:=\widetilde{V}\omega$ is the solution with smallest norm.\vspace{.1cm}\\
To see this, we need to show $x_0\perp\mathrm{null}(A)$.
First, let 
\begin{align*}
\mathrm{null}(\widetilde{V}^T)=\{y\in\mathbb{R}^n,\;\widetilde{V}^Ty=0\}
\end{align*} 
and consider that for each $y\in\mathrm{null}(\widetilde{V}^T)$, we have 
\begin{align*}
x_0^Ty=(\widetilde{V}\omega)^Ty=\omega\widetilde{V}^Ty=0
\end{align*}
so that $x_0\perp \mathrm{null}(\widetilde{V}^T)$.\vspace{.1cm}\\
But since $\mathrm{null}(\widetilde{V}^T)=\mathrm{null}(A)$, we have that $x_0\perp\mathrm{null}(A)$.
\end{itemize}
\end{frame}

\section{LSQR}
% Slide 40
\begin{frame}{Iterative solve of normal equations}
\begin{itemize}
\item We saw that the least-squares solution $x_*$ of $\min_{x\in\mathbb{R}^n}\|Ax-b\|_2$ for $A\in\mathbb{R}^{m\times n}$ and $b\in\mathbb{R}^n$ with $m>n$ is such that $A^TAx_*=A^Tb$, i.e., $x_*$ is solution of the normal equation. 
\item For very large matrices, the cost of computing a QR factorization by Householder QR, or even by CholeskyQR, can be prohibitive.
When the matrix is sparse, computing a QR factorization is generally an overkill.
\item In future lectures, we will look into iterative methods to solve linear systems of the form $Bx=b$ with a square matrix $B\in\mathbb{R}^{n\times n}$.\vspace{.1cm}\\
In particular, if $A$ is full-rank, one can use the conjugate gradient algorithm to solve $A^TAx_*=b$.\vspace{.1cm}\\
However, in practice, for cases where $A$ is ill-conditioned, this approach can suffer from significantly delayed convergence.
\item \textbf{LSQR} is an algorithm proposed by Paige and Sanders (1982) which, in case of exact arithmetic, reproduces the iterates of the conjugate gradient algorithm applied to the normal equation but, in practice, is \textbf{more reliable}.
\end{itemize}
\medskip
\tiny{Paige, C. C., \& Saunders, M. A. (1982). LSQR: An algorithm for sparse linear equations and sparse least squares. ACM Transactions on Mathematical Software (TOMS), 8(1), 43-71.}
\end{frame}

% Slide 41
\begin{frame}{Bidiagonalization}
\begin{itemize}
\item Bidiagonalization is a procedure proposed by Golub and Kahan (1965) which reduces any general matrix $A\in\mathbb{R}^{m\times n}$ into lower bidiagonal form.
\item[] Let $x_0\in\mathbb{R}^n$ be an initial approximation of $x_*$ with residual $r_0:=b-Ax_0$.
\item[] Starting the bidiagonalization procedure with $r_0$ goes as follows:\vspace{-.1cm}
\begin{align*}
\beta_1u_1=r_0,\;\alpha_1v_1=A^Tu_1
\end{align*}
\vspace{-1.1cm}\\
\begin{align*}
\left.
\begin{aligned}
\beta_{i+1}u_{i+1}=&\,Av_i-\alpha_iu_i\\
\alpha_{i+1}v_{i+1}=&\,A^Tu_{i+1}-\beta_{i+1}v_i
\end{aligned}
\right\}
\text{for }i=1,2\dots
\end{align*}
where the scalars $\alpha_i\geq0$ and $\beta_i\geq0$ are chosen so that $\|u_i\|_2=\|v_i\|_2=1$.
\item[] Let $U_k:=[u_1,\dots,u_k]$, $V_k:=[v_1,\dots,v_k]$,\vspace{-.15cm}
{\footnotesize\begin{align*}
B_k:=
\begin{bmatrix}
\alpha_1&&&\\
\beta_2&\alpha_2&&\\
&\ddots&\ddots&\\
&&\beta_k&\alpha_k
\end{bmatrix}
\text{ and }\;
\underline{B}_k:=
\begin{bmatrix}
\alpha_1&&&\\
\beta_2&\alpha_2&&\\
&\ddots&\ddots&\\
&&\beta_k&\alpha_k\\
&&&\beta_{k+1}
\end{bmatrix}.
\end{align*}}
\end{itemize}
\tiny{Golub, G., \& Kahan, W. (1965). Calculating the singular values and pseudo-inverse of a matrix. Journal of the Society for Industrial and Applied Mathematics, Series B: Numerical Analysis, 2(2), 205-224.}\smallskip\\
\tiny{Paige, C. C. (1974). Bidiagonalization of matrices and solution of linear equations. SIAM Journal on Numerical Analysis, 11(1), 197-209.}
\end{frame}

% Slide 42
\begin{frame}{Bidiagonalization, cont'd}
\begin{itemize}
\item[]Then, we have\vspace{-.1cm}
\begin{align*}
\hspace*{.75cm}
U_{k+1}(\beta_1e_1)=&\,r_0
\hspace{5.55cm}{\color{gray}{(e_1:=I_{k+1}[:,1]})}\\
AV_k=&\,U_{k+1}\underline{B}_k=U_kB_k+\beta_{k+1}u_{k+1}e_k^T
\hspace{1.05cm}{\color{gray}{(e_k:=I_{k}[:,k]})}\\
A^TU_{k+1}=&\,V_k\underline{B}_k^T+\alpha_{k+1}v_{k+1}e_{k+1}^T.
\hspace{1cm}{\color{gray}{(e_{k+1}:=I_{k+1}[:,k+1]})}
\end{align*}
In exact arithmetic, we have $U_k^TU_k=I_k$ and $V_k^TV_k=I_k$.
\item[]Clearly, $U_{k+1}^TAV_k=\underline{B}_k$, $U_k^TAV_k=B_k$, $\alpha_k=u_k^TAv_k$ and $\beta_k=u_{k}^TAv_{k-1}$.
\item[]Moreover, the columns of $U_k$ and $V_k$ are orthonormal bases of Krylov subspaces of $AA^T$ and $A^TA$, respectively, i.e.,
\begin{align*}
\text{range}(U_k)=&\,\mathcal{K}_k(AA^T,u_1)=
\text{span}\left\{u_1,AA^Tu_1,\dots,(AA^T)^{k-1}u_1\right\},\\
\text{range}(V_k)=&\,\mathcal{K}_k(A^TA,v_1)=
\text{span}\left\{v_1,A^TAv_1,\dots,(A^TA)^{k-1}v_1\right\}.
\end{align*}
Bidiagonalization is an orthogonal equivalence transformation which plays a key role in the iterative solve of singular value decompositions.\smallskip
\end{itemize}
\tiny{Golub, G., \& Kahan, W. (1965). Calculating the singular values and pseudo-inverse of a matrix. Journal of the Society for Industrial and Applied Mathematics, Series B: Numerical Analysis, 2(2), 205-224.}\tinyskip\\
\tiny{Paige, C. C. (1974). Bidiagonalization of matrices and solution of linear equations. SIAM Journal on Numerical Analysis, 11(1), 197-209.}
\end{frame}

% Slide 43
\begin{frame}{LSQR}
\begin{itemize}
\item LSQR defines a sequence of iterates $x_1,x_2,\dots,x_k$ which approximate the solution $x_*$ of $\min_{x\in\mathbb{R}^n}\|b-Ax\|_2$ by
\begin{align*}
x_k\in&\,x_0+\mathcal{K}_k(A^TA,v_1).
\end{align*}
That is, we search for $x_k=x_0+V_ky_k$ such that
\begin{align*}
\hspace{-.2cm}
x_k=\underset{x\in x_0+\text{range}(V_k)}{\arg\min}\|b-Ax\|_2
\implies
y_k=&\,\underset{y\in\mathbb{R}^k}{\arg\min}\|b-A(x_0+V_ky)\|_2\\
=&\,\underset{y\in\mathbb{R}^k}{\arg\min}\|r_0-U_{k+1}\underline{B}_ky\|_2\\
=&\,\underset{y\in\mathbb{R}^k}{\arg\min}\|U_{k+1}(\beta_1e_1)-U_{k+1}\underline{B}_ky\|_2\\
=&\,\underset{y\in\mathbb{R}^k}{\arg\min}\|\beta_1e_1-\underline{B}_ky\|_2.
\end{align*}
In exact arithmetic, the LSQR iterates exhibit monotonic decrease of residual norm, i.e., $\|r_{k+1}\|_2\leq\|r_k\|_2$.
\end{itemize}
\end{frame}

% Slide 44
\begin{frame}{LSQR, cont'd}
\begin{itemize}
\item A basic implementation of the LSQR algorithm goes as follows:
\begin{itemize}
\item[1.]$r_0:=b-Ax_0$\vspace{.1cm}
\item[2.]$u_1:=r_0$, $\beta_1:=\|u_1\|_2$, $u_1:=u_1/\beta_1$\vspace{.1cm}
\item[3.]$v_1:=A^Tu_1$, $\alpha_1:=\|v_1\|_2$, $v_1:=v_1/\alpha_1$\vspace{.1cm}
\item[4.]for $i=1,2,\dots$\vspace{.1cm}
\item[5.]\hspace{.2cm}$u_{i+1}:=Av_i-\alpha_iu_i$, $\beta_{i+1}:=\|u_{i+1}\|_2$, $u_{i+1}:=u_{i+1}/\beta_{i+1}$\vspace{.1cm}
\item[6.]\hspace{.2cm}$v_{i+1}:=A^Tu_{i+1}-\beta_{i+1}v_i$, $\alpha_{i+1}:=\|v_{i+1}\|_2$, $v_{i+1}:=v_{i+1}/\alpha_{i+1}$\vspace{.1cm}
\item[7.]\hspace{.2cm}$y_i:=\underset{y\in\mathbb{R}^i}{\arg\min}\|\beta_1e_1-\underline{B}_iy\|_2$\vspace{.1cm}
\end{itemize}
When convergence is achieved, the iterate $x_i:=x_0+V_iy_i$ is formed.
\item[]Convergence monitoring is reliant on the evaluation of $\|r_i\|_2$ and $\|A^Tr_i\|_2$.
Note that we have\vspace{-.1cm}
\begin{align*}
\|r_i\|_2
=&\,
\|b-A(x_0+V_iy_i)\|_2
=\|\underline{t}_i\|_2
\;\text{ where }\;
\underline{t}_i:=\beta_1e_1-\underline{B}_iy_i\\
\|A^Tr_i\|_2=&\,
\|A^TU_{i+1}\underline{t}_i\|_2
=\|(V_{i}\underline{B}_i^T+\alpha_{i+1}v_{i+1}e_{i+1}^T)\underline{t}_i\|_2=
\|B_{i+1}^T\underline{t}_i\|_2
\end{align*}
so that convergence can be monitored without forming neither $x_i$ nor $r_i$.
\end{itemize}
\end{frame}

\section{Homework problems}
% Slide 45
\begin{frame}{Homework problem}\vspace{.1cm}
Turn in \textbf{your own} solution to the following problem:\vspace{.15cm}\\
\begin{minipage}[t]{0.1\textwidth}
\textbf{Pb.$\,$17}
\end{minipage}
\begin{minipage}[t]{0.89\textwidth}
Let $A=\begin{bmatrix}1&1\\1&-1\\1&1\end{bmatrix}$ and $b=\begin{bmatrix}1\\2\\3\end{bmatrix}$.\vspace{.4cm}\\
(a) Find a QR decomposition of $A$ applying a Gram-Schmidt procedure with a pen and paper.\vspace{.1cm}\\
(b) Find the least-squares problem solution $x_*=\arg\min_x\|Ax-b\|_2$ making use of the QR factorization.
\end{minipage}\vspace{.15cm}
\end{frame}

\section{Practice session}
% Slide 46
\begin{frame}[fragile]{Practice session}
\begin{enumerate}
\item Implement and test and time your implementation of Householder QR.
\item Implement CholeskyQR, CholeskyQR2 and Shifted CholeskyQR3.
\item Compare the runtime, loss of orthogonality and residual obtained with each CholeskyQR implementation for multiple tall-and-skinny matrices with different conditioning numbers.
\item Implement CGS, CGS2 and MGS procedures.
\item Compare the runtime, loss of orthogonality and residual obtained with each Gram-Schmidt implementation for multiple tall-and-skinny matrices with different conditioning numbers.
\item Implement the Golub-Kahan bidiagonalization procedure.
Then, compare $U_{k+1}^TAV_k$ to $\underline{B}_k$, and check the loss of orthogonality of $U_{k+1}$ and $V_k$. What do you observe?
\item Implement LSQR using the following stopping criterion:
\begin{align*}
\frac{\|A^Tr_i\|_2}{\|A\|_F\|r_i\|_2}<\varepsilon_{tol}.
\end{align*}
Apply your algorithm to large sparse least-squares problems.
\end{enumerate}
\end{frame}

\end{document}














