\documentclass[t,usepdftitle=false]{beamer}

\input{~/Dropbox/Git/tex-beamer-custom/preamble.tex}

\title[NLA for CS and IE -- Lecture 05]{Numerical Linear Algebra\\for Computational Science and Information Engineering}
\subtitle{\vspace{.3cm}Lecture 05\\Sparse Data Structures and Basic Linear Algebra Subprograms}
\hypersetup{pdftitle={NLA-for-CS-and-IE\_Lecture05}}

\date[Summer 2025]{Summer 2025}

\author[nicolas.venkovic@tum.de]{Nicolas Venkovic\\{\small nicolas.venkovic@tum.de}}
\institute[]{Group of Computational Mathematics\\School of Computation, Information and Technology\\Technical University of Munich}

\titlegraphic{\vspace{0cm}\includegraphics[height=1.1cm]{~/Dropbox/Git/logos/TUM-logo.png}}

\begin{document}
	
\begin{frame}[noframenumbering, plain]
	\maketitle
\end{frame}
	
\myoutlineframe
	
\section{Basic linear algebra subprograms (BLAS)}

% Slide 01
\begin{frame}{Basic linear algebra subprograms (BLAS)}
\begin{itemize}
\item What is BLAS?
\begin{itemize}\normalsize
%\item[-] Set of \textbf{low-level routines} for common \textbf{linear algebra operations}.\vspace{.1cm}
\item[-] Originated in the 1970s, as a set of \textbf{low-level routines} for common \textbf{linear algebra operations}, first written in Fortran.\vspace{.03cm}
\item[-] Became a \textbf{standard} for the specification of linear algebra subroutines.\vspace{.03cm}
\end{itemize}
\item Why use BLAS?
\begin{itemize}\normalsize
\item[-] \textbf{Performance}: algorithmic optimizations, multi-threading, vectorization, loop unrolling, cache and register blocking, instruction pipelining, ...\vspace{.03cm}
\item[-] \textbf{Portability}: Consistent interface across different platforms.\vspace{.03cm}
\end{itemize}
\item Over time, \textbf{different BLAS libraries} have been developed, in \textbf{different languages}, for \textbf{different hardware}:
\begin{itemize}\normalsize
\item[-] Intel oneAPI \textbf{MKL}: Proprietary, highly optimized for Intel architectures,\\
\hspace{3.23cm}GPU support through SYCL, comprehensive.\vspace{.03cm}
\item[-] \textbf{OpenBLAS}: Open source, multi-architecture support, some GPU\\
\hspace{2.25cm}support, derived from GotoBLAS, community-driven.\vspace{.03cm}
\item[-] \textbf{BLIS}: Open source, research-oriented (UT Austin).\vspace{.03cm}
\item[-] \textbf{ATLAS}: Open source, empirical auto-tuning during build.\vspace{.03cm}
\item[-] GPU only: Nvidia \textbf{cuBLAS}, AMD \textbf{rocBLAS}, ...
\end{itemize}
\end{itemize}
\end{frame}

% Slide 02
\begin{frame}{Common BLAS subroutines}
BLAS routines are \textbf{organized into levels}, and follow a \textbf{naming convention} for most standard operations.
\begin{itemize}
\item \textbf{Level 1} (\textbf{vector} operations, typically $O(n)$ ops.):
\begin{itemize}\normalsize
\item[-] Dot product $\hspace{.46cm}$({\color{blue}{\texttt{D}}}\texttt{DOT}, {\color{blue}{\texttt{S}}}\texttt{DOT}, ...): $\hspace{.4cm}x^Ty$\vspace{.07cm}
%\item[-] Vector scaling $\hspace{.2cm}$({\color{blue}{\texttt{D}}}\texttt{SCAL}, {\color{blue}{\texttt{S}}}\texttt{SCAL}, ...): $x\leftarrow x$\vspace{.07cm}
\item[-] Vector addition ({\color{blue}{\texttt{D}}}\texttt{AXPY}, {\color{blue}{\texttt{S}}}\texttt{AXPY}, ...): $y\leftarrow\alpha x+y$\vspace{.07cm}
\item[-] Vector norms $\hspace{.34cm}$({\color{blue}{\texttt{D}}}\texttt{NRM2}, {\color{blue}{\texttt{S}}}\texttt{NRM2}, ...): $\|x\|_2$
\end{itemize}
\item \textbf{Level 2} (\textbf{matrix-vector} operations, typically $O(n^2)$ ops.):
\begin{itemize}\normalsize
\item[-] Matrix-vector multiply ({\color{blue}{\texttt{D}}}\texttt{GEMV}, {\color{blue}{\texttt{S}}}\texttt{GEMV}): $y \leftarrow \alpha Ax + \beta y$\vspace{.07cm}
\item[-] Rank-1 update $\hspace{1.2cm}$({\color{blue}{\texttt{D}}}\texttt{GER}, {\color{blue}{\texttt{S}}}\texttt{GER}): $\hspace{.32cm}A \leftarrow \alpha xy^T + A$\vspace{.07cm}
\item[-] Triangular solve $\hspace{.93cm}$ ({\color{blue}{\texttt{D}}}\texttt{TRSV}, {\color{blue}{\texttt{S}}}\texttt{TRSV}): $x \leftarrow T^{-1}x$
\end{itemize}
\item \textbf{Level 3} (\textbf{matrix} operations, typically $O(n^3)$ ops.):
\begin{itemize}\normalsize
\item[-] Matrix-matrix multiply ({\color{blue}{\texttt{D}}}\texttt{GEMM}, {\color{blue}{\texttt{S}}}\texttt{GEMM}, ...):  $C \leftarrow \alpha AB + \beta C$\vspace{.07cm}
\item[-] Rank-$k$ update $\hspace{1.23cm}$({\color{blue}{\texttt{D}}}\texttt{SYRK}, {\color{blue}{\texttt{S}}}\texttt{SYRK}, ...):  $C \leftarrow \alpha AA^T + \beta C$
\end{itemize}
\end{itemize}
The first letter in the name of a subroutine represents the data type:\\
$\hspace*{1.1cm}${\color{blue}{\texttt{D}}}: double precision real$\hspace{1.96cm}${\color{blue}{\texttt{S}}}: single precision real\\
$\hspace*{1.1cm}${\color{blue}{\texttt{C}}}: single precision complex$\hspace{1.37cm}${\color{blue}{\texttt{Z}}}: double precision complex
\end{frame}

% Slide 03
\begin{frame}{Common BLAS subroutines, cont'd}
\vspace{-.2cm}
\begin{center}
\includegraphics[height=7.8cm]{images/BLAS-netlib.jpg}
\end{center}
$\vspace{-.5cm}$\\
\tiny{University of Tennessee, Oak Ridge National Laboratory, Numerical Algorithms Group Ltd. (1997). Basic linear algebra subprograms -- A quick reference guide. (\url{https://www.netlib.org/blas})}
\end{frame}

% Slide 04
\begin{frame}{BLAS in practice}
\begin{itemize}
\item \textbf{BLAS interfaces} tend to be \textbf{mathematically opaque}.
\item Using the \textbf{Intel oneAPI MKL C interface}:
\begin{itemize}\normalsize
\item[-] The Julia code \mintinline{julia}{Ax = A*x; AtAx = A'Ax} becomes:\vspace{.1cm}
\begin{center}
\includegraphics[height=2.3cm]{images/mkl-dgemv.jpg}
\end{center}
\item[-] \textbf{Documentation}:\\
\begin{center}\normalsize
\url{https://www.intel.com/content/www/us/en/docs/onemkl/developer-reference-dpcpp/2024-2/blas-routines.html}
\end{center}
%\item[-] Typically 3 releases per year, i.e., \texttt{202X.\{0,1,2\}}
\end{itemize}
\item For interfaces to other implementations, see
\begin{itemize}\normalsize
\item[-] \textbf{OpenBLAS}: \url{https://github.com/OpenMathLib/OpenBLAS}\vspace{.1cm}
\item[-] \textbf{ATLAS}: \url{https://github.com/flame/blis}\vspace{.1cm}
\item[-] \textbf{BLIS}: \url{http://math-atlas.sourceforge.net/}
\end{itemize}
\end{itemize}
\end{frame}

% Slide 05
\begin{frame}{BLAS in practice, cont'd}
\begin{itemize}
\item The cost of \textbf{enhanced portability} often comes in the form of \textbf{building challenges}.
\begin{itemize}
\item[-] E.g., MKL and OpenBLAS offer support for various CPU vendors and GPUs.
\end{itemize}
\item For \textbf{Intel oneAPI MKL}, there is a dedicated web tool to help with the linking configuration:\vspace{-.25cm}\\
\begin{minipage}[t]{0.44\textwidth}
\begin{center}
\includegraphics[height=5.1cm]{images/mkl-link-line-advisor-01.jpg}
\end{center}
\end{minipage}
\begin{minipage}[t]{0.44\textwidth}
\begin{center}
\includegraphics[height=5.1cm]{images/mkl-link-line-advisor-02.jpg}
\end{center}
\end{minipage}
\end{itemize}
\tiny{\url{https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl-link-line-advisor.html}}
\end{frame}

% Slide 06
\begin{frame}{Linear algebra package (LAPACK)}
\begin{itemize}
\item What is LAPACK?
\begin{itemize}\normalsize
\item[-] Set of Fortran 90 routines to solve \textbf{linear systems}, \textbf{eigenvalue problems}, and \textbf{SVDs} with \textbf{dense but small to moderately sized} as well as \textbf{structured sparse} (banded, tridiagonal, ...) matrices:\vspace{.05cm}
\item[-] Successor to LINPACK (1979, for linear systems and least squares pbs.)\\
\hspace{1.4cm}and EISPACK (1976, for eigenvalue problems).\vspace{.05cm}
\item[-] Developed and maintained by an international \textbf{team of researchers}.%\vspace{.05cm}
\end{itemize}
\item Key characteristics:
\begin{itemize}\normalsize
\item[-] Optimized for \textbf{performance}, \textbf{portability} and \textbf{numerical stability}.\vspace{.05cm}
\item[-] \textbf{Relies heavily on BLAS}, especially Level 2 and 3.\vspace{.05cm}
\item[-] Performance \textbf{depends critically} on the \textbf{BLAS implementation} used.\vspace{.05cm}
\item[-] \textbf{Handles higher-level algorithms} and \textbf{delegates} operations to \textbf{BLAS}.\vspace{.05cm}
\end{itemize}
\item Available through various implementations:
\begin{itemize}\normalsize
\item[-] Reference \textbf{LAPACK}: Standard implementation, focus on correctness.\vspace{.05cm}
\item[-] Intel \textbf{MKL}: Optimized LAPACK routines alongside BLAS.\vspace{.05cm}
\item[-] GPU only: Nvidia \textbf{cuSOLVER}, AMD \textbf{rocSOLVER}.
\end{itemize}
\end{itemize}
\end{frame}

% Slide 07
\begin{frame}{Nomenclature of LAPACK subroutines}
LAPACK routines follow a \textbf{structured naming convention}: \texttt{XYYZZZ}
\begin{itemize}
\item \textbf{Data types} (\texttt{X}):\\
$\hspace*{1.1cm}${\color{blue}{\texttt{D}}}: double precision real$\hspace{1.96cm}${\color{blue}{\texttt{S}}}: single precision real\vspace{.05cm}\\
$\hspace*{1.1cm}${\color{blue}{\texttt{C}}}: single precision complex$\hspace{1.37cm}${\color{blue}{\texttt{Z}}}: double precision complex
\item Common \textbf{matrix types} (\texttt{YY}):\\
$\hspace*{.6cm}${\color{green}{\texttt{GE}}}: general$\hspace{1.4cm}${\color{green}{\texttt{SY}}}: symmetric$\hspace{1.07cm}${\color{green}{\texttt{HG}}}: upper Hessenberg\vspace{.05cm}\\
$\hspace*{.6cm}${\color{green}{\texttt{PO}}}: SPD/HPD$\hspace{.84cm}${\color{green}{\texttt{TR}}}: triangular$\hspace{1.17cm}${\color{green}{\texttt{BD}}}: bidiagonal
\item Common \textbf{computational tasks} (\texttt{ZZZ}):\\
$\hspace*{1.1cm}${\color{red}{\texttt{SV}}}: solve linear system$\hspace{1.97cm}${\color{red}{\texttt{TRF}}}: triangular factorization\vspace{.05cm}\\
$\hspace*{.9cm}${\color{red}{\texttt{TRS}}}: solve using factorization$\hspace{1.12cm}${\color{red}{\texttt{CON}}}: estimate conditioning\vspace{.05cm}\\
$\hspace{1.1cm}${\color{red}{\texttt{EV}}}: solve eigenvalue problem
\item Examples of (driver) subroutines:
\begin{itemize}\normalsize
\item[-] 
{\color{blue}{\texttt{D}}}{\color{green}{\texttt{GE}}}{\color{red}{\texttt{SV}}}: 
linear solve with real general matrix in double precision.\vspace{.05cm}
\item[-] 
{\color{blue}{\texttt{C}}}{\color{green}{\texttt{PO}}}{\color{red}{\texttt{SV}}}: 
linear solve with (complex) HPD matrix in single precision.\vspace{.05cm}
%\item[-] 
%{\color{blue}{\texttt{S}}}{\color{green}{\texttt{TR}}}{\color{red}{\texttt{TRS}}}: 
%linear solve with real triangular matrix in single precision.\vspace{.05cm}
\item[-] 
{\color{blue}{\texttt{Z}}}{\color{green}{\texttt{GE}}}{\color{red}{\texttt{EV}}}: 
eigensolve with general complex matrix in double precision.
\end{itemize}
\end{itemize}
\end{frame}

% Slide 08
\begin{frame}{Structure of LAPACK subroutines}
\begin{itemize}
\item There are three types of LAPACK routines:
\begin{itemize}
\item[-] \textbf{Driver} routines: solves a \textbf{complete problem}, e.g.,\\
\hspace{2.53cm}linear systems, eigenvalue problems, least-squares problems, $\!...\hspace{-1cm}$\vspace{.1cm}
\item[-] \textbf{Computational} routines: performs an \textbf{intermediate level task}, e.g.,\\
\hspace{3.9cm}LU factorization, tridiagonal reduction, ...\vspace{.1cm}
\item[-] \textbf{Auxiliary} routines: \textbf{unblocked sub-tasks of block algorithms},\\
\hspace{2.9cm}BLAS-like operations, other low level tasks.\vspace{.15cm}
\end{itemize}
\item \textbf{Driver} routines listed in the online documentation:
\raisebox{-.8cm}{\includegraphics[height=2cm]{images/lapack-routines.jpg}}\vspace{.15cm}\\
\begin{center}
\url{https://www.netlib.org/lapack/explore-html/modules.html}
\end{center}
\item \textbf{Computational} routines listed by module:
\begin{center}
\url{https://www.netlib.org/lapack/lug/node37.html}
\end{center}
\item \textbf{Auxiliary} routines listed by category:
\begin{center}
\url{https://www.netlib.org/lapack/lug/node144.html}
\end{center}
\end{itemize}
\end{frame}

% Slide 09
\begin{frame}{BLAS and LAPACK in Julia}
\begin{itemize}
\item Default implementation:
\begin{itemize}\normalsize
\item[-] Ships with \textbf{multi-threaded OpenBLAS} and \textbf{reference LAPACK}.\vspace{.02cm}
\item[-] \textbf{Flexible}, i.e., can use other implementations, e.g., \textbf{MKL}, \textbf{BLIS}, ...\vspace{.02cm}
\end{itemize}
\item Three \textbf{implementation-independent} levels of \textbf{access} (like in Python):
\begin{itemize}\normalsize
\item[-] \textbf{Interface wrappers} via \texttt{LinearAlgebra.\{BLAS,LAPACK\}}:
\begin{center}\texttt{BLAS.gemm!}, \texttt{LAPACK.getrf!}, ...\end{center}
{\color{green}{\textbf{most control}}}\hfill
{\color{green}{\textbf{no extra copies/allocations}}}\hfill
{\color{red}{\textbf{math-implicit}}}$\hspace{.4cm}$
\item[-] \textbf{Intermediate level functions}:
\begin{center}
\mintinline{julia}{dot(x,y)},
\mintinline{julia}{mul!(C,A,B)},
\mintinline{julia}{lu(A)}, ...
\end{center}
{\color{orange}{\textbf{less control}}}\hfill
{\color{green}{\textbf{in-place versions available}}}\hfill
\textbf{good compromise}$\hspace{.4cm}$
\item[-] \textbf{High-level syntax}:
\begin{center}
\mintinline{julia}{A * x},
\mintinline{julia}{A \ b},
\mintinline{julia}{A / B}, ...
\end{center}
{\color{red}{\textbf{least control}}}\hfill
{\color{red}{\textbf{extra copies/allocations}}}\hfill
{\color{green}{\textbf{math-explicit}}}$\hspace{.4cm}$
\end{itemize}
\item Key features:
\begin{itemize}\normalsize
\item[-] $\!\!$\textbf{Matrix type} specified by \textbf{data structure}, $\!$e.g., $\!$\texttt{Symmetric}, $\!$\texttt{Tridiagonal}.$\hspace{-1cm}$\vspace{.02cm}
\item[-] $\!\!$\textbf{Multiple dispatch}: function behavior depends on types of \textbf{all} arguments.$\hspace{-1cm}$\vspace{.02cm}
\item[-] $\!\!$Operations \textbf{preserve matrix structure} when applicable.
\end{itemize}
\end{itemize}
\end{frame}

\section{Sparse matrix data structures\\{\small Section 9.1 in Darve \& Wootters (2021)}}

% Slide 10
\begin{frame}{Sparse matrices}
\begin{itemize}
\item \textbf{Sparse matrices} are \textbf{matrices} with relatively \textbf{few non-zero components}.$\hspace{-1cm}$\vspace{.05cm}
\item Natural occurrence in scientific applications:
\begin{itemize}\normalsize
\item[-] \textbf{Discretized differential equations}:
\begin{itemize}\normalsize
\item[o] ODEs: chemical reactions, multi-body systems with short-range\\
\hspace{1.24cm}interactions, multi-agent systems with local interactions, ...\vspace{.05cm}
\item[o] PDEs: fluid dynamics, solid mechanics, electromagnetics, ...\vspace{.05cm}
\item[o] DAEs: circuit simulation, power grid modeling, ...
\end{itemize}\vspace{.05cm}
\item[-] \textbf{Networks and graphs}:
\begin{itemize}\normalsize
\item[o] Adjacency, transition and Laplacian matrices of sparse graphs.
\end{itemize}\vspace{.05cm}
\item[-] \textbf{Data science}:
\begin{itemize}\normalsize
\item[o] Feature matrices in high-dimensional data.
\end{itemize}
\end{itemize}
\item Important properties:
\begin{itemize}\normalsize
\item[-] \textbf{Inverses} of sparse matrices are generally \textbf{dense}, i.e., not sparse.\vspace{.05cm}
\item[-] \textbf{Factorizations} of sparse matrices \textbf{may be reasonably sparse}.\vspace{.05cm}
\item[-] \textbf{Dense matrices} can be \textbf{approximated by sparse matrices}, i.e.,\\
using sparse approximate inverses (SPAI).
\end{itemize}
\end{itemize}
\end{frame}

% Slide 11
\begin{frame}{Repository of sparse matrices}
\begin{itemize}
\item \textbf{Researchers} and \textbf{developers} often \textbf{need} multiple \textbf{sparse matrices} with documented characteristics \textbf{to benchmark} NLA \textbf{algorithms}.
\item In particular, the \textbf{SuiteSparse Matrix Collection} is widely used for this:
\begin{center}\url{https://sparse.tamu.edu/}\end{center}
\begin{itemize}\normalsize
\item[-] \textbf{Close to 3,000 matrices} available.
\item[-] Matrices \textbf{from all sorts of applications}.
\item[-] \textbf{Metadata available} include: author, application field, rank, condition number, singular values, definiteness, symmetry and lack thereof, ...
\end{itemize}
\item We can generally distinguish between two types of sparse matrices:
\begin{itemize}\normalsize
\item[-] \textbf{Structured}: typically coming from differential equations discretized on structured grids/meshes.\\
E.g., \texttt{sherman5} (computational fluid dynamics problem): 
\raisebox{-.4cm}{\includegraphics[height=1cm]{images/sherman5.jpg}}\vspace{.15cm}\\
\item[-] \textbf{Unstructured}: most other cases.\\
E.g., \texttt{bp\_1000} (optimization problem): 
\raisebox{-.4cm}{\includegraphics[height=1cm]{images/bp_1000.jpg}}
\end{itemize}
\end{itemize}
\end{frame}

% Slide 12
\begin{frame}{Sparse matrix data structures}
\begin{itemize}
\item The use of \textbf{proper data structures} is essential \textbf{to}
\begin{center}\textbf{limit memory requirements} and \textbf{achieve good performance}\end{center}
when deploying basic linear algebra operations and NLA algorithms \textbf{with sparse matrices}.
\item There is \textbf{no unique} sparse matrix \textbf{data structure to optimally serve all purposes in all situations}.
\item In general, the \textbf{choice of} a sparse \textbf{data structure} can be \textbf{influenced by}
\begin{itemize}\normalsize
\item[-] \textbf{Sparsity pattern} of the matrix.\vspace{.15cm}
\item[-] \textbf{Hardware architecture}:
\begin{itemize}\normalsize
\item[o] Memory layout.
\item[o] Sequential vs parallel with shared and/or distributed memory vs GPU.\vspace{.1cm}
\end{itemize}
\item[-] \textbf{Algorithm and operations}:
\begin{itemize}\normalsize
\item[o] Type of access.
\item[o] BLAS level, i.e., 1, 2 or 3.
\end{itemize}
\item[-] \textbf{Implementation requirements}.
\end{itemize}
\end{itemize}
\end{frame}

% Slide 13
\begin{frame}{Sparse matrix data structures, cont'd\textsubscript{1}}
\begin{itemize}
\item There are many sparse matrix data structure formats. In particular:\vspace{.05cm}
\begin{itemize}\normalsize
\item[-] Coordinate (\texttt{COO})\vspace{.1cm}\\
\fbox{\begin{minipage}{0.88\textwidth}
{\color{green}{\textbf{intuitive/explicit}}}\hfill
{\color{red}{\textbf{not efficient}}}\hfill
{\color{green}{\textbf{large community support}}}
\begin{center}\textbf{most convenient/used for construction}\end{center}
\end{minipage}}\vspace{.3cm}
\item[-] Compressed sparse row (\texttt{CSR}), compressed sparse column (\texttt{CSC})\vspace{.1cm}\\
\fbox{\begin{minipage}{0.88\textwidth}
{\color{green}{\textbf{lowest memory need}}}\hfill
{\color{green}{\textbf{efficient}}}\hfill
{\color{green}{\textbf{large community support}}}
\begin{center}\textbf{most used}\end{center}
\end{minipage}}\vspace{.3cm}
\end{itemize}
\item Variants of \texttt{CSR} and \texttt{CSC}:
\begin{itemize}
\item[-] Block sparse row (\texttt{BSR}/\texttt{BCSR}), block sparse column (\texttt{BSC}/\texttt{BCSC})\\
{\color{green}{\textbf{good for block matrices}}}\hfill
{\color{red}{\textbf{overhead otherwise}}}\hfill
{\color{green}{\textbf{large support}}}$\hspace{.5cm}$\vspace{.1cm}
\item[-] Mapped block row (\texttt{MBR}) sparse\\
{\color{green}{\textbf{lower memory need}}}\hfill
{\color{green}{\textbf{more efficient}}}\hfill
{\color{red}{\textbf{limited community support}}}$\hspace{.5cm}$\vspace{.1cm}
\item[-] Modified sparse row (\texttt{MSR}/\texttt{MCSR}), modified sparse column (\texttt{MSC}/\texttt{MCSC})$\hspace{-1cm}$\\
{\color{green}{\textbf{fast diagonal access}}}\hfill
{\color{red}{\textbf{square matrices only}}}$\hspace{.5cm}$\\
\begin{center}{\color{red}{\textbf{limited community support}}}\end{center}
\end{itemize}
\end{itemize}
\end{frame}

% Slide 14
\begin{frame}{Sparse matrix data structures, cont'd\textsubscript{2}}
\begin{itemize}
\item Vector architectures and GPU:%\vspace{.2cm}
\begin{itemize}\normalsize
\item[-] Ellpack (\texttt{ELL})\\
{\color{green}{\textbf{good for uniform sparsity}}}\hfill
{\color{green}{\textbf{community support}}}\hfill
{\color{green}{\textbf{GPU-friendly}}}$\hspace{.5cm}$\vspace{.3cm}
\end{itemize}
\item Banded matrices:
\begin{itemize}\normalsize
\item[-] Diagonal (\texttt{DIA})\\
{\color{green}{\textbf{good for fixed bandwidth}}}\hfill
{\color{red}{\textbf{wasteful otherwise}}}$\hspace{.5cm}$
\begin{center}{\color{orange}{\textbf{moderate support}}}\end{center}\vspace{.1cm}
\item[-] Non-symmetric skyline (\texttt{NSK}), symmetric skyline (\texttt{SSK})\\
{\color{green}{\textbf{good for variable bandwidth}}}\hfill
{\color{red}{\textbf{wasteful for isolated bands}}}$\hspace{.5cm}$\\
\begin{center}{\color{orange}{\textbf{moderate support}}}\end{center}\vspace{.3cm}
\end{itemize}
\item Pythonic environment:
\begin{itemize}
\item[-] List of lists (\texttt{LIL})\\
\textbf{used for construction}\hfill
\textbf{Python-specific support}\hfill
{\color{red}{\textbf{not efficient}}}$\hspace{.5cm}$\vspace{.1cm}
\item[-] Dictionary of keys (\texttt{DOK})\\
\textbf{used for construction}\hfill
\textbf{Python-specific support}\hfill
{\color{red}{\textbf{not efficient}}}$\hspace{.5cm}$
\end{itemize}
\end{itemize}
\end{frame}

% Slide 15
\begin{frame}{Coordinate (\texttt{COO}) format}
\begin{itemize}
\item A COO data structures format is composed of:
\begin{itemize}\normalsize
\item[-] Array of \textbf{non-zero components} (\texttt{val})
\item[-] Array of \textbf{row indices of each component} (\texttt{row\_idx})
\item[-] Array of \textbf{column indices of each components} (\texttt{col\_idx})\vspace{.02cm}
\end{itemize}
\item Example:\vspace{-.8cm}\\
\hspace*{-.05cm}\begin{minipage}[t]{0.9\textwidth}
\begin{empheq}[box=\fbox]{align*}
A=&\;\left[\begin{matrix}
a_{11}&a_{12}&a_{13}&0\\
a_{21}&a_{22}&0     &0     \\
0     &0     &a_{33}&a_{34}\\
0     &0     &a_{43}&0
\end{matrix}\right]\\
\texttt{val}=&\;[a_{11}, a_{12}, a_{13}, a_{21}, a_{22}, a_{33}, a_{34}, a_{43}]\\
\texttt{row\_idx}=&\;[1, 1, 1, 2, 2, 3, 3, 4]\\
\texttt{col\_idx}=&\;[1, 2, 3, 1, 2, 3, 4, 3]
\end{empheq}
\end{minipage}\vspace{.1cm}
\item Key characteristics:
\begin{itemize}\normalsize
\item[-] Explicit storage of all indices (higher memory usage)
\item[-] \textbf{No particular ordering} required
\item[-] \textbf{Duplicates allowed} (values must be summed)
\item[-] \textbf{Flexible} for matrix \textbf{construction} and \textbf{modification}
\end{itemize}
\end{itemize}
\end{frame}

% Slide 16
\begin{frame}{Compressed sparse row (\texttt{CSR}) format}
\begin{itemize}
\item A CSR data structures format is composed of:
\begin{itemize}\normalsize
\item[-] Array of \textbf{non-zero components} (\texttt{val})
\item[-] Array of \textbf{column indices of each component} (\texttt{col\_idx})
\item[-] Array of \textbf{non-zero value indices where each row starts} (\texttt{row\_start})$\hspace{-1cm}$\vspace{.02cm}
\end{itemize}
\item Example:\vspace{-.8cm}\\
\hspace*{.15cm}\begin{minipage}[t]{0.9\textwidth}
\begin{empheq}[box=\fbox]{align*}
A=&\;\left[\begin{matrix}
a_{11}&a_{12}&a_{13}&0\\
a_{21}&a_{22}&0     &0     \\
0     &0     &a_{33}&a_{34}\\
0     &0     &a_{43}&0
\end{matrix}\right]\\
\texttt{val}=&\;[a_{11}, a_{12}, a_{13}, a_{21}, a_{22}, a_{33}, a_{34}, a_{43}]\\
\texttt{col\_idx}=&\;[1, 2, 3, 1, 2, 3, 4, 3]\\
\texttt{row\_start}=&\;[1, 4, 6, 8, 9]
\end{empheq}
\end{minipage}\vspace{.1cm}
\item Key characteristics:
\begin{itemize}\normalsize
\item[-] Compact storage (lower memory than COO)
\item[-] \textbf{Fast row access}
\item[-] \textbf{Values must be ordered} by row
\item[-] \textbf{Difficult to modify} structure dynamically
\end{itemize}
\end{itemize}
\end{frame}

% Slide 17
\begin{frame}{Compressed sparse column (\texttt{CSC}) format}
\begin{itemize}
\item A CSC data structures format is composed of:
\begin{itemize}\normalsize
\item[-] Array of \textbf{non-zero components} (\texttt{val})
\item[-] Array of \textbf{row indices of each component} (\texttt{row\_idx})
\item[-] Array of \textbf{non-zero indices where each column starts} (\texttt{col\_start})$\hspace{-1cm}$\vspace{.02cm}
\end{itemize}
\item Example:\vspace{-.8cm}\\
\hspace*{.15cm}\begin{minipage}[t]{0.9\textwidth}
\begin{empheq}[box=\fbox]{align*}
A=&\;\left[\begin{matrix}
a_{11}&a_{12}&a_{13}&0\\
a_{21}&a_{22}&0     &0     \\
0     &0     &a_{33}&a_{34}\\
0     &0     &a_{43}&0
\end{matrix}\right]\\
\texttt{val}=&\;[a_{11}, a_{21}, a_{12}, a_{22}, a_{13}, a_{33}, a_{43}, a_{34}]\\
\texttt{row\_idx}=&\;[1, 2, 1, 2, 1, 3, 4, 3]\\
\texttt{col\_start}=&\;[1, 3, 5, 8, 9]
\end{empheq}
\end{minipage}\vspace{.1cm}
\item Key characteristics:
\begin{itemize}\normalsize
\item[-] Compact storage (lower memory than COO)
\item[-] \textbf{Fast column access}
\item[-] \textbf{Values must be ordered} by column
\item[-] \textbf{Difficult to modify} structure dynamically
\end{itemize}
\end{itemize}
\end{frame}

% Slide 18
\begin{frame}{Block sparse row (\texttt{BSR}) format}
\begin{itemize}
\item A BSR (or BCSR) data structure format is composed of:
\begin{itemize}\normalsize
\item[-] \textbf{Block dimensions} (\texttt{r}$\times$\texttt{c})
\item[-] Array (or matrix) of \textbf{all components of non-zero blocks} (\texttt{val})
\item[-] Array of \textbf{non-zero block column indices} (\texttt{col\_idx})
\item[-] Array of \textbf{block indices where each block row starts} (\texttt{row\_start})\vspace{.02cm}
\end{itemize}
\item Example:\vspace{-.8cm}\\
\hspace*{.9cm}\begin{minipage}[t]{0.9\textwidth}
\begin{empheq}[box=\fbox]{align*}
A=&\;\left[\begin{matrix}
a_{11}&a_{12}&a_{13}&0\\
a_{21}&a_{22}&0     &0     \\
0     &0     &a_{33}&a_{34}\\
0     &0     &a_{43}&0
\end{matrix}\right]\\
\texttt{r}=&\;2,\texttt{c}=2\\
\texttt{val}=&\;[a_{11}, a_{12}, a_{21}, a_{22}, a_{13}, 0, 0, 0, a_{33}, a_{34}, a_{43}, 0]\\
\texttt{col\_idx}=&\;[1, 2, 2]\\
\texttt{row\_start}=&\;[1, 3, 4]
\end{empheq}
\end{minipage}\vspace{.06cm}
\item Key characteristics:\vspace{-.04cm}
\begin{itemize}\normalsize
\item[-] \textbf{Zero values within non-zero blocks} are \textbf{stored}\vspace{-.02cm}
\item[-] \textbf{Similar to CSR but} operates \textbf{on blocks}
\end{itemize}
\end{itemize}
\end{frame}

% Slide 19
\begin{frame}{Mapped block row (\texttt{MBR}) format}
\begin{itemize}
\item A MBR data structure format is composed of:
\begin{itemize}\normalsize
\item[-] \textbf{Block dimensions} (\texttt{r}$\times$\texttt{c})
\item[-] Array of \textbf{non-zero components of non-zero blocks} (\texttt{val})$\hspace{-1cm}$
\item[-] Array of \textbf{non-zero block column indices} (\texttt{col\_idx})
\item[-] Array of \textbf{sparsity pattern encoding} (\texttt{b\_map})
\item[-] Array of \textbf{block indices where each block row starts} (\texttt{row\_start})\vspace{.02cm}
\end{itemize}
\item Example:\vspace{-1.2cm}\\
\hspace*{1.18cm}\begin{minipage}[t]{0.9\textwidth}
\begin{empheq}[box=\fbox]{align*}
A=&\;\left[\begin{matrix}
a_{11}&a_{12}&a_{13}&0\\
a_{21}&a_{22}&0     &0     \\
0     &0     &a_{33}&a_{34}\\
0     &0     &a_{43}&0
\end{matrix}\right]\\
\texttt{r}=&\;2,\texttt{c}=2\\
\texttt{val}=&\;[a_{11}, a_{12}, a_{21}, a_{22}, a_{13}, a_{33}, a_{34}, a_{43}]\\
\texttt{col\_idx}=&\;[1, 2, 2]\hspace{.3cm}
\texttt{b\_map}=[15, 1, 7]\hspace{.3cm}
\texttt{row\_start}=[1, 3, 4]
\end{empheq}
\end{minipage}\vspace{.06cm}
\item Key characteristic:\vspace{-.04cm}
\begin{itemize}\normalsize
\item[-] \textbf{Non-zero values within non-zero blocks} are \textbf{not stored}
\end{itemize}
\end{itemize}
\end{frame}

% Slide 20
\begin{frame}{Modified sparse row (\texttt{MSR}) format}
\begin{itemize}
\item A MSR data structure format is composed of:
\begin{itemize}\normalsize
\item[-] Array of \textbf{diagonal elements first}, then \textbf{other non-zeros} (\texttt{val})\vspace{.02cm}
%\item[-] Vector of \textbf{column indices of off-diagonal elements} (\texttt{col\_idx})
\item[-] Composite array $\texttt{idx}:=[\texttt{row\_start},\texttt{col\_idx}]$ where:
\begin{itemize}\normalsize
\item[o] $\texttt{row\_start}$ contains the \textbf{index of off-diagonal non-zero value where each row starts}.
\item[o] $\texttt{col\_idx}$ contains \textbf{column indices of each off-diagonal non-zero component}.
\end{itemize}
\end{itemize}
\item Example:\vspace{-.8cm}\\
\hspace*{.1cm}\begin{minipage}[t]{0.9\textwidth}
\begin{empheq}[box=\fbox]{align*}
A=&\;\left[\begin{matrix}
a_{11}&a_{12}&a_{13}&0\\
a_{21}&a_{22}&0     &0     \\
0     &0     &a_{33}&a_{34}\\
0     &0     &a_{43}&0
\end{matrix}\right]\\
\texttt{val}=&\;[a_{11}, a_{22}, a_{33}, 0, -1, a_{12}, a_{13}, a_{21}, a_{34}, a_{43}]\\
\texttt{idx}=&\;[6,8,9,10,11,2,3,1,4,3]
\end{empheq}
\end{minipage}\vspace{.1cm}
\item Key characteristics:
\begin{itemize}\normalsize
\item[-] \textbf{Diagonal elements stored first} $\implies$ \textbf{Fast diagonal access}
\item[-] Dummy element, here $-1$, stored in \texttt{val} for consistency with \texttt{idx} (?).
\end{itemize}
\end{itemize}
\end{frame}

% Slide 21
\begin{frame}{Ellpack (\texttt{ELL}) format}
\begin{itemize}
\item An ELL data structure format is composed of:
\begin{itemize}\normalsize
\item[-] \textbf{Maximum number} of \textbf{non-zero components on a row} (\texttt{row\_nnz})
\item[-] Array of \textbf{all components stored in column-major order, from the block of left-aligned non-zero components} (\texttt{val})
\item[-] Array of \textbf{column indices of stored components} (\texttt{col\_idx})$\hspace{-1cm}$\vspace{.02cm}
\end{itemize}
\item Example:\vspace{-.8cm}\\
\hspace*{.68cm}\begin{minipage}[t]{0.9\textwidth}
\begin{empheq}[box=\fbox]{align*}
A=&\;\left[\begin{matrix}
a_{11}&a_{12}&a_{13}&0     \\
a_{21}&a_{22}&0     &0     \\
0     &0     &a_{33}&a_{34}\\
0     &0     &a_{43}&0
\end{matrix}\right]\\
\texttt{row\_nnz}=&\;3\\
\texttt{val}=&\;[a_{11}, a_{21}, a_{33}, a_{43}, a_{12}, a_{22}, a_{34}, 0, a_{13}, 0, 0, 0]\\
\texttt{col\_idx}=&\;[1, 1, 3, 3, 2, 2, 4, -1, 3, -1, -1, -1]
\end{empheq}
\end{minipage}\vspace{.1cm}
\item Key characteristics:
\begin{itemize}\normalsize
\item[-] Stores 2$\times $\texttt{row\_nnz} values, including some zeros
\item[-] Wasteful if number of non-zero components varies significanly from one row to another
%\item[-] \textbf{Fixed-size structure} with vector size (padded with dummy entries, here, $-1$)
%\item[-] \textbf{Good for uniform number of non-zeros per row}
%\item[-] Wasteful if rows have varying numbers of non-zero values
%\item[-] \textbf{Well-suited for} vector architectures and \textbf{GPU}
\end{itemize}
\end{itemize}
\end{frame}

% Slide 22
\begin{frame}{Diagonal (\texttt{DIA}) format}
\begin{itemize}
\item A DIA data structure format is composed of:
\begin{itemize}\normalsize
\item[-] Array of \textbf{components on non-zero diagonals} padded to $n$ (\texttt{val})
\item[-] Array of \textbf{offset indices} (\texttt{ioff})$\hspace{-1cm}$\vspace{.02cm}
\end{itemize}
\item Example:\vspace{-1.2cm}\\
\hspace*{1.1cm}\begin{minipage}[t]{0.9\textwidth}
\begin{empheq}[box=\fbox]{align*}
A=&\;\left[\begin{matrix}
a_{11}&a_{12}&a_{13}&0     \\
a_{21}&a_{22}&0     &0     \\
0     &0     &a_{33}&a_{34}\\
0     &0     &a_{43}&0
\end{matrix}\right]\\
\texttt{val}=&\;
[*,a_{21},0,a_{43},a_{11},a_{22},a_{33},0,a_{12},0,a_{34},*,a_{13},0,*,*]\\
\texttt{ioff}=&\;[-1, 0, 1, 2]
\end{empheq}
\end{minipage}\vspace{.1cm}
\item Key characteristics:
\begin{itemize}\normalsize
\item[-] \textbf{Fast diagonal access}
\item[-] Wasteful for diagonal with large offset indices (?)
%\item[-] \textbf{Fixed-size structure} with vector size (padded with dummy entries, here, $-1$)
%\item[-] \textbf{Good for uniform number of non-zeros per row}
%\item[-] Wasteful if rows have varying numbers of non-zero values
%\item[-] \textbf{Well-suited for} vector architectures and \textbf{GPU}
\end{itemize}
\end{itemize}
\end{frame}

% Slide 23
\begin{frame}{List of list (\texttt{LIL}) format}
\begin{itemize}
\item A LIL data structure format is composed of:
\begin{itemize}\normalsize
\item[-] A \textbf{list} (\texttt{rows}) \textbf{of lists}, one per row, \textbf{each list storing column indices of non-zero components}.
\item[-] A \textbf{list} (\texttt{data}) \textbf{of lists}, one per row, \textbf{each list storing non-zero components, ordered consistently with the indices in} \texttt{rows}.
\end{itemize}
\item Example:\vspace{-.65cm}\\
\hspace*{-.2cm}\begin{minipage}[t]{0.99\textwidth}
\begin{empheq}[box=\fbox]{align*}
A=&\;\left[\begin{matrix}
a_{11}&a_{12}&a_{13}&0     \\
a_{21}&a_{22}&0     &0     \\
0     &0     &a_{33}&a_{34}\\
0     &0     &a_{43}&0
\end{matrix}\right]
\texttt{rows}=\left[\begin{matrix*}[l]
[1, 2, 3]\\
[1, 2]\\
[3, 4]\hfill\\
[3]\hfill
\end{matrix*}\right]
\texttt{data}=\left[\begin{matrix*}[l]
[a_{11}, a_{12}, a_{13}]\\
[a_{21}, a_{22}]\\
[a_{33}, a_{34}]\hfill\\
[a_{43}]\hfill
\end{matrix*}\right]
\end{empheq}
\end{minipage}\vspace{.1cm}
\item Key characteristics:
\begin{itemize}\normalsize
\item[-] \textbf{No particular ordering} required for column indices
\item[-] \textbf{Unordered column indices} slows down access
\item[-] Mostly used for matrix \textbf{construction}, particularly in \textbf{Python}
\end{itemize}
\end{itemize}
\end{frame}

% Slide 24
\begin{frame}[fragile]{Sparse matrix data structures in practice}
\begin{itemize}
\item \textbf{Intel oneAPI MKL} supports sparse vectors, and the sparse matrix data structures \texttt{CSR}, \texttt{CSC}, \texttt{COO} and \texttt{BSR}.\vspace{.1cm}\\
For example, using the C interface:
\begin{itemize}\normalsize
\item[-] A \texttt{COO} \textbf{matrix} can be \textbf{created} as follows:\vspace{.2cm}
\begin{center}
\includegraphics[height=1.4cm]{images/mkl-coo.jpg}
\end{center}
$\vspace{-.45cm}$\\
\item[-] Sparse matrices can be \textbf{defined in other formats}, namely \texttt{CSR}, \texttt{CSC} and \texttt{BSR}, \textbf{directly from their underlying data structures}.\vspace{.1cm}
\item[-] Only two functions to \textbf{convert constructed sparse matrices} into
\begin{center}
$\hspace{.58cm}$\texttt{CSR} (\mintinline{c}{mkl_sparse_convert_csr})\\ 
and \texttt{BSR} (\mintinline{c}{mkl_sparse_convert_bsr}).
\end{center}
Possible to convert $A$ into \texttt{CSC}, by using the \texttt{CSR} representation of $A^T$.\vspace{.1cm}
\item[-] \textbf{Documentation}:
\begin{center}\small
\url{https://www.intel.com/content/www/us/en/docs/onemkl/developer-reference-c/2024-2/matrix-manipulation-routines.html}
\end{center}
\end{itemize}
\end{itemize}
\end{frame}

% Slide 25
\begin{frame}[fragile]{Sparse matrix data structures in practice, cont'd}
\begin{itemize}
\item \textbf{Nvidia cuSPARSE} also supports several vectors, and several sparse matrix data structures:
\begin{itemize}\normalsize
\item[-] \texttt{COO}, \texttt{CSR}, \texttt{CSC} and \texttt{BSR}\vspace{.02cm}
\item[-] Sliced Ellpack (\texttt{SELL})\vspace{.02cm}
\item[-] Blocked Ellpack (\texttt{BLOCKED-ELL})
\end{itemize}
\textbf{Documentation}:
\begin{center}\small
\url{https://docs.nvidia.com/cuda/cusparse/#cusparse-storage-formats}\vspace{.2cm}
\end{center}
\item Other implementations:
\begin{itemize}\normalsize
\item[-] \textbf{AMD ROCsparse}: proprietary, for GPU\vspace{.05cm}
\item[-] \textbf{SuiteSparse}, \textbf{PETSc},\textbf{Trilinos}, \textbf{OSKI}, \textbf{PSBLAS}, ... : open-source
\end{itemize}
\end{itemize}
\end{frame}

% Slide 26
\begin{frame}{Sparse matrix data structures in Julia}
\begin{itemize}
\item Support of basic structured formats through \texttt{LinearAlgebra.jl}:
\begin{center}\texttt{Diagonal}, \texttt{Bidiagonal}, \texttt{Tridiagonal}, \texttt{SymTridiagonal}, ...
\end{center}
\item Standard library support through \texttt{SparseArrays.jl}:
\vspace{.05cm}\\
\begin{itemize}\normalsize
\item[-] \textbf{Only} \texttt{CSC} (\texttt{SparseMatrixCSC}) is \textbf{supported by default}:\vspace{.1cm}
\begin{center}
\hspace{-2cm}\includegraphics[height=1.9cm]{images/jl-csc-struct.jpg}
\end{center}
$\vspace*{-.55cm}$\\
\item[-] Construction using \texttt{COO}-style input:\vspace{.1cm}
\begin{center}
\hspace{-2cm}\includegraphics[height=.65cm]{images/jl-coo-csc.jpg}
\end{center}
$\vspace{-.5cm}$\\
with immediate \textbf{conversion} to \texttt{CSC}.\vspace{.1cm}
\item[-] Construction using the \texttt{SparseMatrixCSC} \mintinline{julia}{struct}:
\begin{center}
\hspace{-2cm}\includegraphics[height=1.15cm]{images/jl-csc-from-struct.jpg}
\end{center}
\end{itemize}
\end{itemize}
\end{frame}

% Slide 27
\begin{frame}{Sparse matrix data structures in Julia, cont'd}
\begin{itemize}
\item[]
\begin{itemize}
\item[-] \textbf{Random} constructor for \textbf{sparse matrix} of density $d$ with iid non-zero elements \textbf{distributed uniformly} in $[0,1)$, \mintinline{julia}{sprand(m,n,d)}.\vspace{.05cm}
\item[-] \textbf{Random} constructor for \textbf{sparse matrix} of density $d$ with iid non-zero elements \textbf{distributed according to the standard normal distribution}, \mintinline{julia}{sprandn(m,n,d)}.
\end{itemize}
\item More formats supported through other packages:
\begin{itemize}\normalsize
\item[-] \texttt{SparseMatricesCSR.jl}: Julia native implementation of \texttt{CSR} formats.\vspace{.05cm}
\item[-] \texttt{MKLSparse.jl}: Julia wrappers to Intel oneAPI MKL sparse interface.\vspace{.05cm}
\item[-] \texttt{SuiteSparse.jl}: Julia wrappers to SuiteSparse library.\vspace{.05cm}
\item[] $\hspace{1cm}\vdots\hspace{7cm}\vdots$
\end{itemize}
\end{itemize}
\end{frame}

\section{Sparse BLAS\\{\small Section 9.1 in Darve \& Wootters (2021)}}

% Slide 28
\begin{frame}{Sparse basic linear algebra subprograms}
\begin{itemize}
\item \textbf{Sparse BLAS} is the extension of BLAS for \textbf{sparse matrices and vectors}.\vspace{.03cm}
\item \textbf{Level 1} (\textbf{vector} operations):\vspace{.05cm}\\
\textbf{Intel oneAPI MKL} functions use a compressed sparse vector format:\\
\begin{center}\small
\url{https://www.intel.com/content/www/us/en/docs/onemkl/developer-reference-c/2024-2/sparse-blas-level-1-routines.html}
\end{center}
\begin{itemize}\normalsize
\item[-] Sparse $y\leftarrow\alpha x+y$ (\texttt{SpAXPY}): \texttt{mkl\_sparse\_{\color{blue}{x}}\_axpy}\vspace{.03cm}
\end{itemize}
\item \textbf{Level 2-3} functions have \textbf{format-specific implementations}.\vspace{.05cm}\\
\textbf{Intel oneAPI MKL} offers access through an Inspector-Executor API:
\begin{center}\small
\url{https://www.intel.com/content/www/us/en/docs/onemkl/developer-reference-c/2024-2/inspector-executor-sparse-blas-execution-routines.html}
\end{center}
\begin{itemize}\normalsize
\item[-] \textbf{Level 2} (\textbf{matrix-vector} operations):
\begin{itemize}\normalsize
\item[o] Sparse matrix-vector product (\texttt{SpMV}): \texttt{mkl\_sparse\_{\color{blue}{x}}\_mv}\vspace{.03cm}
\end{itemize}
\item[-] \textbf{Level 3} (\textbf{matrix-matrix} operations):
\begin{itemize}\normalsize
\item[o] Sparse matrix-(dense) matrix product (\texttt{SpMM}): \texttt{mkl\_sparse\_{\color{blue}{x}}\_mm}\vspace{.03cm}
\item[o] Sparse matrix-(sparse) matrix product (\texttt{SpGEMM}): \texttt{mkl\_sparse\_spmm}
\end{itemize}
\end{itemize}
\end{itemize}


\end{frame}

\section{Sparse matrices and graphs\\{\small Section 9.2 in Darve \& Wootters (2021)}}

% Slide 29
\begin{frame}{A few definitions}
\begin{itemize}
\item Basics of graph theory are essential to sparse matrix computation.
\begin{definition}[Graph]
\begin{itemize}
\item[-] An \textbf{undirected graph} is a pair $G=(V,E)$ formed by a non-empty finite set $V$ of \textbf{vertices} and a set $E\subseteq V\times V$ of \textbf{unordered pairs of vertices} referred to as \textbf{edges}.\vspace{-.1cm}
\item[-] A \textbf{directed graph} $G=(V,E)$ is formed by a set $E$ of \textbf{ordered edges}.
\end{itemize}
\end{definition}
\vspace{.15cm}
\begin{center}
\includegraphics[height=2.3cm]{images/graphs.jpg}\\
\includegraphics[height=1.6cm]{images/graph-caption.jpg}
\end{center}
\end{itemize}
\tiny{Darve, E., \& Wootters, M. (2021). Numerical linear algebra with Julia. Society for Industrial and Applied Mathematics.}
\end{frame}

% Slide 30
\begin{frame}{A few definitions, cont'd}
\begin{itemize}
\item A \textbf{path} from a vertex $u$ to another vertex $v$ is a \textbf{sequence of edges} $(u_0,u_1),\dots,(u_{t-1},u_t)$
such that $u_0=u$ and $u_t=v$.\vspace{.1cm}
\item A graph is \textbf{connected} if there is a \textbf{path from any vertex} $u$ \textbf{to any vertex} $v$.
\item A \textbf{tree} is a \textbf{connected graph without cycles}, i.e., with no path from a vertex to itself.
\end{itemize}
\begin{minipage}{0.65\textwidth}
$\vspace{-.4cm}$
\begin{itemize}
\item[] A tree has a \textbf{root}, i.e., a \textbf{designated vertex} represented \textbf{at the top} of the tree.
\item If a tree has an edge $(u,v)$, and $u$ is \textbf{closer to the root} $r$ \textbf{than} $v$ \textbf{is}, then we say that $v$ is a \textbf{parent} and $u$ is a \textbf{child}.\vspace{.1cm}\\
Each vertex in a tree has a unique parent.
\item A \textbf{leaf} is a \textbf{vertex} in a tree \textbf{with no children}.
\item Family logic applies to define \textbf{descendants} and \textbf{ancestors}.
\end{itemize}
\end{minipage}
\begin{minipage}{0.3\textwidth}
$\hspace{.3cm}$\includegraphics[height=5cm]{images/tree.jpg}
\end{minipage}
$\vspace{.05cm}$
\tiny{Darve, E., \& Wootters, M. (2021). Numerical linear algebra with Julia. Society for Industrial and Applied Mathematics.}
\end{frame}

% Slide 31
\begin{frame}{Graph representation of sparsity patterns}
\begin{itemize}
\item The \textbf{sparsity pattern} of a square matrix $A\in\mathbb{F}^{n\times n}$ can be represented as a \textbf{directed graph} with $n$ \textbf{vertices}.
\item In Darve and Wooters (2021), the convention is that a \textbf{directed edge} $(i,j)$ from vertex $j$ to vertex $i$ exists if and only if $a_{ij}\neq 0$.\vspace{.1cm}\\
For example:\vspace{-.1cm}
\begin{center}
\includegraphics[height=4cm]{images/sparsity-graph.jpg}
\end{center}
\item The sparsity pattern of \textbf{symmetric matrices} can be represented by \textbf{undirected graphs}.
\end{itemize}
$\vspace{.23cm}$\\
\tiny{Darve, E., \& Wootters, M. (2021). Numerical linear algebra with Julia. Society for Industrial and Applied Mathematics.}
\end{frame}

\section{Homework problem}

% Slide 32
\begin{frame}{Homework problem}\vspace{.1cm}
Turn in \textbf{your own} solution to the following problem:\vspace{.15cm}\\
\begin{minipage}[t]{0.1\textwidth}
\textbf{Pb.$\,$15}
\end{minipage}
\begin{minipage}[t]{0.89\textwidth}
Consider the matrices
\begin{align*}
A=
\begin{bmatrix}
\bullet&\bullet&0      &\bullet&0      &0      \\
0      &\bullet&0      &0      &0      &\bullet\\
0      &\bullet&\bullet&0      &0      &0      \\
0      &\bullet&0      &0      &\bullet&0      \\
0      &0      &0      &0      &\bullet&0      \\
0      &0      &0      &0      &0      &\bullet
\end{bmatrix}
\text{ and }
B=
\begin{bmatrix}
\bullet&0      &0      &0      &0      &0      \\
\bullet&0      &\bullet&0      &\bullet&0      \\
0      &\bullet&0      &0      &0      &0      \\
\bullet&\bullet&0      &0      &0      &0      \\
0      &\bullet&0      &\bullet&\bullet&0      \\
0      &0      &\bullet&0      &0      &\bullet
\end{bmatrix}
\end{align*}
where each $\bullet$ denotes a non-zero component.\vspace{.1cm}\\
Show the adjacency graphs of $A$, $B$, $AB$ and $BA$.
You may assume that there are no numerical cancellations in computing the products $AB$ and $BA$.\vspace{.05cm}\\
Problem excerpted from Pb.$\,$4 in Chap.$\,$3 of Saad (2003).
\end{minipage}\vspace{.15cm}
\tiny{Saad, Y. (2003). Iterative methods for sparse linear systems. Society for Industrial and Applied Mathematics.}
\end{frame}

\section{Practice session}

% Slide 33
\begin{frame}[fragile]{Practice session}
\begin{enumerate}
\item Use the \mintinline{julia}{mmread} function from MatrixMarket.jl to read the matrix \texttt{cage3} from the SuiteSparse website. 
Investigate the default sparse data structure in which the matrix is stored in Julia.
\item Write a function called \mintinline{julia}{dcscmv} to perform SpMV in CSC format.
\item Write a function called \mintinline{julia}{csc_to_coo} to convert a CSC matrix to COO format. 
Use the following custom type:
\small
\begin{minted}{julia}
	mutable struct SparseMatrixCOO
	    m::Int # Number of rows
	    n::Int # Number of columns
	    rowval::Vector{Int} # Starting index for each row
	    colval::Vector{Int} # Column indices
	    nzval::Vector{Float64} # Matrix entries
	end
\end{minted}
\normalsize
\item Write a function called \mintinline{julia}{dcoomv} to perform SpMV in COO format.
\item Write a function called \mintinline{julia}{coo_to_csr} to convert a COO matrix to CSR format using a custom type \mintinline{julia}{SparseMatrixCSR} with arguments \mintinline{julia}{m::Int}, \mintinline{julia}{n::Int}, \mintinline{julia}{rowptr::Vector{Int}}, \mintinline{julia}{colval::Vector{Int}} and \mintinline{julia}{nzval::Vector{Float64}}.
\end{enumerate}
\end{frame}

\begin{frame}[fragile]{Practice session, cont'd}
\begin{enumerate}
\setcounter{enumi}{5}
\item Write a function called \mintinline{julia}{coo_to_csr2} to convert a COO matrix to CSR format making use of the built-in \mintinline{julia}{sparse} function.\\
Hint:Think of the relation between CSC and CSR.
\item Write a function called \mintinline{julia}{dcsrmv} to perform SpMV in CSR format.
\item Write a function called \mintinline{julia}{coo_to_ell} to convert a COO matrix to ELL format using a custom type \mintinline{julia}{SparseMatrixELL} with arguments \mintinline{julia}{m::Int}, \mintinline{julia}{n::Int}, \mintinline{julia}{rownnz::Int}, \mintinline{julia}{colval::Vector{Int}} and \mintinline{julia}{nzval::Vector{Float64}}.
\item Write a function called \mintinline{julia}{dellmv} to perform SpMV in ELL format.
\end{enumerate}
\end{frame}


\end{document}
